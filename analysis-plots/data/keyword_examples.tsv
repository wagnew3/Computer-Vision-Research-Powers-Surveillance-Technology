keyword	patent	title	sentence	section
advertisement				
	WO2014005022A1	Individualizing generic communications	[0055] In embodiments of the present invention, the communication is an ADVERTISEMENT or a public service communication.	[0055] In embodiments of the present invention, the communication is an advertisement or a public service communication. In a further embodiment, the communicator receives a reply from the individual in response to the communication. 
	US9189886	Method and apparatus for estimating body shape	 Such attributes might be items for sale, information about preferred clothing sizes, images, textual information or ADVERTISEMENTs.	Finally, a means for body shape matching takes a body produced from some measurements (tailoring measures, images, range sensor data) and returns one or more “scores” indicating how similar it is in shape to another body or database of bodies. This matching means is used to rank body shape similarity to, for example, reorder a display of attributes associated with a database of bodies. Such attributes might be items for sale, information about preferred clothing sizes, images, textual information or advertisements. The display of these attributes presented to a user may be ordered so that the presented items are those corresponding to people with bodies most similar to theirs. The matching and ranking means can be used to make selective recommendations based on similar body shapes. The attributes (e.g. clothing size preference) of people with similar body shapes can be aggregated to recommend attributes to a user in a form of body-shape-sensitive collaborative filtering.
	US10534808	Architecture for responding to visual query	 Similarly, if a scan of a magazine was used as the visual query, the interactive results document may include visual identifiers for photographs or trademarks in ADVERTISEMENTs on the page as well as a visual identifier for the text of an article also on that page.	In some embodiments, the interactive results document produced in response to a visual query can include a plurality of links that correspond to results from the same search system. For example, a visual query may be an image or picture of a group of people. The interactive results document may include bounding boxes around each person, which when activated returns results from the facial recognition search system for each face in the group. For some visual queries, a plurality of links in the interactive results document corresponds to search results from more than one search system (310). For example, if a picture of a person and a dog was submitted as the visual query, bounding boxes in the interactive results document may outline the person and the dog separately. When the person (in the interactive results document) is selected, search results from the facial recognition search system are returned, and when the dog (in the interactive results document) is selected, results from the image-to-terms search system are returned. For some visual queries, the interactive results document contains an OCR result and an image match result (312). For example, if a picture of a person standing next to a sign were submitted as a visual query, the interactive results document may include visual identifiers for the person and for the text in the sign. Similarly, if a scan of a magazine was used as the visual query, the interactive results document may include visual identifiers for photographs or trademarks in advertisements on the page as well as a visual identifier for the text of an article also on that page.
	US9467750	Placing unobtrusive overlays in video content	Given the broad distribution of such video content and growing proliferation of viewing and playback devices for viewing such video content, providers and distributors of video content often employ video-advertising techniques to insert ADVERTISEMENTs into video content.	Given the broad distribution of such video content and growing proliferation of viewing and playback devices for viewing such video content, providers and distributors of video content often employ video-advertising techniques to insert advertisements into video content.
	US8189957	View projection for dynamic configurations	 For example, the moving object could be an ADVERTISEMENT display, and the transformed image pI could provide additional information related to the ADVERTISEMENT display.	This method would be particularly useful for advisement purposes. For example, the moving object could be an advertisement display, and the transformed image pI could provide additional information related to the advertisement display.
airport				
	US9363489	Video analytics configuration	 The network may simply comprise a single video camera monitoring a portal which, for example, may be a door, a corridor, a lift, an entrance hall, an exit hall, a concourse, an AIRPORT security gate or anywhere there may be numerous individuals passing through.	Video surveillance networks comprise one or more video cameras arranged to provide surveillance of a particular geographical location. The network may simply comprise a single video camera monitoring a portal which, for example, may be a door, a corridor, a lift, an entrance hall, an exit hall, a concourse, an airport security gate or anywhere there may be numerous individuals passing through. In some cases, such as an entrance hall for a large shopping center or a departures hall of an airport, there may be thousands of people passing through in a relatively short space of time. This makes tracking individuals who may be considered to be a threat or of interest to security in such cases fraught with difficulty.
	US9483689	Biometric matching technology	 The system 300 also determines whether the context of the situation is in a crime-sensitive area or jewelry store, whether the context of the situation is in an AIRPORT or railway station, whether the context of the situation is in a public transport location, a public park, or vehicular surveillance, or whether the context of the situation is in home security or school campus security.	The system 300 references the data structure 1500 in setting the batch size. For example, the system 300 determines the context and criticality of the situation based on user input, a pre-defined setting, and/or an alert feed (e.g., a threat level alert provided by a government organization or other organization). In this example, the system 300 determines whether the criticality of the situation is very high, high, medium, or low. The system 300 also determines whether the context of the situation is in a crime-sensitive area or jewelry store, whether the context of the situation is in an airport or railway station, whether the context of the situation is in a public transport location, a public park, or vehicular surveillance, or whether the context of the situation is in home security or school campus security. Based on the determinations, the system 300 references the data structure 1500 and sets the batch size as the number of images defined by the data structure 1500. For instance, the system 300 selects fifty images based on determining that the criticality is very high and the context is in a crime-sensitive area or jewelry store, selects twenty images based on determining that the criticality is high and the context is in an airport or railway station, selects ten images based on determining that the criticality is medium and the context is in a public transport location, a public park, or vehicular surveillance, and selects five images based on determining that the criticality is low and the context is in home security or school campus security.
	US9076030	Liveness detection	While not illustrated in FIG. 4, in parallel with or after verifying that the face in the image sequence is a living face, the face is for example compared to the image of the identity document, and if there is a match, the user 102 is for example permitted to access a restricted area, for example an AIRPORT departure lounge.	While not illustrated in FIG. 4, in parallel with or after verifying that the face in the image sequence is a living face, the face is for example compared to the image of the identity document, and if there is a match, the user 102 is for example permitted to access a restricted area, for example an airport departure lounge. For example, the document checkpoint 100 of FIG. 1 may comprising an automatic barrier, that is opened only if a living face of the user 102 is found to match the registered image of the identity document.
	US10677932	Systems, methods, and devices for geo-localization	 Accordingly, small wearable devices can be used for navigating AIRPORT buildings, hospitals, malls, campus buildings, homes, or the like.	The embodiments described herein can provide seamless continuous navigation between indoor and outdoor environment. Specifically, GIS and BIM can be abstracted to map graphs that are used for navigation. The map graphs can be memory efficient. Thus, small low cost solutions can be provided by using low resolution sensors and map graphs on memory that are scaled to the size of the location being navigated. Additionally, the small solutions can be provided without requiring communication signals, i.e., internet or GPS, which can decrease power demands. Accordingly, small wearable devices can be used for navigating airport buildings, hospitals, malls, campus buildings, homes, or the like.
	WO2018193123A1	Detection system, detection device and method therefor	 More particularly, the invention relates to a system for detecting a passenger which may be used by a security agent, an airline agent, or other agent at an AIRPORT or other transportation hub such as a railway station, or bus station.	FIELD OF THE INVENTION This invention relates in general to an image processing system. More particularly, this invention relates to a system, apparatus, method or computer program for detecting an individual such as a customer or passenger, or a device for use by an agent. More particularly, the invention relates to a system for detecting a passenger which may be used by a security agent, an airline agent, or other agent at an airport or other transportation hub such as a railway station, or bus station. 
apartment				
	CN103269751B	Light therapy apparatus		
	US10867496	Methods and systems for presenting video feeds	 The present teachings are also applicable, without limitation, to duplexes, townhomes, multi-unit APARTMENT buildings, hotels, retail stores, office buildings, industrial buildings, and more generally any living space or work space.	It is to be appreciated that “smart home environments” may refer to smart environments for homes such as a single-family house, but the scope of the present teachings is not so limited. The present teachings are also applicable, without limitation, to duplexes, townhomes, multi-unit apartment buildings, hotels, retail stores, office buildings, industrial buildings, and more generally any living space or work space.
	US9672427	Systems and methods for categorizing motion events	 The present teachings are also applicable, without limitation, to duplexes, townhomes, multi-unit APARTMENT buildings, hotels, retail stores, office buildings, industrial buildings, and more generally to any living space or work space.	It is to be appreciated that “smart home environments” may refer to smart environments for homes such as a single-family house, but the scope of the present teachings is not so limited. The present teachings are also applicable, without limitation, to duplexes, townhomes, multi-unit apartment buildings, hotels, retail stores, office buildings, industrial buildings, and more generally to any living space or work space.
	US9544636	Method and system for editing event categories	 The present teachings are also applicable, without limitation, to duplexes, townhomes, multi-unit APARTMENT buildings, hotels, retail stores, office buildings, industrial buildings, and more generally any living space or work space.	It is to be appreciated that “smart home environments” may refer to smart environments for homes such as a single-family house, but the scope of the present teachings is not so limited. The present teachings are also applicable, without limitation, to duplexes, townhomes, multi-unit apartment buildings, hotels, retail stores, office buildings, industrial buildings, and more generally any living space or work space.
	US9224044	Method and system for video zone monitoring	 The present teachings are also applicable, without limitation, to duplexes, townhomes, multi-unit APARTMENT buildings, hotels, retail stores, office buildings, industrial buildings, and more generally any living space or work space.	It is to be appreciated that “smart home environments” may refer to smart environments for homes such as a single-family house, but the scope of the present teachings is not so limited. The present teachings are also applicable, without limitation, to duplexes, townhomes, multi-unit apartment buildings, hotels, retail stores, office buildings, industrial buildings, and more generally any living space or work space.
army				
	US10293485	Systems and methods for robotic path planning	 By way of illustration, the first control characteristic can correspond to a first wheel angle, first ARMY position, and/or any other control characteristic described in this disclosure.	In some cases, the actuator commands can take the form of adjusting actuators in accordance to control characteristics identified for robot 200 to travel along the first path portion. For example, robot 200 can generate an actuator command to effectuate the first control characteristic of a string of control characteristics of the first path portion (e.g., taking the control characteristic corresponding to the first arrow of trajectory 726, and/or any other trajectory, path, and/or path portion). In some cases, this first control characteristic can be the only control characteristic effectuated, wherein robot 200 then reassesses the path (e.g., finds another first path portion in accordance to method 400) after sending the actuator command comprising the first control characteristic. Advantageously, this can allow robot 200 to reassess the path after each move, giving greater potential for optimal path planning. In some cases, the first control characteristic can be a model predictive control. By way of illustration, the first control characteristic can correspond to a first wheel angle, first army position, and/or any other control characteristic described in this disclosure.
	US9888105	Intuitive computing methods and systems	 Forward the pixels to the “cloud” and have a vast ARMY of anonymous computers apply every known image recognition algorithm to the data until one finally identifies the depicted subject.	Hypothetically, the problem has a straightforward solution. Forward the pixels to the “cloud” and have a vast army of anonymous computers apply every known image recognition algorithm to the data until one finally identifies the depicted subject. (One particular approach would be to compare the unknown image with each of the billions of images posted to web-based public photo repositories, such as Flickr and Facebook. After finding the most similar posted photo, the descriptive words, or “metadata,” associated with the matching picture could be noted, and used as descriptors to identify the subject of the unknown image.) After consuming a few days or months of cloud computing power (and megawatts of electrical power), an answer would be produced.
	CN103763515A	Video anomaly detection method based on machine learning	Along with the development of Chinese society and the rise of electronic applications, video monitoring system has been widely used in all trades and professions very much, no longer be confined to the special dimensions such as public security, finance, bank, traffic, ARMY and port in the past, our daily life tentacle energy and community, office building, hotel, public place, factory, market, community, even family, has all installed video monitoring system.	Along with the development of Chinese society and the rise of electronic applications, video monitoring system has been widely used in all trades and professions very much, no longer be confined to the special dimensions such as public security, finance, bank, traffic, army and port in the past, our daily life tentacle energy and community, office building, hotel, public place, factory, market, community, even family, has all installed video monitoring system.But along with the continuous increase of application scenario CCTV camera quantity, the time of monitoring constantly extends, artificial more and more difficult to the maintenance of monitoring software in real time, so also more and more urgent for the intellectuality of monitoring software.
	US9202184	Optimizing the selection, verification, and deployment of expert resources in a time of chaos	 A cohort or unified group may be considered an entity rather than a group of individual skills, such as a fully functioning mobile ARMY surgical hospital (MASH) unit.	The needed skills are optimized based on requirements and constraints for expert services, a potential skills pool, cohorts of a related set of skills, and enabling resources. Optimization is the process of finding a solution that is the best fit based on the available resources and specified constraints. The solution is skills and resources that are available and is recognized as the best solution among numerous alternatives because of the constraints, requirements, and other circumstances and criteria of the chaotic event. A cohort or unified group may be considered an entity rather than a group of individual skills, such as a fully functioning mobile army surgical hospital (MASH) unit.
	CN102945374A	Method for automatically detecting civil aircraft in high-resolution remote sensing image	In recent years, along with the development of remote sensing technology and improving constantly of image resolution ratio, automatically detection not only has important application value in ARMY with the identification all types of target from high-resolution remote sensing image, also more and more becomes the focus that civil aviaton remote sensing field is paid close attention to.	All the time, to the detection of interesting target in the image and identification all be one important subject.In recent years, along with the development of remote sensing technology and improving constantly of image resolution ratio, automatically detection not only has important application value in army with the identification all types of target from high-resolution remote sensing image, also more and more becomes the focus that civil aviaton remote sensing field is paid close attention to.Wherein the aircarrier aircraft in the high-resolution remote sensing image detects automatically, as the important component part of target detection, because its important guiding effect at aspects such as city planning, airdrome controls, in also studying always and exploring.But generally, because the impact of the factors such as the size of the complicacy of high-resolution remote sensing image and airframe, contrast is wanted to guarantee higher verification and measurement ratio and be not easy under the less prerequisite of false-alarm.
baggage				
	US10148918	Modular shelving systems for package tracking	 Although shown in FIG. 2 as trucks, a delivery vehicle 202 may be any form of transport, including, but not limited to, an airplane, automobile, van, sea-going vessel, train, airplane BAGGAGE cart.	 FIG. 2 shows an example of an implementation of the package tracking system 100 (FIG. 1) within a delivery system 200. For illustration purposes, the delivery system 200 includes multiple delivery vehicles 202-1, 202-n (generally, 202) and scanners 124-1, 124-n (generally, 124) used by personnel to obtain package identification information from packages. Although shown in FIG. 2 as trucks, a delivery vehicle 202 may be any form of transport, including, but not limited to, an airplane, automobile, van, sea-going vessel, train, airplane baggage cart. The delivery vehicles 202 and scanners 124 are in communication with a central server (or servers) 204 over communication connections 206. The server 204 (or servers) can be cloud based, meaning that a provider of the server 204 makes applications, services, and resources available on demand to users over a network (e.g., the Internet). The communication connections 206 may be established using any type of communication system including, but not limited to, a cellular network, private network, local network, wired network, wireless network, or any combination thereof.
	US9875392	System and method for face capture and matching	, via another image capture device 102) as they exit the customs area, allowing for the calculation of the BAGGAGE retrieval and customs process time, and the total arrivals process time.	Referring to FIG. 7, a more detailed example 160 of passenger timing is illustrated. At Position 1 at an air bridge, as passengers step outside the aircraft and walk through the jetty, their face may be captured for the first time by the image capture device 102, and stored in the enrolled identities database 115. At Position 2, at the immigration hall entrance, passengers' faces may be captured (e.g., via another image capture device 102) as they enter the immigration hall, giving the dwell time for transit from gate to hall, and providing for the calculation of time to clear immigration. At Position 3, at automated border clearance (ACS) gates, the photos from the ACS gates may be used to monitor the immigration clearance time of passengers using the ACS gates compared to the overall passenger flow, without additional image capture hardware being needed. At Position 4, at the immigration hall exit, passengers' faces may be captured (e.g., via another image capture device 102) as they clear immigration, allowing for the calculation of the dwell time for the immigration process. At Position 5, at the arrivals hall, passengers' faces may be captured (e.g., via another image capture device 102) as they exit the customs area, allowing for the calculation of the baggage retrieval and customs process time, and the total arrivals process time.
	US9355308	Auditing video analytics through essence generation	 Human auditors generally make fewer mistakes than results obtained by auditing through filtering by machine intelligence, and thus human auditors may provide better efficiencies in applications where true events may occur infrequently, such as in retail fraud detection or abandoned BAGGAGE alerts, and still other examples will be apparent to one skilled in the art.	Visual essences are generally compact and/or condensed so that they require less bandwidth than video data for visualization in an auditing system; only frames or limited video selections need be transmitted to the auditor for review, not a much larger or entire video feed as is typically required in the prior art. Smaller visual essences (for example, discrete data files) may also be more readily downloaded to, and stored in local machines for faster system responses compared to larger video feeds. Further, as analysts need only watch summaries about an event, pruning operations are faster and their throughput rate may be higher. Human auditors generally make fewer mistakes than results obtained by auditing through filtering by machine intelligence, and thus human auditors may provide better efficiencies in applications where true events may occur infrequently, such as in retail fraud detection or abandoned baggage alerts, and still other examples will be apparent to one skilled in the art.
	WO2018193123A1	Detection system, detection device and method therefor		
	EP3113070A1	Method and system for optical user recognition		
caste				
	EP2263598B2	Method for fabricating a plurality of dental incremental position adjustment appliances		
	CN107292314A	A kind of lepidopterous insects species automatic identification method based on CNN		
	US6858826	Method and apparatus for scanning three-dimensional objects		
citizen				
	WO2017160469A1	Visual perception determination system and method	 [0001] As small and inexpensive imaging devices become more readily available, there is an increasing occurrence of video recording by ordinary CITIZENs of interactions involving, for example, law enforcement officers and alleged criminals.	 [0001] As small and inexpensive imaging devices become more readily available, there is an increasing occurrence of video recording by ordinary citizens of interactions involving, for example, law enforcement officers and alleged criminals. Additionally, many state and local law enforcement jurisdictions already mandate or encourage officers to use imaging devices during the course of their duties. For example, dashboard-mounted imaging devices are widely used. In many instances, such imaging devices record images of activity located at the front of the vehicle from a vantage point on the dashboard looking outward through the vehicle windshield and past the vehicle hood. The use of wearable or body-mounted imaging devices is growing and such imaging devices may capture images from a vantage point that matches the location at which the imaging device is worn, which may vary. Video or other images may also be available from security, traffic, and other imaging devices. Thus, there is an increasing prevalence of video or still images available for use in evidentiary and other judicial proceedings. However, the scene shown in various recorded video or still images may not always correspond to the scene as perceived by a person involved in the recorded incident at the time of video or still image capture. 
	US10698995	Method to verify identity using a previously collected biometric image/data	11 a,b,g,n, wireless LAN, WMAN, broadband fixed access, WiMAX, any cellular technology including CDMA, GSM, EDGE, 3G, 4G, 5G, TDMA, AMPS, FRS, GMRS, CITIZEN band radio, VHF, AM, FM, and wireless USB.	It is contemplated that the mobile device, and hence the first wireless transceiver 240 and a second wireless transceiver 244 may be configured to operate according to any presently existing or future developed wireless standard including, but not limited to, Bluetooth, WI-FI such as IEEE 802.11 a,b,g,n, wireless LAN, WMAN, broadband fixed access, WiMAX, any cellular technology including CDMA, GSM, EDGE, 3G, 4G, 5G, TDMA, AMPS, FRS, GMRS, citizen band radio, VHF, AM, FM, and wireless USB.
	US9602738	Automatic event detection, text generation, and use thereof	 In particular, the automated event detection and text generation combined with the video insertion and/or geographical information and universal time aspects of the disclosed embodiments provides for high speed, pinpointed, and seamless search and retrieval for information such as video surveillance, which is elemental in providing safety for CITIZENs in many different situations.	The embodiments described above improve existing video surveillance systems by providing automated, intuitive methods for reviewing and searching for events captured in video. In particular, the automated event detection and text generation combined with the video insertion and/or geographical information and universal time aspects of the disclosed embodiments provides for high speed, pinpointed, and seamless search and retrieval for information such as video surveillance, which is elemental in providing safety for citizens in many different situations. The embodiments described above can be used for various fields. For example, in video surveillance, they can be used to detect potential criminal or terrorist activities, to monitor and improve traffic design, or for general investigation of events of interest. The embodiments can also be used in marketing and research fields, and in urban planning environments, for example, to monitor activity in different parts of a city, and plan for future projects.
	EP2863338A2	Delayed vehicle identification for privacy enforcement		
	US10515379	Computerized detection and semantic characterization of trends in digital media content	” The latent space is “cross-modal, in the sense that embedded vectors representing visual and textual information are treated as the same class of CITIZENs and thus image-to-image, text-to-image, and image-to-text retrieval tasks can in principle all be handled in exactly the same way.	 FIG. 5 is a flowchart illustrating operation of an embodiment of image selection. In the embodiment shown in FIG. 5, image feature detection is performed using a technique known as Kernalized Canonical Correlation Analysis (KCCA), as described by Yunchao Gong, Qifa Ke, Michael Isard, and Svetlana Lazebnik, in “A multi-view embedding space for modeling internet images, tags, and their semantics”, International Journal of Computer Vision, 106(2):210-233, 2014. As Gong et al describe, Canonical Correlation Analysis (CCA) is a “technique that maps two view, given by visual and textual features, into a common latent space where the correlation between the two views is maximized.” The latent space is “cross-modal, in the sense that embedded vectors representing visual and textual information are treated as the same class of citizens and thus image-to-image, text-to-image, and image-to-text retrieval tasks can in principle all be handled in exactly the same way.” Gong et al. describe a modified KCCA technique in which a scalable approximation scheme based on efficient explicit kernel mapping is employed followed by linear dimensionality reduction and linear CCA. A benefit of using KCCA to perform image feature detection is that it can address the problem of missing tags as noted above. With KCCA, a tag missing for an image may come from another image.
combat				
	US9055200	Content delivery based on a light positioning system	 To COMBAT this, lighting manufactures design flicker above 200 Hz into their lighting products.	The modulation frequency of the light source is highly dependent on the receiving circuitry. While incandescent and fluorescent technologies generally do not “flicker” on and off during the course of normal operation, LED lighting sources are sometimes designed to flicker above the rate which the eye can see in order to increase their longevity, and consume less power. Most humans cannot see flicker above 60 Hz, but in rare instances can perceive flicker at 100 Hz to 110 Hz. To combat this, lighting manufactures design flicker above 200 Hz into their lighting products.
	US9465980	Pose tracking pipeline	 In another example, the motion of a player holding an object may be tracked and utilized for controlling an on-screen weapon in an electronic COMBAT game.	In some embodiments, a target may include a human and an object. In such embodiments, for example, a player of an electronic game may be holding an object, such that the motions of the player and the object are utilized to adjust and/or control parameters of the electronic game. For example, the motion of a player holding a racket may be tracked and utilized for controlling an on-screen racket in an electronic sports game. In another example, the motion of a player holding an object may be tracked and utilized for controlling an on-screen weapon in an electronic combat game.
	US9462253	Optical modules that reduce speckle contrast and diffraction artifacts	 In another example embodiment, the motion of a player holding an object may be tracked and utilized for controlling an on-screen weapon in an electronic COMBAT game.	In example embodiments, the human target such as the user 118 may have an object. In such embodiments, the user of an electronic game may be holding the object such that the motions of the player and the object may be used to adjust and/or control parameters of the game. For example, the motion of a player holding a racket may be tracked and utilized for controlling an on-screen racket in an electronic sports game. In another example embodiment, the motion of a player holding an object may be tracked and utilized for controlling an on-screen weapon in an electronic combat game. Objects not held by the user can also be tracked, such as objects thrown, pushed or rolled by the user (or a different user) as well as self-propelled objects. In addition to boxing, other games can also be implemented.
	US10237489	Method and system for configuring an imaging device for the reception of digital pulse recognition information	 To COMBAT this, lighting manufactures design flicker above 200 Hz into their lighting products.	The modulation frequency of the light source is highly dependent on the receiving circuitry. While incandescent and fluorescent technologies generally do not “flicker” on and off during the course of normal operation, LED lighting sources are sometimes designed to flicker above the rate which the eye can see in order to increase their longevity, and consume less power. Most humans cannot see flicker above 60 Hz, but in rare instances can perceive flicker at 100 Hz to 110 Hz. To combat this, lighting manufactures design flicker above 200 Hz into their lighting products.
	US10159411	Detecting irregular physiological responses during exposure to sensitive data	, sports, motorcycle, bicycle, and/or COMBAT helmets) and/or a brainwave-measuring headset.	Sentences in the form of “a frame configured to be worn on a user's head” or “a frame worn on a user's head” refer to a mechanical structure that loads more than 50% of its weight on the user's head. For example, an eyeglasses frame may include two temples connected to two rims connected by a bridge; the frame in Oculus Rift™ includes the foam placed on the user's face and the straps; and the frames in Google Glass™ and Spectacles by Snap Inc. are similar to eyeglasses frames. Additionally or alternatively, the frame may connect to, be affixed within, and/or be integrated with, a helmet (e.g., sports, motorcycle, bicycle, and/or combat helmets) and/or a brainwave-measuring headset.
crime				
	US9483689	Biometric matching technology	 For instance, when the context is a high CRIME area and the criticality of the situation is relatively high, the system 300 sets a relatively large batch size because the circumstances justify the expense of processing more images in parallel.	The system 300 considers the context and/or criticality and sets the batch size based on the context and/or criticality. The system 300 sets the batch size as appropriate for the context and/or criticality. For instance, when the context is a high crime area and the criticality of the situation is relatively high, the system 300 sets a relatively large batch size because the circumstances justify the expense of processing more images in parallel. When the context is a low crime area and the criticality of the situation is relatively low, the system 300 sets a relatively small batch size because the circumstances do not justify the expense of processing more images in parallel. To determine the batch size, the system 300 may reference a look-up table that stores batch sizes to use for the various possible values (or value ranges) for the context and/or criticality used by the system 300 to set the batch size. The system 300 may store the look-up table in electronic storage based on user input provided by an operator of the system 300.
	US9189886	Method and apparatus for estimating body shape	 For CRIME scene video containing clothed subjects, this provides important evidence beyond standard methods.	The parametric shape model can be recovered for people wearing clothing and used to extract biometric measurements such as subject height and weight (Section 10). For crime scene video containing clothed subjects, this provides important evidence beyond standard methods. Body shape can also be used for persistent surveillance. By identifying the shape of people in images, they can be tracked over time and when they leave and re-enter the scene, their body shape can be used to reestablish tracking and determine identity among a group of people using the shape distance score.
	US9214021	Distributed position identification	 As yet another example, robot 214 may include a robot that is used for military or CRIME prevention applications.	As another example, mobile platform 204 be robot 214. For example, without limitation, robot 214 may include an industrial robot or another type of robot that may be used for the manufacture, assembly, inspection, or testing of products, or for any combination of these or other functions. For example, without limitation, robot 214 may be an overhead crane. As another example, robot 214 may include a robot that is used for scientific purposes, such as deep sea or planetary exploration, or for operation in any other environment. As yet another example, robot 214 may include a robot that is used for military or crime prevention applications.
	US10860683	Pattern change discovery between high dimensional data sets	 For example, in financial applications, if we consider the customer daily transactional data as the time-series data, we may apply this invention to detect any abnormal transactional patterns that may indicate potential CRIMEs such as money laundering.	For time evolving data streams, self-adaptive algorithms can be further applied. There is abundant literature on this topic (See [154][155][156][157]). This technology can be applied to many important areas. For example, in financial applications, if we consider the customer daily transactional data as the time-series data, we may apply this invention to detect any abnormal transactional patterns that may indicate potential crimes such as money laundering. When the invention is used to daily sales data on the other hand, we are able to identify new customer interests. As yet another example, if we consider the daily online transactional data, we may be able to use this technology to discover new business models.
	US10861163	System and method for identification and suppression of time varying background objects	Surveillance security systems have been traditionally used to help protect people, property, and reduce CRIME for homeowners and businesses alike and have become an increasingly cost-effective tool to reduce risk.	Surveillance security systems have been traditionally used to help protect people, property, and reduce crime for homeowners and businesses alike and have become an increasingly cost-effective tool to reduce risk. These security systems are used to monitor buildings, lobbies, entries/exits, and secure areas within the buildings, to list a few examples. The security systems also identify illegal activity such as theft or trespassing, in examples.
criminal				
	US10679047	System and method for pose-aware feature learning	In particular, the present invention may have the capability of searching for persons such as CRIMINAL suspects or missing people based on the clothing that the persons are wearing.	In particular, the present invention may have the capability of searching for persons such as criminal suspects or missing people based on the clothing that the persons are wearing. For example, in a particular embodiment, a user may query the present invention with “show me all people with a black down jacket and denim pants from time x to time y, considering all cameras in the downtown area”, and in response to the query, extract images from the video recorded by those cameras which satisfy the query, and present the user with the extracted images.
	US9483689	Biometric matching technology	 The watch list may include CRIMINALs that a government agency is trying to locate, missing persons, persons blacklisted from an establishment, or any type of persons of interest that an organization would like to locate.	The system 300 manages a watch list (410). For instance, the system 300 manages a watch list of persons of interest that includes biometric data (e.g., a face image) for each of the persons of interest. The watch list may include criminals that a government agency is trying to locate, missing persons, persons blacklisted from an establishment, or any type of persons of interest that an organization would like to locate.
	CN106599910B	Mimeograph documents discrimination method based on texture recombination		
	CN105023001A	Selective region-based multi-pedestrian detection method and system	Target detection is a focus in computer vision, is applied in CRIMINAL investigation monitoring, specific objective retrieval, robotics and intelligent vehicle widely; Pedestrian is again particularly important factor in target detection, and therefore pedestrian detection causes and pays much attention to and research in the time in recent years.	"Target detection is a focus in computer vision, is applied in criminal investigation monitoring, specific objective retrieval, robotics and intelligent vehicle widely; Pedestrian is again particularly important factor in target detection, and therefore pedestrian detection causes and pays much attention to and research in the time in recent years.But the many attitude change of illumination under different scene, noise and pedestrian makes the research of pedestrian detection face very large challenge.Pedestrian detection algorithm most is at present all at document ("" Histogramsof oriented gradients for human detection.In Computer Vision and Pattern Recognition "" by means of Dalal-Triggs, 2005.CVPR 2005.IEEE Computer Society Conference on, volume 1, pages 886-893.) the middle HOG feature proposed.Through years of researches, this field achieves very large improvement.At document (the Piotr Dollar of Piotr Dollar, ChristianWojek, Bernt Schiele, and Pietro Perona.Pedestrian detection:An evaluation of the state of the art.Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34 (4): 743-761, 2012.) can find in the investigation of current 16 kinds of pedestrian detection algorithms comparison in, still two problems is there is to such an extent as to verification and measurement ratio is low: a large amount of flase drops being exhaustive scanning and bringing in pedestrian's testing process, another is the low problem of verification and measurement ratio under circumstance of occlusion.Testing process comprises two stages of training and testing: the first, from training image, extract feature, and wherein feature contains color, the base attribute information such as texture and profile of pedestrian, the features training of extraction is gone out SVM classifier; The second, from test set image, extract feature, these features are sent to the sorter trained, finally provide classification results.Can find out from the testing result of HOG feature, there is most flase drop is that current techniques is very scabrous; The residual error ratio that existence is blocked under environment is more serious."
	US10698995	Method to verify identity using a previously collected biometric image/data	 In this manner, a CRIMINAL that has stolen a credit card and attempts to use the card from a distant location (as compared to the retail location) is unable to complete a transaction because the user's phone is not at the location of the retail establishment.	According to one embodiment, as an additional measure of security, the GPS information from the mobile device may also be sent to the authentication server to authenticate and allow the retail transaction. For example, the GPS coordinates from the mobile device may be compared with the coordinates of the retail establishment to confirm that the user is actually present in the retail establishment. In this manner, a criminal that has stolen a credit card and attempts to use the card from a distant location (as compared to the retail location) is unable to complete a transaction because the user's phone is not at the location of the retail establishment. IP addresses may also be used to determine location.
defense				
	CN107113381A	The tolerance video-splicing that space-time local deformation and seam are searched		
	US9808549	System for detecting sterile field events and related methods	The primary DEFENSE against such infections is deceptively simple: avoid contact events between any potentially contaminated surfaces and sterile surfaces.	The primary defense against such infections is deceptively simple: avoid contact events between any potentially contaminated surfaces and sterile surfaces. Simulation based training has been shown to help [14] but it is very difficult to maintain the sterile field, and to have confidence that you have done so. One has to prepare the area, unpack the catheter kit (FIG. 3, 40), prepare the patient, and perform the procedure on a moving patient. It is relatively easy to violate the sterile field, and not even know it. Worse, the violations can occur through a chain of contacts that can transmit unseen pathogens (e.g., bacteria, viruses, fungi, and parasites) from one surface to another, where the final transmission of the pathogen to the patient happens during what would otherwise seem like a sterile contact. For example, one might unknowingly contact a non-sterile surface with the back of a hand (sterile glove), then scratch that spot using the fingers of the other hand (sterile glove), and then touch the catheter.
	EP3599575A1	Learning an autoencoder		
	US10201752	System for interactive sports analytics using multi-template alignment and discriminative clustering	 With respect to FIG. 9a , a basketball game state is shown with ten players positioned on the court, five on offense and five on DEFENSE.	The system also permits interactive statistical analysis by the user based on a graphical representation of the game players and trajectories. For example, the system allows a user to specify a current play-of-interest (such as by selecting the play from a list of exemplar plays, or by manipulating graphical objects on a screen to represent the play) as a query to the database of plays. Using statistical information associated with the plays in the database, the system can present a statistical probability for a particular event occurring in the queried state. With respect to FIG. 9a , a basketball game state is shown with ten players positioned on the court, five on offense and five on defense. The system queries the database using the game situation (or a subset of the elements of the game situation), and displays a statistical probability of a successful outcome for players in that situation (e.g., 34% for Brown), based on similarly situated plays in the database that were retrieved in response to the query. Other types of statistical event probabilities (e.g., pass probability, foul probability, turnover probability, etc.) can also be estimated and presented based on the query.
	US10691133	Adaptive and interchangeable neural networks	, in military DEFENSE applications such as missiles and drones), and etc.	The automated machine can broadly refer to a machine that is to be controlled by a control mechanism, with some human intervention if necessary. Examples of an automated machine can be appliances (e.g., ovens, refrigerators) with automated controllers (e.g., Internet of Things, “IoT” controllers), a speech generator, a speech recognition system, a facial recognition system, an automated personal assistant (e.g., Alexa by Amazon, Inc., Ski by Apple, Inc.), an autonomous vehicle, a robot, a target recognition system (e.g., in military defense applications such as missiles and drones), and etc. Also, an automated machine does not necessarily mean a completely automated manual-less machine that requires no human intervention, but it may require a qualified person to take over the control (e.g., driving) under certain circumstances.
disability				
	US10849532	Computer-vision-based clinical assessment of upper extremity function	Stroke is the leading cause of serious chronic physical DISABILITY in the United States, with 95% of stroke survivors exhibiting some Upper Extremity (UE) dysfunction and 30 to 66% with impaired ability to use the impaired arm.	Stroke is the leading cause of serious chronic physical disability in the United States, with 95% of stroke survivors exhibiting some Upper Extremity (UE) dysfunction and 30 to 66% with impaired ability to use the impaired arm. This has a substantial impact on the healthcare system and allocation of resources for an estimated 7.2 million affected Americans ≥20 years of age, and 610,000 new cases every year. Standardized routine Outcome Measures (OMs) of UE impairment are critical for driving clinical decisions about rehabilitation protocols and monitoring progression of sensorimotor deficits.
	US10475127	Methods of providing insurance savings based upon telematics and insurance incentives	The insurance policy may be an automobile insurance policy or another type of insurance policy, such as a life insurance policy, a health insurance policy, a DISABILITY insurance policy, an accident insurance policy, a homeowners insurance policy, a renters insurance policy, and/or an excess liability insurance policy.	The insurance policy may be an automobile insurance policy or another type of insurance policy, such as a life insurance policy, a health insurance policy, a disability insurance policy, an accident insurance policy, a homeowners insurance policy, a renters insurance policy, and/or an excess liability insurance policy.
	US10349887	Blood pressure measuring smartglasses	Elevated blood pressure is a significant cause of death and DISABILITY in the world.	Elevated blood pressure is a significant cause of death and disability in the world. Attaining accurate blood pressure measurements is vital in the prevention and treatment of various blood-pressure-related diseases. However, continuous monitoring with existing blood pressure monitors (e.g., cuff-based devices) can be difficult, uncomfortable, and impractical to perform in real-world settings (e.g., at work, while commuting, etc.). Thus, there is a need to a way to continuously monitor blood pressure in a comfortable way.
	US9685174	Mood monitoring of bipolar disorder using speech analysis	 Bipolar disorder is among the leading causes of DISABILITY worldwide.	Bipolar disorder (BP) is a common and severe psychiatric illness characterized by pathological swings of mania and depression and is associated with devastating personal, social, and vocational consequences (suicide occurs in up to 20% of cases, by some reports). Bipolar disorder is among the leading causes of disability worldwide. The cost in the United States alone was estimated at $45 billion annually (in 1991 dollars). These economic and human costs, along with the rapidly increasing price of health care provide the impetus for a major paradigm shift in health care service delivery, namely to monitor and prioritize care with a focus on prevention.
	US10776423	Motor task analysis system and method	0, indicative of a neurostatus score, a DISABILITY status scale, and/or an expanded DISABILITY status scale.	The method 1000 may also include calculating a disease severity score based on at least one of the motion descriptors. The disease severity may be assessed based at least in part on the disease severity score. In one embodiment, the disease severity score may have a value in a range from about 0.0 to about 12.0, indicative of a neurostatus score, a disability status scale, and/or an expanded disability status scale. In some embodiments, a disease severity score may be scaled to have any desired range of values, such as between 0 and 100, 0 and 10, 1 and 10, etc.
enemy				
	US9841602	Location indicating avatar in head worn computing	 Similarly, ENEMY fire traces 2104 may be color coded or otherwise distinguished on the displayed digital content.	In embodiments, certain user positions may be known and thus identified in the FOV. For example, the shooter of the friendly fire trace 2108 may be from a known friendly combatant and as such his location may be known. The position may be known based on his GPS location based on a mobile communication system on him, such as another HWC 102. In other embodiments, the friendly combatant may be marked by another friendly. For example, if the friendly position in the environment is known through visual contact or communicated information, a wearer of the HWC 102 may use a gesture or external user interface 104 to mark the location. If a friendly combatant location is known the originating position of the friendly fire trace 2108 may be color coded or otherwise distinguished from unidentified traces on the displayed digital content. Similarly, enemy fire traces 2104 may be color coded or otherwise distinguished on the displayed digital content. In embodiments, there may be an additional distinguished appearance on the displayed digital content for unknown traces.
	US10699719	System and method for taxonomically distinguishing unconstrained signal data segments	 For example, certain combinations of features may signify suitable landing zones in a particular region of engagement, or certain mountain terrain features such as draws may signify key areas of ENEMY activity.	In addition to matching established terrain classes, the subject system and method may be suitably applied by end-users to identify their own signature classes of interest. For example, certain combinations of features may signify suitable landing zones in a particular region of engagement, or certain mountain terrain features such as draws may signify key areas of enemy activity. Training examples may be highlighted by the user, such that similar structures may be then automatically identified in a local area. In another application, certain salient ground features may be identified for use as referential waypoints or landmarks in small dismounted ground unit navigation.
	US10698223	See-through computer display systems	 Similarly, ENEMY fire traces 2104 may be color coded or otherwise distinguished on the displayed digital content.	In embodiments, certain user positions may be known and thus identified in the FOV. For example, the shooter of the friendly fire trace 2108 may be from a known friendly combatant and as such his location may be known. The position may be known based on his GPS location based on a mobile communication system on him, such as another HWC 102. In other embodiments, the friendly combatant may be marked by another friendly. For example, if the friendly position in the environment is known through visual contact or communicated information, a wearer of the HWC 102 may use a gesture or external user interface 104 to mark the location. If a friendly combatant location is known the originating position of the friendly fire trace 2108 may be color coded or otherwise distinguished from unidentified traces on the displayed digital content. Similarly, enemy fire traces 2104 may be color coded or otherwise distinguished on the displayed digital content. In embodiments, there may be an additional distinguished appearance on the displayed digital content for unknown traces.
	US9958674	Eye imaging in head worn computing	 Similarly, ENEMY fire traces 2104 may be color coded or otherwise distinguished on the displayed digital content.	In embodiments, certain user positions may be known and thus identified in the FOV. For example, the shooter of the friendly fire trace 2108 may be from a known friendly combatant and as such his location may be known. The position may be known based on his GPS location based on a mobile communication system on him, such as another HWC 102. In other embodiments, the friendly combatant may be marked by another friendly. For example, if the friendly position in the environment is known through visual contact or communicated information, a wearer of the HWC 102 may use a gesture or external user interface 104 to mark the location. If a friendly combatant location is known the originating position of the friendly fire trace 2108 may be color coded or otherwise distinguished from unidentified traces on the displayed digital content. Similarly, enemy fire traces 2104 may be color coded or otherwise distinguished on the displayed digital content. In embodiments, there may be an additional distinguished appearance on the displayed digital content for unknown traces.
	US10269148	Real-time image undistortion for incremental 3D reconstruction	, military observation of a region to locate an ENEMY or ascertain strategic features), disaster relief, or other time-sensitive operations.	Embodiments generate a 3D reconstruction as a textured model, in real or near-real time as the imagery is obtained, and place that model in a 3D visualization of the earth from live imagery (e.g., still images or video) from a manned or unmanned vehicle. Accordingly, embodiments allow for fast decision making by providing rapid feedback for inspection (e.g., in commercial applications), reconnaissance (i.e., military observation of a region to locate an enemy or ascertain strategic features), disaster relief, or other time-sensitive operations.
ethnicity				
	US8884980	System and method for changing hair color in digital images	 At least one characteristic capable of affecting the result of the hair color product may be obtained, such as a hair texture, an ETHNICITY, a starting hair color, or any other characteristic capable of affecting the result of a hair color product on a subject.	A hair color product associated with the target hair color may be displayed, including a product image, product name, product brand, product color, or any other product information. The output image may represent a simulation of the hair color product on the subject. At least one characteristic capable of affecting the result of the hair color product may be obtained, such as a hair texture, an ethnicity, a starting hair color, or any other characteristic capable of affecting the result of a hair color product on a subject. At least one of the target color distribution and the color transformation may be modified based on the at least one characteristic. The at least one characteristic may be obtained by extracting one or more characteristics from the starting image.
	US10140521	Method, system and apparatus for processing an image	The appearance distance of the head segment in equation (5) is treated with a lower power than the appearance distance of the shoulder segment because the appearance of the head segment is less consistent (as it can appear as either face or hair depending on the direction of the head with respect to the camera) and less discriminative (as people of the same ETHNICITY often have similar skin colour).	The appearance distance of the head segment in equation (5) is treated with a lower power than the appearance distance of the shoulder segment because the appearance of the head segment is less consistent (as it can appear as either face or hair depending on the direction of the head with respect to the camera) and less discriminative (as people of the same ethnicity often have similar skin colour).
	US9483689	Biometric matching technology	 The image processor 230 also may determine the angle, gender, ETHNICITY, and any other detectable attributes of a person within the one or more images based on an analysis of the one or more images.	The image processor 230 processes the one or more images to determine attributes of the images and/or persons within the one or more images. For instance, the image processor 230 may compute the distance between a person within an image and the camera 210 based on an analysis of the captured image. The image processor 230 also may determine the angle, gender, ethnicity, and any other detectable attributes of a person within the one or more images based on an analysis of the one or more images. The image processor 230 also may determine general characteristics (e.g., blurriness, etc.) of the one or more images based on an analysis of the one or more images.
	US10546417	Method and apparatus for estimating body shape	 to infer ETHNICITY).	In many applications, the gender of a person being scanned may be known or the user may specify that information. In these cases, body shape using the appropriate gender-specific body model is estimated (Section 3) When gender is not known there are several options. One can fit a gender-neutral body model that is capable of representing male or female bodies. Second, one can fit using both male and female body shape models and select the one that achieves a lower error of the objective function. Third, one can fit a gender-neutral model and then classify gender directly from the estimated shape coefficients, as described in Section 10. Once gender is known, a refined shape estimate using the appropriate gender-specific shape model is produced. The same strategies can be used for other subpopulations (e.g. to infer ethnicity).
	WO2018175357A1	Methods for age appearance simulation	 The present application relates generally to methods for age appearance simulation and more specifically to utilizing ETHNICITY, age, and gender inputs to predict face shape, color, and/or texture for simulating an age of an individual using statistical models.	 The present application relates generally to methods for age appearance simulation and more specifically to utilizing ethnicity, age, and gender inputs to predict face shape, color, and/or texture for simulating an age of an individual using statistical models. 
face				
	US9020210	Image processing system, image processing apparatus, image processing method, and program	A technique exists which applies a manipulation to a FACE region recognized from an image by using a FACE recognition technique.	A technique exists which applies a manipulation to a face region recognized from an image by using a face recognition technique. In the case of manipulation using a face recognition technique, although only a face region is subject to manipulation in many cases, for example, a manipulation can be applied to the body portion through application of a rule that a body exists below a face region. However, depending on the pose of a human, such manipulation using a face recognition technique often results in a manipulated image that looks unnatural.
	CN106952301A	A kind of RGB D saliency computational methods		
	US10679044	Human action data set generation in a machine learning system	 Large data sets, such as thousands or millions of photos/videos of objects, animals, FACEs, or scenes, are required to enable complex neural networks to train successfully.	As described above, advanced machine learning techniques and training data require computational power and deep learning networks. Large data sets, such as thousands or millions of photos/videos of objects, animals, faces, or scenes, are required to enable complex neural networks to train successfully. However, in some cases, only small data sets are available, which does not provide enough data for a machine learning process. Common methods for generating additional data sets to be used for training data are limited by the number of subjects or environments available or by the similarity between an existing image/video and a desired image/video.
	US9262674	Orientation state estimation device and orientation state estimation method	For the method of recognizing an object of interest within the image, one may employ a technique involving: creating strong classifiers by combining the sums of a plurality of weak classifiers based on rectangular information through AdaBoost; combining the strong classifiers in a cascade; and recognizing a FACE as an object of interest within the image, for example.	For the method of recognizing an object of interest within the image, one may employ a technique involving: creating strong classifiers by combining the sums of a plurality of weak classifiers based on rectangular information through AdaBoost; combining the strong classifiers in a cascade; and recognizing a face as an object of interest within the image, for example. For the image features, scale-invariant feature transform (SIFT) features may be employed (e.g., see NPL 2), for example. SIFT features are configured with 128-dimensional vectors, and are values that are computed for each pixel. Because SIFT features are unaffected by scale changes, rotation, or translation of the object to be detected, they are particularly effective for detecting parts that are rotatable in various directions, e.g., the arms. In other words, SIFT features are suited for the present embodiment which defines posture states through the relative joint positions and angles of two or more parts of interest.
	US10540749	System and method for learning-based image super-resolution	 For example, FACE upsampling or a FACE super-resolution is the task of generating a high-resolution FACE image from a low-resolution input image of the FACE.	Super-resolution is a task aiming to produce a high-resolution image from a low-resolution image. For example, face upsampling or a face super-resolution is the task of generating a high-resolution face image from a low-resolution input image of the face. The face upsampling has widespread application in surveillance, authentication and photography. Face upsampling is particularly challenging when the input face resolution is very low (e.g., 12×12 pixels), the magnification rate is high (e.g. 8×), and/or the face image is captured in an uncontrolled setting with pose and illumination variations.
facial				
	US9148463	Methods and systems for improving error resilience in video delivery	 For example, the Microsoft® Kinect camera system generates detailed models of both a subject's face (detecting over 100 FACIAL landmarks) and body position (skeletal joints) in less than a frame time.	In recent years, there have been advances in computer analysis and modeling of the human body. For example, the Microsoft® Kinect camera system generates detailed models of both a subject's face (detecting over 100 facial landmarks) and body position (skeletal joints) in less than a frame time. Additionally, other systems are also available with varying degrees of analysis capability.
	US10586372	Online modeling for real-time facial animation	 No. 15/641,428, entitled “Online Modeling For Real-Time Facial Animation,” which further is a continuation of, and claims priority to, U.	This is a continuation of, and claims priority to, U.S. patent application Ser. No. 15/641,428, entitled “Online Modeling For Real-Time Facial Animation,” which further is a continuation of, and claims priority to, U.S. patent application Ser. No. 15/167,966, entitled “Online Modeling For Real-Time Facial Animation,” which further is a continuation of, and claims priority to, U.S. patent application Ser. No. 13/912,378, all of which are hereby incorporated by reference in their entirety.
	US9483689	Biometric matching technology	, fingerprints, retina scans, FACIAL images, etc.	A typical biometric matching system includes a database of biometric information (e.g., fingerprints, retina scans, facial images, etc.) about individuals. To identify or authenticate a sample of biometric information, the typical biometric matching system compares the sample with entries in the database one by one until a match is found. As a result, the time to find a matching entry grows linearly and may be time consuming when the database includes many entries.
	US7426292	Method for determining optimal viewpoints for 3D face modeling and face recognition	, “Realistic Modeling for Facial Animations,” Proceedings of SIGGRAPH 95, pages 55-62, August, 1995, or the availability of high quality of texture images as a substitute for exact face geometry, see Guenter et al.	In computer graphics, it is still a fundamental problem to synthetically construct realistic human heads, particularly the face portion. Hereinafter, when referring to ‘head’ or ‘face’, the invention is most interested in that portion of the head extending from chin-to-brow, and ear-to-ear. Most prior art methods require either extensive manual labor by skilled artists, expensive active 3D scanners, Lee et al., “Realistic Modeling for Facial Animations,” Proceedings of SIGGRAPH 95, pages 55-62, August, 1995, or the availability of high quality of texture images as a substitute for exact face geometry, see Guenter et al., “Making Faces,” Proceedings of SIGGRAPH 98, pages 55-66, July 1998, Lee et al., “Fast Head Modeling for Animation,” Image and Vision Computing, Vol. 18, No. 4, pages 355-364, March 2000, Tarini et al., “Texturing Faces,” Proceedings Graphics Interface 2002, pages 89-98, May 2002.
	US9076703	Method and apparatus to use array sensors to measure multiple types of data at full resolution of the sensor	, FACIAL recognition) in the images and finds task-relevant objects to modified or augmented (such as in augmented reality).	Computer vision has become a field that is integral to different imaging applications running on several devices that are used daily. Computer vision goes to the next step after imaging, which is using the information in the image to achieve different tasks defined by the user. For example, computer vision applications find faces (e.g., facial recognition) in the images and finds task-relevant objects to modified or augmented (such as in augmented reality). A prime example of these systems are the Human Natural User Interfaces (HNUI). These interfaces are used in many devices that are becoming more and more adopted by users.
facial recognition				
	US9483689	Biometric matching technology	In some implementations, a multi-dimensional approach that leverages FACIAL RECOGNITION technology is used to identify a suspect in a crowd in real time.	In some implementations, a multi-dimensional approach that leverages facial recognition technology is used to identify a suspect in a crowd in real time. The approach converts a sequential face matching process to a parallel process, leveraging flexible computing and storage resources in the cloud for parallel processing while constraining the number of resources to be used in the cloud to optimize expenses without sacrificing performance.
	WO2015132575A1	Apparatus and method for generating and using a subject-specific statistical motion model	 The images may also be two dimensional; this may be appropriate, for example, in a FACIAL RECOGNITION systems.	 The geometric representations may, for example, comprise images. The images may be three-dimensional, especially for representing the three-dimensional shape of the anatomical structure. The images may also be two dimensional; this may be appropriate, for example, in a facial recognition systems. The images may be obtained from one or more suitable imaging modalities, such as (without limitation) magnetic resonance imaging (MRI), X-ray computed tomography (CT), positron emission tomography (PET), ultrasound, photo-acoustic imaging (PAT), optical, X-ray or gamma ray imaging, optical microscopy, electron microscopy, etc. 
	US10534808	Architecture for responding to visual query	 The plurality of visual query search processes includes at least: optical character recognition (OCR), FACIAL RECOGNITION, and a first query-by-image process other than OCR and FACIAL RECOGNITION.	According to some embodiments, there is computer-implemented method of processing a visual query at a server system. A visual query is received from a client system. The visual query is processed by sending the visual query to a plurality of parallel search systems for simultaneous processing. Each of the plurality of search systems implements a distinct visual query search process of a plurality of visual query search processes. The plurality of visual query search processes includes at least: optical character recognition (OCR), facial recognition, and a first query-by-image process other than OCR and facial recognition. A plurality of search results is received from one or more of the plurality of parallel search systems. At least one of the plurality of search results is sent to the client system.
	US9141855	Accelerated object detection filter using a video motion estimation module	 For example, in FACIAL RECOGNITION applications, the reference vector may represent a random representative face or a face representing a statistical mean of faces, or the reference vector may be a zero-valued reference vector.	As described in greater detail below, such a calculation may be repurposed for object detection pre-filtering. For example, an individual region of an input image may be selected or determined. The above calculation may be repurposed to determine a computation that sums the dot product of a weighting vector and a vector of the absolute value difference of a vector representing the region and a reference vector. In this context, the reference vector may be chosen to enhance the classification of the region as likely including an object or portion of an object of interest (i.e., passing the linear classifier pre-filtering) or unlikely to include an object or portion of an object of interest (i.e., failing the linear classifier pre-filtering and being rejected). For example, in facial recognition applications, the reference vector may represent a random representative face or a face representing a statistical mean of faces, or the reference vector may be a zero-valued reference vector. Similarly, in this context, the weighting vector may be pre-trained to enhance object recognition pre-filtering. For example, in facial recognition applications, the weighting vector may be pre-trained by a number of images including faces and a number of images not including faces.
	US9734433	Estimating photometric properties using a camera and a display screen	 The map is then used by image processing routines to better identify and track foreground objects, such as FACIAL RECOGNITION and tracking routines, object recognition and tracking routines, area surveillance and motion detection routines, etc.	Having identified which regions contain specular surfaces and which regions contain diffuse surfaces, a map of the room is generated. The map may include scalable and/or non-scalable boundaries defining regions, and information for each region characterizing the region as diffuse, specular, or some degree there between, and providing surface normal for the regions. Boundaries of the specular regions may be refined by determining areas of the images that share similar local image characteristics such as color and edges. The values assigned to a region characterizing its specularity may, for example, be a value approximating its shininess based on the degree of interdependence between the variations in luminance from the region and the variations in illumination from the display 116. The map is then used by image processing routines to better identify and track foreground objects, such as facial recognition and tracking routines, object recognition and tracking routines, area surveillance and motion detection routines, etc.
female				
	US10586372	Online modeling for real-time facial animation	 The identity PCA model may be computed from a data set consisting of 100 male and 100 FEMALE head scans of young adults, such as the data provided described by V.	An exemplary implementation of one embodiment of the present disclosure may employ a blendshape model of 34 blendshapes. The identity PCA model may be computed from a data set consisting of 100 male and 100 female head scans of young adults, such as the data provided described by V. Blanz and T. Vetter in “A morphable model for the synthesis of 3D faces”, SIGGRAPH 1999. 50 PCA basis vectors could be used to approximate the neutral expression. The corrective deformation fields may be represented by 50 Laplacian eigenvectors for each coordinate. Suitable parameters for the optimizations as discussed above may be set to β2=0.5, β2=0.1, and β3=0.001, as well as λ1=10 and λ2=20, and σ=10 for the coverage threshold.
	US9808549	System for detecting sterile field events and related methods	 Ten participants (77%) were FEMALE and three (23%) were male.	Gonzalez and Sole [22] recently carried out a study where Baccalaureate nursing students with prior documentation of competency demonstrated performance of urinary catheterization on a task trainer. The procedure was recorded and breaches in technique were identified through review of the digital recordings. Data was available for 13 participants. Participants ranged in age from 21-43 years (mean 26.6). The majority of participants (11; 85%) were right handed. Ten participants (77%) were female and three (23%) were male. The participants' mean self-rating of confidence was 3.6 on a 5-point scale (range 3-5), indicating some confidence in performing the skill. Examination of the video recorded data showed that 10 participants (77%) breached aseptic technique in at least one category, and in some instances several categories.
	US9483689	Biometric matching technology	 The system 300 also may analyze faces detected within the multiple images and determine whether features of the detected faces have characteristics that suggest the face is of a FEMALE or a male.	In some implementations, the system 300 may analyze the camera data (e.g., the multiple images) and determine criteria relevant to the captured images based on the analysis. In these implementations, the system 300 may analyze the camera data to determine a distance of one or more objects (e.g., persons) within the multiple images. The system 300 also may analyze faces detected within the multiple images and determine whether features of the detected faces have characteristics that suggest the face is of a female or a male. In this regard, the system 300 may determine the gender of the detected faces within the multiple images. The system 300 further may perform skin tone processing of faces detected within the multiple images and determine ethnicity of persons within the multiple images based on the skin tone processing.
	US10436342	Flow meter and related method	 A male latch component is connected to the first housing component opposite the pivot connection and a FEMALE latch component is coupled to the second housing component opposite the pivot connection.	In certain embodiments of the present disclosure, a system for controlling flow through a drip chamber includes a drip chamber holster, an imaging device, a flexible tube, and a valve. The drip chamber holster receives and secures a drip chamber. The imaging device is configured to capture images of the drip chamber and create image data from the captured images. The flexible tube is connected to the drip chamber and the lumen defined by the tube is in fluid communication with the drip chamber. The valve is axially disposed around a portion of the flexible tube and controls flow through the tube and ultimately the drip chamber. The valve includes first and second casing components pivotally connected to each other and complimentarily align to form an enclosure when in a closed position. Inlet and outlet holes are defined in the valve casing when it is closed and a plunger hole is defined in the first casing component. A male latch component is connected to the first housing component opposite the pivot connection and a female latch component is coupled to the second housing component opposite the pivot connection. A substantially incompressible filler is enclosed within the casing. The filler defines a conduit, sized for a specific tube, which connects the inlet and the outlet holes of the valve casing. There are a plurality of variations in the stiffness of the filler. The portion of the filler proximate the tube may be stiffer than the surrounding filler. The plunger is longitudinally aligned with the plunger hole and attached to the actuator. The actuator is configured to actuate the plunger into and out of the plunger hole to engage the filler. Changes in displacement by the plunger alter the forces on the section of the tube within the casing resulting in the lumen changing size. The area of the head of the plunger can be smaller than the longitudinal cross-section of the lumen disposed within the housing.
	US9448164	Systems and methods for noninvasive blood glucose and other analyte detection and measurement using collision computing	 The use of multiple illumination-detection pairs to target the same layer in the skin tissue, as described above (that is by increasing the tomographic states), can expand the glucose measurement capability and increase the ability of the system to compensate for male and FEMALE subjects with varying skin thickness, surface texture, and changes due to aging, gender, and/or ethnic differences.	The probe of FIG. 91, as detailed in FIG. 92, is designed such that targeting of different skin tissue layers by tomographic sequences EVa1, EVa2, . . . , EVax; DVb1, DVb2, . . . DVby; and, SVc1, SVc2, . . . SVcz can be verified based on examination of acquired spectra based on absorption of spectral bands known to be associated with biochemical compounds other than the analyte of interest, (such as by examination of intensity and absorbance amplitude profiles of the fat, protein and collagen bands as shown in FIG. 93 for different ring illuminations). Profiles of absorption gradients leading to the assessment of acceptable feature pairs is a function of the tomographic sequence and the results of the above expression, based on an anatomical and physiological understanding of skin tissue. The use of multiple illumination-detection pairs to target the same layer in the skin tissue, as described above (that is by increasing the tomographic states), can expand the glucose measurement capability and increase the ability of the system to compensate for male and female subjects with varying skin thickness, surface texture, and changes due to aging, gender, and/or ethnic differences.
foot traffic				
	US9734388	System and method for detecting, tracking and counting human objects of interest using a counting system and a data capture device	 Again, the types of sensors 1220 and video sensors 1246 should not be considered limiting, and it should be obvious that any sensor 1220, including image capturing devices, thermal sensors and infrared video devices, capable of counting the total FOOT TRAFFIC and generating a starting frame, an ending frame, and a direction will be compatible with the present system and may be adopted.	For exemplary purposes, the sensor 1220 may include at least one stereo camera with two or more video sensors 1246 (similar to the sensor shown in FIG. 2), which allows the camera to simulate human binocular vision. A pair of stereo images comprises frames 1248 taken by each video sensor 1246 of the camera. The sensor 1220 converts light images to digital signals through which the counting system 1330 obtains digital raw frames 1248 comprising pixels. Again, the types of sensors 1220 and video sensors 1246 should not be considered limiting, and it should be obvious that any sensor 1220, including image capturing devices, thermal sensors and infrared video devices, capable of counting the total foot traffic and generating a starting frame, an ending frame, and a direction will be compatible with the present system and may be adopted.
	US10733427	System and method for detecting, tracking, and counting human objects of interest using a counting system and a data capture device	 Again, the types of sensors 1220 and video sensors 1246 should not be considered limiting, and it should be obvious that any sensor 1220, including image capturing devices, thermal sensors and infrared video devices, capable of counting the total FOOT TRAFFIC and generating a starting frame, an ending frame, and a direction will be compatible with the present system and may be adopted.	For exemplary purposes, the sensor 1220 may include at least one stereo camera with two or more video sensors 1246 (similar to the sensor shown in FIG. 2), which allows the camera to simulate human binocular vision. A pair of stereo images comprises frames 1248 taken by each video sensor 1246 of the camera. The sensor 1220 converts light images to digital signals through which the counting system 1330 obtains digital raw frames 1248 comprising pixels. Again, the types of sensors 1220 and video sensors 1246 should not be considered limiting, and it should be obvious that any sensor 1220, including image capturing devices, thermal sensors and infrared video devices, capable of counting the total foot traffic and generating a starting frame, an ending frame, and a direction will be compatible with the present system and may be adopted.
	US10776655	Estimating color of vehicles on a roadway	 Other public cameras may also be installed on an “as needed” basis as FOOT TRAFFIC in area increases, new government facilities are constructed, etc.	Installation of traffic cameras is usually on an “as needed” basis. As traffic in an area increases, a jurisdiction (e.g., a state department of transportation, a city, etc.) may install a camera to monitor the traffic. Thus, traffic cameras can have varied capabilities and configurations. For example, if a traffic camera is installed in an intersection and 5 years later another traffic camera installed in an adjacent intersection, it is likely the capabilities and configuration of the other (new) camera is far superior. Other public cameras may also be installed on an “as needed” basis as foot traffic in area increases, new government facilities are constructed, etc. Private cameras may also be installed in different locations at different times.
	US10268223	System and method for managing energy	In an illustrative embodiment, a method of managing energy consumption in an enclosed spaced includes the step of providing temperature and FOOT TRAFFIC data to a plan generator.	In an illustrative embodiment, a method of managing energy consumption in an enclosed spaced includes the step of providing temperature and foot traffic data to a plan generator. The method further includes the steps of generating an energy plan based on the temperature and foot traffic data and controlling one or more energy consuming devices based on the energy plan.
	US10545500	Model for determining drop-off spot at delivery location	 Contextual data 604 may include, for example, current and future weather conditions, crime statistics of the neighborhood surrounding the delivery destination, and FOOT TRAFFIC patterns along walkways near the delivery destination, among other data.	 Contextual data 604 may include, for example, current and future weather conditions, crime statistics of the neighborhood surrounding the delivery destination, and foot traffic patterns along walkways near the delivery destination, among other data. Based on this information, ANN 610 may be configured to consider, for example, the safety of the determined drop-off spots. For example, when contextual data 604 indicates incoming rain, ANN 610 may determine that, in spite of blocking door 724, the object is best left at drop-off spot 718 atop the stairs rather than at drop- off spots  720 or 722 within lower areas where rainwater might pool, as illustrated in FIG. 7D. In another example, when contextual data 604 indicates that that delivery destination 702 is located in a neighborhood with a high rate of crime or on a street with foot traffic above a threshold foot traffic value, ANN 610 may determine a drop-off spot in the backyard, behind fence 726. In some embodiments, this determination may take place prior to dispatch of the delivery vehicle so that a delivery vehicle capable of traversing fence 726 may be dispatched (e.g., an aerial delivery vehicle).
fraud				
	US10498401	System and method for guiding card positioning using phone sensors	 Examples of tasks that may be performed at least partially using machine-learning models include various types of scoring; bioinformatics; cheminformatics; software engineering; FRAUD detection; customer segmentation; generating online recommendations; adaptive websites; determining customer lifetime value; search engines; placing advertisements in real time or near real time; classifying DNA sequences; affective computing; performing natural language processing and understanding; object recognition and computer vision; robotic locomotion; playing games; optimization and metaheuristics; detecting network intrusions; medical diagnosis and monitoring; or predicting when an asset, such as a machine, will need maintenance.	Different machine-learning models may be used interchangeably to perform a task. Examples of tasks that may be performed at least partially using machine-learning models include various types of scoring; bioinformatics; cheminformatics; software engineering; fraud detection; customer segmentation; generating online recommendations; adaptive websites; determining customer lifetime value; search engines; placing advertisements in real time or near real time; classifying DNA sequences; affective computing; performing natural language processing and understanding; object recognition and computer vision; robotic locomotion; playing games; optimization and metaheuristics; detecting network intrusions; medical diagnosis and monitoring; or predicting when an asset, such as a machine, will need maintenance.
	US7693806	Classification using a cascade approach	 Another embodiment is a credit card FRAUD detection system that is optimized to have a low false positive rate for identifying good transactions as FRAUDulent ones in order to prevent inconvenience to the credit card customer.	In one aspect, the invention is embodied in an e-mail spam filtering system where the classifier has been optimized to have a low false positive rate for identifying good messages as spam. Other embodiments for spam filtering can include, but are not limited to: an instant messaging service, where the classifier has been optimized to have a low false positive rate for classifying good instant messages as spam; a telemarketing screening service, where the classifier is optimized to have a low false positive rate for identifying a phone call that is a non-telemarketing call as a telemarketing call; a device display pop-up blocker that is optimized to have a low false positive rate for identifying relevant pop-ups, such as warnings, reminders, application windows, as spam pop-ups; and an audio stream filter for classifying music from advertisements and/or talk, that can be optimized for a low false positive rate for identifying music as advertising and/or talk if a user wants to avoid hearing ads and talk but doesn't want a song to be cut off in the middle. In another embodiment, a cancer screening system is optimized to have a low false negative rate for positive samples. Another embodiment is a credit card fraud detection system that is optimized to have a low false positive rate for identifying good transactions as fraudulent ones in order to prevent inconvenience to the credit card customer. A customer who was frequently being denied when trying to use a credit card may stop using the card. In another fraud detection system, a credit card company may have a fraud detection system that is set to a low false negative rate for identifying patterns of charges across many accounts as fraud that are not actually fraudulent. Detecting macro fraudulent patterns can assist security in focusing their investigations and prevent organized credit card fraud. The invention can also be embodied in an intrusion detection system, such as in a high security environment. For example, a nuclear power plant employs biometrics to identify people that are authorized to enter the facility. It is important to keep unauthorized people out of the facility. The biometric system can be optimized to have a low false negative rate for identifying unauthorized users as authorized. Along similar lines a high security computing environment may be optimized for a low false negative for classifying an intruder (login, connection, download, application) as legitimate. It should be appreciated that the above embodiments are exemplary and are not intended to limit the scope of the invention to particular scenarios, operating conditions or applications. The invention is applicable to any classifier where there is need to optimize for a specific region of interest. Furthermore, the data being classified can include any of, but are not limited to, domains, web pages, Uniform Resource Locators(URL), text, images, videos, audio, documents, files, directories, data structures and the like.
	US10698995	Method to verify identity using a previously collected biometric image/data	In some embodiments, the system may reward users who successfully utilize the authentication system or who otherwise take FRAUD preventing measures.	In some embodiments, the system may reward users who successfully utilize the authentication system or who otherwise take fraud preventing measures. Such rewards may include leaderboards, status levels, reward points, coupons or other offers, and the like. In some embodiments, the authentication system may be used to login to multiple accounts.
	US10055733	Biometric chain of provenance	The potential for FRAUD in financial transactions has increased significantly due to the increasing diversity in the means for transactions to be performed.	The potential for fraud in financial transactions has increased significantly due to the increasing diversity in the means for transactions to be performed. For example, it is often challenging to ensure that biometrics acquired by a biometrics device are really those of an individual at the biometrics device. Moreover, in certain contexts, it may be necessary or more acceptable to acquire biometric of a moving individual without constraining the individual's advance or movements. However, conventional systems are typically not very robust against fraud and/or mistake when biometric acquisition is decoupled from traditional access control systems, or when individuals whose biometrics are being acquired are not rigidly constrained for the biometric acquisition process. Some ability to track an individual across one or more transactions may be a way to reduce fraudulent activity.
	US10657457	Automatic selection of high quality training data using an adaptive oracle-trained learning framework	 A system that automatically performs email FRAUD identification based on data sampled from a data stream is an example of a system that processes dynamic data.	Data being continuously sampled from a data stream representing data collected from a variety of online sources (e.g., websites, blogs, and social media) is an example of dynamic data. A system that automatically performs email fraud identification based on data sampled from a data stream is an example of a system that processes dynamic data. Analysis of such dynamic data typically is based on data-driven models that can be generated using machine learning. One type of machine learning is supervised learning, in which a statistical predictive model is derived based on a training data set of examples representing the modeling task to be performed.
friend				
	EP3579196A1	Human clothing transfer method, system and device		
	US9275499	Augmented reality interface for video	 The acquiring and retrieving of imagery may be performed by different persons, including FRIENDs or clients for example.	In a second embodiment, the metadata may include annotations by a server or a user acquiring the video. The annotations may include details of a person, an object, or a location being photographed. The annotations may help users share their experiences and/or recommended locations. The acquiring and retrieving of imagery may be performed by different persons, including friends or clients for example.
	US9448164	Systems and methods for noninvasive blood glucose and other analyte detection and measurement using collision computing	 The analyte concentration 40 can be output to the user, or further used in computation of clinical, diagnostic, screening, therapy effectiveness or regimen monitoring or wellness analytics; alarms provided to users, their care-team, FRIENDs and family; population analytics; and longitudinal analysis.	As shown in FIG. 78, using tissue spectra, the NRSEGs are computed for all feature pairs for all illumination states and are averaged over all repeats of the respective illumination states on a feature pair by feature pair basis. For each of the feature pairs for each sample found to be acceptable as defined below, a Normalized Absorption Gradient (NAG) numerical value is computed (step 20) to select the appropriate mapped individual projector curve, from a set of several mapped individual projectors curves included in a mapped projector curve set. In some embodiments, there may be only one individual mapped projector curve in the mapped projector curve set. Once the applicable mapped projector curve is selected, it is used to transform the NRSEG into an analyte concentration in step 30. The analyte concentration 40 can be output to the user, or further used in computation of clinical, diagnostic, screening, therapy effectiveness or regimen monitoring or wellness analytics; alarms provided to users, their care-team, friends and family; population analytics; and longitudinal analysis. Details on processes for computing NAG, developing and using the mapped projector curve set (step 25) are described below.
	WO2014005022A1	Individualizing generic communications	" For the ""FRIENDs"" dimension of the IV, work colleagues were used."	"[0062] Highly popular (but neutral on other aspects, e.g., beauty, liking) female and male celebrities were chosen from a survey in the pre-study. For the ""friends"" dimension of the IV, work colleagues were used. As with celebrities, it should be noted that there are also other categories of people, e.g., more casual friends, who are also personally familiar to a person, thus the study is specific to the choices made. Using the participants' company calendars, ""friends"" were selected based on whether the participant meets the friend face-to-face regularly, both of them have at least one point of contact such as a common project, and if they were of the same gender. "
	US9189886	Method and apparatus for estimating body shape	The body identifier may be provided by the user to retailers, on-line stores, or made available to FRIENDs and relatives with or without privacy protection.	The body identifier may be provided by the user to retailers, on-line stores, or made available to friends and relatives with or without privacy protection. In providing access to their body model, the user may provide limited rights using standard digital property rights management methods. For example, they may provide access to a friend or family member who can then provide their information to a clothing retailer, but that person could be prohibited from viewing the body model graphically. As another example, a user could provide access to display the body to video game software to enable the use of the model as a video game avatar, but restrict the further transmission of the model or its derived measurements.
gender				
	US9483689	Biometric matching technology	 The image processor 230 also may determine the angle, GENDER, ethnicity, and any other detectable attributes of a person within the one or more images based on an analysis of the one or more images.	The image processor 230 processes the one or more images to determine attributes of the images and/or persons within the one or more images. For instance, the image processor 230 may compute the distance between a person within an image and the camera 210 based on an analysis of the captured image. The image processor 230 also may determine the angle, gender, ethnicity, and any other detectable attributes of a person within the one or more images based on an analysis of the one or more images. The image processor 230 also may determine general characteristics (e.g., blurriness, etc.) of the one or more images based on an analysis of the one or more images.
	US10413226	Noncontact monitoring of blood oxygen saturation using camera	 The subjects included different GENDERs (three males, three females), ages (27.	Referring now to FIG. 11, an example of correlation between the lowest SpO2 values obtained from the presented noncontact and reference contact methods. Line 110 is a linear fit of the data points. To demonstrate the robustness of the disclosed noncontact method to monitor SpO2, a small-scale pilot study was conducted and statistical analysis of the data was completed. Six subjects were enrolled in the Institutional Review Board study approved by Arizona State University (No. STUDY00002240). The subjects included different genders (three males, three females), ages (27.3±2.8 years old, mean±SD), and skin colors. Informed consents were obtained from all the subjects following an approved protocol. None of the subjects had any known respiratory disease. The test was repeated as described above on different subjects and the lowest SpO2 values were compared as determined by using the presented method and reference pulse oximetry. Plot 111 is a plot of the lowest SpO2 values from 43 tests and linear least square regression. A good linear correlation (R2=0.87) was found between the presented and reference methods over a wide range of oxygen saturation levels. Slope of the linear fitting curve is about 0.86, which is small than the ideal value of 1, with standard error of 0.05. The data are dispersed around the fitted linear curve 110, which may be attributed to subject movement, and light scattering effects.
	US9189886	Method and apparatus for estimating body shape	 Additionally, it allows several novel methods to extract standard tailoring measurements, clothing sizes, GENDER and other information from body scans.	By construction, in the presently disclosed method every body model recovered from measurements is in full correspondence with every other body model. This means that a mesh vertex on the right shoulder in one person corresponds to the same vertex on another person's shoulder. This is unlike traditional laser or structured light scans where the mesh topology for every person is different. This formulation allows body shapes to be matched to each other to determine how similar they are; the method makes use of this in several ways. Additionally, it allows several novel methods to extract standard tailoring measurements, clothing sizes, gender and other information from body scans. Unlike traditional methods for measuring body meshes, the presently disclosed methods use a database of body shapes with known attributes (such as height, waist size, preferred clothing sizes, etc) to learn a mapping from body shape to attributes. The presently disclosed method describes both parametric and non-parametric methods for estimating attributes from body shape.
	WO2014098308A1	Method for displaying unified app information based on open app store, and computer readable recording medium therefor		
	US9986211	Low-power always-on face detection, tracking, recognition and/or analysis using events-based vision sensor	 There are many methods and applications available for detection, tracking, recognition of face(s) in still images and videos, including emotion detection, GENDER classification, lip reading, eye/gaze tracking, etc.	Human-computer interaction uses many modalities including language (typing, voice recognition, on-screen text display, speech synthesis, and the like) and vision (still and video cameras, graphic displays, and the like). Face detection, recognition, expressions, and so forth forms an important part of human-to-human communication and thereby is important for human-machine interaction as well. There are many methods and applications available for detection, tracking, recognition of face(s) in still images and videos, including emotion detection, gender classification, lip reading, eye/gaze tracking, etc.
geolocation				
	WO2020043350A1	Method(s) and system(s) for vehicular cargo management	 For example, when delivery personnel get to a customer stop, the system (104) identifies the customer at which they’ve arrived based on the GEOLOCATION of the vehicle.	[0048] In one aspect, the vehicle cargo management techniques of this disclosure may be used to direct couriers to the proper package at the time of delivery/unloading. For example, when delivery personnel get to a customer stop, the system (104) identifies the customer at which they’ve arrived based on the geolocation of the vehicle. The system (104) may then cross-reference the location with geofences drawn in a predefined radius around each customer delivery location. Alternatively, delivery personnel can select a particular customer from their delivery manifest via their handheld device (108), an in-vehicle console (108), or other application display (108). When a door to the cargo space is opened, the system (104) may identify the package IDs corresponding to the particular customer and look up the last known location of that package in the cargo space. 
	US10677932	Systems, methods, and devices for geo-localization	 A GEOLOCATION of the trajectory determination device can be determined.	In one embodiment a trajectory determination device for geo-localization can include one or more relative position sensors, one or more processors, and memory. The one or more relative position sensors can generate relative position signals. The one or more processors can be communicatively coupled to the one or more relative position sensors and the memory. The memory can store machine readable instructions. The one or more processors can execute the machine readable instructions to receive the relative position signals from the one or more relative position sensors. The relative position signals can be transformed into a sequence of relative trajectories. Each of the relative trajectories can include a distance and directional information indicative of a change in orientation of the trajectory determination device. A progressive topology can be created based upon the sequence of relative trajectories; this progressive topology can be compared to map data. A geolocation of the trajectory determination device can be determined.
	US10812861	Systems and methods for providing access to still images derived from a video	, a title, GEOLOCATION information associated with the content of the edited video, and/or other information associated with the edited video), editing-user information (e.	In some implementations, effectuating communication of one or more notifications may comprise communicating notifications to the one or more identified users via one or more of the one or more online platforms, an external communication (e.g., a text message and/or other communications), and/or other types of distribution. An some implementations, a notification may include notification information. Notification information may include one or more of a location to consume the edited video (e.g., a link), edited video information (e.g., a title, geolocation information associated with the content of the edited video, and/or other information associated with the edited video), editing-user information (e.g., a name, username, and/or other information associated with a user who generated the edited video), user interaction information (e.g., information identifying the type of user interaction previously engaged in by the user who is receiving the notification), and/or other information. In some implementations, a location to consume the edited video may include one or more of a location within a social media outlet, a location external to the social media outlet, a privately accessible location, a publically accessible location, and/or other locations.
	US9947128	Methods for improving accuracy, analyzing change detection, and performing data compression for multiple images	The absolute GEOLOCATION, georeference accuracy of GeoEye-1 was measured, improved, and reported by GeoEye's Kohm and Mulawa in 2009, 2010, and 2011.	The absolute geolocation, georeference accuracy of GeoEye-1 was measured, improved, and reported by GeoEye's Kohm and Mulawa in 2009, 2010, and 2011. The GeoEye-1 accuracies were validated by Fraser from the University of Melbourne in 2011. Mulawa showed that GeoEye-1 intra-image geometric accuracy, e.g., within an image, is +/−1 meter across the 15 kilometer swath. This accuracy was achieved through geometric calibration while on-orbit using repeated measurements at multiple sites in the United States and worldwide. The key finding was that for 64 or so images, the error of the means were ˜1 meter in the vertical and horizontal.
	US10371518	Surveying system	In some applications interactive GEOLOCATION without GNSS signal may be required.	In some applications interactive geolocation without GNSS signal may be required. The following method then can be used:
hand				
	US9148463	Methods and systems for improving error resilience in video delivery	 The human portion may be further refined into finer objects such as body, arm, HAND, finger, head, nose, eye, mouth, etc.	In more detail, according to at least one example embodiment a most recent available image frame for a video scene including a person (e.g., on a one-to-one video call) is analyzed and separated into its semantic components. At its simplest, the image frame may be divided into a “human” (or more generally an “object” portion) and a “background” portion. The background may be assumed to be essentially static, and depending on the application, may not need to be further broken down. The human portion may be further refined into finer objects such as body, arm, hand, finger, head, nose, eye, mouth, etc.
	CN102819662B	Computing method of video fluid height	At present, in augmented reality research, the research combined for rigid body actual situation has achieved preliminary achievement, and for the scene containing fluids such as water, smog, cloud, fire, because fluid belongs to strong texture, in motion process, there is the problem blocked and reproduce, so the research of the augmented reality of associated fluid also exists certain challenge, the achievement obtained in this on the one HAND research is remarkable not enough.	At present, in augmented reality research, the research combined for rigid body actual situation has achieved preliminary achievement, and for the scene containing fluids such as water, smog, cloud, fire, because fluid belongs to strong texture, in motion process, there is the problem blocked and reproduce, so the research of the augmented reality of associated fluid also exists certain challenge, the achievement obtained in this on the one hand research is remarkable not enough.Fluid reconstruction technique is the key issue in research, how quickly and accurately Fluid Computation height, set up augmented reality scene that is realistic and the fluid natural landscape of interactivity, be still in this field and have challenging problem, its research has important practical significance and using value.
	US9870068	Depth mapping with a head mounted display using stereo cameras and structured light	 FIG. 5A shows a HAND being tracked by a light pattern comprising a plurality of horizontal bars orientated parallel to the HAND, in accordance with an embodiment.	 FIG. 5A shows a hand being tracked by a light pattern comprising a plurality of horizontal bars orientated parallel to the hand, in accordance with an embodiment.
	US9020210	Image processing system, image processing apparatus, image processing method, and program	On the other HAND, computer-graphics generated animated images are built on the basis of scenario information describing camera works, character movements, and the like.	On the other hand, computer-graphics generated animated images are built on the basis of scenario information describing camera works, character movements, and the like. Therefore, by using the scenario information, effects can be semi-automatically applied to computer-graphics generated animated images (see, for example, Japanese Patent No. 4245433). However, since so-called actual-captured images actually captured with an image capturing apparatus are not images based on such scenario information, it is difficult to apply effects to such actual-captured images semi-automatically.
	EP3432204A1	Telepresence framework for region of interest marking using headmount devices		
iris				
	US9483689	Biometric matching technology	, images of IRISes, fingerprints, faces, etc.	 FIG. 3 illustrates an exemplary biometric matching system 300. The system 300 includes an input module 310, a data store 320, one or more processors 330, one or more I/O (Input/Output) devices 340, and memory 350. The input module 320 may be used to input any type of information used in enrolling and verifying biometric data. For example, the input module 310 may be used to receive new biometric data to enroll in a database or receive sample biometric data to verify against biometric data stored in a database. In some implementations, data from the input module 310 is stored in the data store 320. The data included in the data store 320 may include, for example, any type of biometric data (e.g., images of irises, fingerprints, faces, etc.) and similarity scores computed for biometric data. The similarity scores may be used to sort biometric data in the data store 320 using techniques described throughout this disclosure.
	US10372982	Methods and apparatus for repetitive iris recognition	: PCT/US2015/010293, filed Jan. 6, 2015, titled “METHODS AND APPARATUS FOR REPETITIVE IRIS RECOGNITION” which in turn claims priority to U.	This application is a national stage of, and claims priority to and the benefit of the Patent Application No.: PCT/US2015/010293, filed Jan. 6, 2015, titled “METHODS AND APPARATUS FOR REPETITIVE IRIS RECOGNITION” which in turn claims priority to U.S. provisional application 61/924,055, filed Jan. 6, 2014, titled “METHODS AND APPARATUS FOR REPETITIVE IRIS RECOGNITION”; U.S. provisional application 62/056,598, filed Sep. 28, 2014, titled “METHODS AND APPARATUS FOR REPETITIVE IRIS RECOGNITION” and U.S. provisional application No. 62/056,600, filed Sep. 28, 2014, titled “METHODS AND APPARATUS FOR ILLUMINATING THE IRIS FOR IRIS RECOGNITION,” all of which are hereby incorporated by reference in their entireties for all purposes.
	US10479647	Depth sensor based sensing for special passenger conveyance loading conditions	 That is, the sensor data from the security sensors 280 essentially provides data to the control system 32 to include, but not be limited to, facial recognition, badge identification, fingerprints IRIS data, security card information, etc.	Referring now to FIG. 25, the fusion based passenger tracking system 270 may include a plurality of security sensors 280 a-280 n that communicate with the elevator system 20 via the control system 32. That is, the sensor data from the security sensors 280 essentially provides data to the control system 32 to include, but not be limited to, facial recognition, badge identification, fingerprints iris data, security card information, etc. In areas without surveillance coverage or where the analytics processes may not perform well, the additional security sensors can recognize the person and then, using sensor fusion, close the gaps in the traffic list to make the whole process more robust. In any instance where identity is associated with a passenger, the identity and associated passenger tracking data is maintained in a way that preserves privacy by using encryption, authentication, and other security measures.
	US9596983	Methods and systems for tracking a torsional orientation and position of an eye	In one embodiment, a method comprises selecting at least one marker on the IRIS of the eye in the first image.	In one embodiment, a method comprises selecting at least one marker on the iris of the eye in the first image. A corresponding marker is located on the iris in the second image. The first image of the eye and the second image of the eye are registered by substantially matching a common reference point in the first and second images and matching the marker on the iris of the image of the first eye and the marker on the iris of the image of the second eye. Thereafter, a laser treatment can be centered and torsionally aligned with the second image of the eye. In some embodiments, the second image of the eye can be obtained while the patient's eye is aligned with a laser beam that is to deliver the laser treatment.
	US10159411	Detecting irregular physiological responses during exposure to sensitive data	, an IRIS scan), and/or multi-factor authentication.	Some embodiments of the system configured to detect an irregular physiological response while being exposed to sensitive data include added security measures such as encryption of the sensitive data. Optionally, the system receives the certain sensitive data in an encrypted form, and the computer decrypts the certain sensitive data before presentation via the HMD. The decryption may involve hardware-based decryption, requesting a password from the user, and/or measuring the user with a sensor (e.g., an iris scan), and/or multi-factor authentication.
irises				
	US9280706	Efficient method and system for the acquisition of scene imagery and iris imagery using a single sensor	 Feedback mechanisms can guide the user to move the image capturing device to locate the user's IRISES within an appropriate capture region.	Another approach incorporated in some embodiments of our methods and systems for acquiring optimal standard scene imagery and iris imagery on the same sensor, relating to the wavelength of the illumination, involves multiplexing or positioning an infra-red cut filter over a standard image sensor or lens. In one embodiment, a portion of the sensor (for example, 20% of the sensor or sensor nodes) may be designated primarily for iris recognition, while the remaining (e.g., 80%) portion may be used for standard image acquisition, for example as shown in FIG. 14. A lower portion (e.g., 80%) of the sensor, as in this example, may be covered by a standard IR-cut filter. The remaining 20% of the sensor may remain uncovered. In iris recognition mode, the covered region may be ignored. For example, an iris recognition application executing on the image capturing device may guide the user to position their eyes within the sensing region of the uncovered 20% area. Feedback mechanisms can guide the user to move the image capturing device to locate the user's irises within an appropriate capture region. For example, since the face will be visible in the remaining 80% of the imager, this can be used for user guidance feedback, optionally with icons appearing in place of the eye region. In some embodiments, the image sensor may adjust its orientation to capture an image of the user's iris using the uncovered region.
	US9852338	Biometric imaging method and device	 Taking iris as an example, the micromotor controller 130 can perform simultaneous imaging of the regions of human eyes, and for each frame of the acquired image calculate in real time the central positions of pupils of human left and right eyes using an image processing algorithm to thereby realize real-time lookup and real-time location of IRISES of both eyes in the entire imaging image and cut out said imaging image into a monocular iris image of the left eye or right eye.	For example, the micromotor controller 130 can rapidly locate a biometric region of interest, such as iris region of human eyes with respect to any frame of electronic image as acquired by the image sensor 120. Taking iris as an example, the micromotor controller 130 can perform simultaneous imaging of the regions of human eyes, and for each frame of the acquired image calculate in real time the central positions of pupils of human left and right eyes using an image processing algorithm to thereby realize real-time lookup and real-time location of irises of both eyes in the entire imaging image and cut out said imaging image into a monocular iris image of the left eye or right eye. As shown in FIG. 6, the image resolution is generally 640×480. The monocular iris image of the left eye or right eye obtained by image segmentation can be used as an analysis object for assessment of the definition of the image acquired by the biometric imaging device 100. Then, with respect to any monocular iris or irises of eyes, image analysis is performed on an image quality function (ImageQualityMetrics) thereof. The calculation of said function can be realized by means of a plurality of energy transfer functions F, including but not limited to Discrete Cosine Transform (DCT), Fast Fourier Transform (FFT) or Wavelet Transform (Wavelet), etc. The image quality information obtained by calculation may be an array of image quality parameters, and may also be a single image quality parameter including but not limited to definition, contrast, average gray scale, image information entropy, interpupillary distance, pupil diameter, iris diameter, etc. of the image.
	US9841563	Shuttered waveguide light field display	 Detection is also easily extended to other useful features of the face and eyes, including the eyebrows, nose, mouth, eyelids, scleras, IRISES and pupils [Betke00, Lienhart03, Hansen10].	Approximate eye detection is typically intrinsic to face detection, and more accurate eye positions can be estimated after face detection [Hansen10]. Detection is also easily extended to other useful features of the face and eyes, including the eyebrows, nose, mouth, eyelids, scleras, irises and pupils [Betke00, Lienhart03, Hansen10].
	US9313460	Depth-aware blur kernel estimation method for iris deblurring	The present invention relates to apparatuses and methods for identifying personnel and, more particularly, to apparatuses and methods for identifying personnel based on visual characteristics of the IRISES of their eyes.	The present invention relates to apparatuses and methods for identifying personnel and, more particularly, to apparatuses and methods for identifying personnel based on visual characteristics of the irises of their eyes.
	US10733924	Foveated light field display	 Detection is also easily extended to other useful features of the face and eyes, including the eyebrows, nose, mouth, eyelids, scleras, IRISES and pupils [Betke00, Lienhart03, Hansen10].	Approximate eye detection is typically intrinsic to face detection, and more accurate eye positions can be estimated after face detection [Hansen10]. Detection is also easily extended to other useful features of the face and eyes, including the eyebrows, nose, mouth, eyelids, scleras, irises and pupils [Betke00, Lienhart03, Hansen10].
jail				
	US10296784	Verifying presence of a person during an electronic visitation	, municipal JAILs, county JAILs, state prisons, federal prisons, military stockades, juvenile facilities, detention camps, home incarceration environments, etc.	This specification discloses methods and systems for verifying the presence of one or more actual and/or authorized persons during an electronic visitation. The various techniques described herein may find applicability in a wide variety of controlled-environment facilities (as well as outside of controlled-environment facilities). Various types of controlled-environment facilities are present in today's society, and persons may be voluntary or involuntary residents of such facilities, whether temporarily or permanently. Examples of controlled-environment facilities may include correctional institutions or facilities (e.g., municipal jails, county jails, state prisons, federal prisons, military stockades, juvenile facilities, detention camps, home incarceration environments, etc.), healthcare facilities (e.g., hospitals, nursing homes, mental health facilities, rehabilitation clinics, such as drug and alcohol rehabilitation facilities, etc.), restricted living quarters (e.g., hotels, resorts, camps, dormitories, barracks, etc.), and the like.
kid				
	US9665702	Restricted execution modes	 61/695,294 filed Aug. 30, 2012 entitled “Mobile Device Kid Space”, the disclosure of which is incorporated by reference herein in its entirety.	This application is a continuation of U.S. patent application Ser. No. 13/726,099 filed Dec. 22, 2012 entitled “Restricted Execution Modes,” which claims priority under 35 U.S.C. Section 119(e) to U.S. Provisional Application Ser. No. 61/580,147 filed Dec. 23, 2011 entitled “Mobile Device Shared”, the disclosures of which are incorporated by reference herein in its entirety. This application also claims priority under 35 U.S.C. Section 119(e) to U.S. Provisional Application Ser. No. 61/695,294 filed Aug. 30, 2012 entitled “Mobile Device Kid Space”, the disclosure of which is incorporated by reference herein in its entirety.
	US9426451	Cooperative photography	Imagine you are a parent attending the pre-prom obligatory photo-fest with your KID, their date, and eight other teen-couples to boot.	Imagine you are a parent attending the pre-prom obligatory photo-fest with your kid, their date, and eight other teen-couples to boot. Lots of parents—all with smartphones held proudly forth on the now-milling, now-posing couples, sometimes alone, then ad hoc pairings and then of course the 30 minutes of the big group shots. The prairie dog cameras shooting up and down and all around in the process, also culminating in a large group of parents lined opposite the smaller group of kids, most cameras now in action.
	US9491589	Mobile device safe driving	 61/695,294 filed Aug. 30, 2012 entitled “Mobile Device Kid Space,” the entire disclosures of each of these applications being incorporated by reference in their entirety.	This application is a continuation of and claims priority under 35 U.S.C. Section 120 to U.S. patent application Ser. No. 13/726,097, filed Dec. 22, 2012, entitled “Mobile Device Safe Driving” which claims priority under 35 U.S.C. Section 119(e) to U.S. Provisional Application Ser. No. 61/580,131 filed Dec. 23, 2011 entitled “Mobile Device Safe Driving” and U.S. Provisional Application Ser. No. 61/695,294 filed Aug. 30, 2012 entitled “Mobile Device Kid Space,” the entire disclosures of each of these applications being incorporated by reference in their entirety.
	US9330296	Recognizing entity interactions in visual media	, human interactions, such as, “the KIDs visiting grandma,” “the girls out on the town,” “us walking down the aisle” or “me cutting the birthday cake.	The illustrative image/video tagger module 132 performs such tagging automatically, e.g., without the need for manual review of the images by a human. The image/video tagger module 132 can, among other things, facilitate image search and retrieval using higher-level/conceptual search terms or natural language, including familiar phrases that describe, e.g., human interactions, such as, “the kids visiting grandma,” “the girls out on the town,” “us walking down the aisle” or “me cutting the birthday cake.” The image/video tagger module 132 may be particularly useful in contexts in which the desired high-level classifications are difficult to represent using lower-level features.
	CN107194338A	Traffic environment pedestrian detection method based on human body tree graph model		
license plate				
	US9685079	Short-time stopping detection from red light camera evidentiary photos	 These systems can detect the violating vehicles by identifying LICENSE PLATE numbers and/or the make and model of the vehicles from photographs captured by red light cameras.	Red Light Camera Systems (RLCS) are traffic regulation enforcement systems that detect and identify vehicles that enter an intersection against a red traffic light and, therefore, are in violation of a traffic regulation. These systems can detect the violating vehicles by identifying license plate numbers and/or the make and model of the vehicles from photographs captured by red light cameras. A citation is then issued to the owner of the vehicle identified in a photograph.
	US9443314	Hierarchical conditional random field model for labeling and segmenting images	 In one embodiment, LICENSE PLATEs appearing in the street view image can be detected and blurred or removed.	As described above, in one embodiment, the hierarchical classification technique can be used to determine pixel-by-pixel labels for unlabeled street view images. This would allow, for example, a mapping application to automatically identify objects depicted in large street view image databases. Such an automated labeling technique is desirable because street view images for such applications are acquired in very large quantities (e.g., by driving a car along streets and continuously capturing images). The labels may be useful, for example, to identify buildings in the street view images which may then be associated with a particular address, company, or person in the map application. Additionally, for privacy purposes, miscellaneous objects such as people and cars could be automatically blurred or removed from street view images. In one embodiment, license plates appearing in the street view image can be detected and blurred or removed.
	US10371518	Surveying system	 On the image data, algorithms for face detection and detection of LICENSE PLATEs on cars can be applied.	The invention also relates to a blurring functionality of of a surveying system. On the image data, algorithms for face detection and detection of license plates on cars can be applied. In order to protect the privacy of people, the detected faces or license plates can then be made unrecognizable, e. g. by blurring the corresponding areas in the images and also in the point cloud.
	WO2018072233A1	Method and system for vehicle tag detection and recognition based on selective search algorithm		
	US10467458	Joint face-detection and head-pose-angle-estimation using small-scale convolutional neural network (CNN) modules for embedded systems	 Due to these constraints, the hardware CNN module within Hi3519 SoC is typically only suitable for performing simple applications such as handwritten digit recognition and LICENSE PLATE recognition.	For the example of the hardware CNN module within Hi3519 SoC, the maximum input dimension for the first FC layer is 1024, and the number of neurons in the middle FC layers is at most 256. The dimension of the CNN module output is at most 256. Due to these constraints, the hardware CNN module within Hi3519 SoC is typically only suitable for performing simple applications such as handwritten digit recognition and license plate recognition. For more challenging applications such as face recognition, directly applying a small-scale CNN module such as CNN module 100 would be infeasible at least because of the following reasons. First, the maximum input resolution of 1280 pixels (such as 40×32) is very restrictive, because a face image down-sampled to this resolution loses too much important facial information. Second, the learning capacity of the small CNN module 100 is also extremely limited.
limb				
	EP3579196A1	Human clothing transfer method, system and device		
	US9384448	Action-based models to identify learned tasks	 As shown, the articulated avatar 36 is defined by the position of the standard tree-like LIMB structure composed of connected ellipsoids associated with the head 38, the torso 40, the  upper arms  42 and 44, the  lower arms  46 and 48, the  upper legs  50 and 52, and the  lower legs  54 and 56.	 FIG. 4 illustrates an example of an image 68 that may be input to the agent 20 and is indicative of one example learning environment state 26. As shown in the image 68, the avatar 36 is articulated to illustrate its current state in the environment 22. As shown, the articulated avatar 36 is defined by the position of the standard tree-like limb structure composed of connected ellipsoids associated with the head 38, the torso 40, the  upper arms  42 and 44, the  lower arms  46 and 48, the  upper legs  50 and 52, and the  lower legs  54 and 56. In one embodiment, the three dimensional pose of the avatar 36 is defined by the three dimensional joint angles between connected body parts. During operation, the agent 20 controls the avatar 36 by issuing actions of the form (x, xi), where x refers to a specific joint angle and xi defines a positive or negative increment. Each body part inherits the coordinate system of its parent part. If, for example, the angle connecting the  upper leg  50 or 52 to the torso 40 is changed, the entire leg will rotate accordingly.
	US9448164	Systems and methods for noninvasive blood glucose and other analyte detection and measurement using collision computing	 The DC component of the signal is attributable to the bulk absorption of the skin tissue, while the AC component is directly attributable to variation in blood volume in the tissue caused by the pressure pulse of the cardiac cycle as each cardiac cycle of the heart pumps blood to the LIMBs.	As the tissue is highly perfused, it is relatively easy to detect the pulsatile component of the cardiac cycle. The DC component of the signal is attributable to the bulk absorption of the skin tissue, while the AC component is directly attributable to variation in blood volume in the tissue caused by the pressure pulse of the cardiac cycle as each cardiac cycle of the heart pumps blood to the limbs. Even though this pressure pulse is somewhat damped by the time it reaches the skin, it is enough to distend the arteries and arterioles in the subcutaneous tissue. The cardiac cycle refers to a complete heartbeat from its generation to the beginning of the next beat, and so includes the diastole, the systole, and the intervening pause. The frequency of the cardiac cycle is described by the heart rate, which is typically expressed as beats per minute.
	US9189886	Method and apparatus for estimating body shape	There are several methods for representing body shape with varying levels of specificity: 1) non-parametric models such as visual hulls (Starck and Hilton 2007, Boyer 2006), point clouds and voxel representations (Cheung et al. 2003); 2) part-based models using generic shape primitives such as cylinders or cones (Deutscher and Reid 2005), superquadrics (Kakadiaris and Metaxas 1998; Sminchisescu and Telea 2002) or “metaballs” (Plankers and Fua 2003); 3) humanoid models controlled by a set of pre-specified parameters such as LIMB lengths that are used to vary shape (Grest et al.	There are several methods for representing body shape with varying levels of specificity: 1) non-parametric models such as visual hulls (Starck and Hilton 2007, Boyer 2006), point clouds and voxel representations (Cheung et al. 2003); 2) part-based models using generic shape primitives such as cylinders or cones (Deutscher and Reid 2005), superquadrics (Kakadiaris and Metaxas 1998; Sminchisescu and Telea 2002) or “metaballs” (Plankers and Fua 2003); 3) humanoid models controlled by a set of pre-specified parameters such as limb lengths that are used to vary shape (Grest et al. 2005; Hilton et al. 2000; Lee et al. 2000); 4) data driven models where human body shape variation is learned from a training set of 3D body shapes (Anguelov et al. 2005; Balan et al. 2007a; Seo et al. 2006; Sigal et al. 2007, 2008).
	WO2016193030A1	Sleep monitoring method and system	 Examples of gross body movements of the subject may include whole body movement or LIMB movements.	 Optionally, the method further comprises analyzing the body movement data to detect gross body movements of the subject, and the determining the sleep state of the subject to be the awake-non- vocalization state is further based on the detection of the gross movement. Examples of gross body movements of the subject may include whole body movement or limb movements. In case of, for example, darkness or a particular positioning of the face of the subject, in addition to eye openness it may be more reliable to account for the gross body movements as well. Namely, for example, presence of gross body movements during a prolong period of time may indicate an awake state and absence of gross body movements may indicate sleep. As such, the determining the sleep state of the subject to be the awake-non- vocalization state may be more reliable and accurate when the determining is based on both on the detection of eye openness and gross body movements. 
male				
	US10586372	Online modeling for real-time facial animation	 The identity PCA model may be computed from a data set consisting of 100 MALE and 100 feMALE head scans of young adults, such as the data provided described by V.	An exemplary implementation of one embodiment of the present disclosure may employ a blendshape model of 34 blendshapes. The identity PCA model may be computed from a data set consisting of 100 male and 100 female head scans of young adults, such as the data provided described by V. Blanz and T. Vetter in “A morphable model for the synthesis of 3D faces”, SIGGRAPH 1999. 50 PCA basis vectors could be used to approximate the neutral expression. The corrective deformation fields may be represented by 50 Laplacian eigenvectors for each coordinate. Suitable parameters for the optimizations as discussed above may be set to β2=0.5, β2=0.1, and β3=0.001, as well as λ1=10 and λ2=20, and σ=10 for the coverage threshold.
	US10436342	Flow meter and related method	 A MALE latch component is connected to the first housing component opposite the pivot connection and a feMALE latch component is coupled to the second housing component opposite the pivot connection.	In certain embodiments of the present disclosure, a system for controlling flow through a drip chamber includes a drip chamber holster, an imaging device, a flexible tube, and a valve. The drip chamber holster receives and secures a drip chamber. The imaging device is configured to capture images of the drip chamber and create image data from the captured images. The flexible tube is connected to the drip chamber and the lumen defined by the tube is in fluid communication with the drip chamber. The valve is axially disposed around a portion of the flexible tube and controls flow through the tube and ultimately the drip chamber. The valve includes first and second casing components pivotally connected to each other and complimentarily align to form an enclosure when in a closed position. Inlet and outlet holes are defined in the valve casing when it is closed and a plunger hole is defined in the first casing component. A male latch component is connected to the first housing component opposite the pivot connection and a female latch component is coupled to the second housing component opposite the pivot connection. A substantially incompressible filler is enclosed within the casing. The filler defines a conduit, sized for a specific tube, which connects the inlet and the outlet holes of the valve casing. There are a plurality of variations in the stiffness of the filler. The portion of the filler proximate the tube may be stiffer than the surrounding filler. The plunger is longitudinally aligned with the plunger hole and attached to the actuator. The actuator is configured to actuate the plunger into and out of the plunger hole to engage the filler. Changes in displacement by the plunger alter the forces on the section of the tube within the casing resulting in the lumen changing size. The area of the head of the plunger can be smaller than the longitudinal cross-section of the lumen disposed within the housing.
	US9448164	Systems and methods for noninvasive blood glucose and other analyte detection and measurement using collision computing	 The use of multiple illumination-detection pairs to target the same layer in the skin tissue, as described above (that is by increasing the tomographic states), can expand the glucose measurement capability and increase the ability of the system to compensate for MALE and feMALE subjects with varying skin thickness, surface texture, and changes due to aging, gender, and/or ethnic differences.	The probe of FIG. 91, as detailed in FIG. 92, is designed such that targeting of different skin tissue layers by tomographic sequences EVa1, EVa2, . . . , EVax; DVb1, DVb2, . . . DVby; and, SVc1, SVc2, . . . SVcz can be verified based on examination of acquired spectra based on absorption of spectral bands known to be associated with biochemical compounds other than the analyte of interest, (such as by examination of intensity and absorbance amplitude profiles of the fat, protein and collagen bands as shown in FIG. 93 for different ring illuminations). Profiles of absorption gradients leading to the assessment of acceptable feature pairs is a function of the tomographic sequence and the results of the above expression, based on an anatomical and physiological understanding of skin tissue. The use of multiple illumination-detection pairs to target the same layer in the skin tissue, as described above (that is by increasing the tomographic states), can expand the glucose measurement capability and increase the ability of the system to compensate for male and female subjects with varying skin thickness, surface texture, and changes due to aging, gender, and/or ethnic differences.
	US9189886	Method and apparatus for estimating body shape	 One can fit a gender-neutral body model that is capable of representing MALE or feMALE bodies.	In many applications, the gender of a person being scanned may be known or the user may specify that information. In these cases, body shape using the appropriate gender-specific body model is estimated (Section 3). When gender is not known there are several options. One can fit a gender-neutral body model that is capable of representing male or female bodies. Second, one can fit using both male and female body shape models and select the one that achieves a lower error of the objective function. Third, one can fit a gender-neutral model and then classify gender directly from the estimated shape coefficients, as described in Section 10. Once gender is known, a refined shape estimate using the appropriate gender-specific shape model is produced. The same strategies can be used for other subpopulations (e.g. to infer ethnicity).
	US10665326	Deep proteome markers of human biological aging and methods of determining a biological aging clock	 The group of patients taking both insulin and glucose-lowering drugs and the group taking only glucose-lowering drugs tend to be predicted younger than their chronological age for MALE samples.	Additionally, DNN predictors of biological age can be based on blood test values, such as the blood protein concentrations. FIG. 15 shows an example of a biological age clock or a report thereof. To investigate the predictive ability of deep proteomic clocks on the efficacy of drugs in diseased patients, we explored the log 2aging ratios. Blood samples from the group of diabetic patients were used to predict their biological age. In general, all diabetic patients tended to be predicted to have an older biological age compared to their chronological age. The group of patients taking both insulin and glucose-lowering drugs and the group taking only glucose-lowering drugs tend to be predicted younger than their chronological age for male samples. The difference between groups taking both insulin and glucose-lowering drugs (e.g., first group, far left) and taking insulin only (e.g., second group, middle right) is significant, and the first group is predicted younger than the second group. The first group also tends to be predicted to be biologically aged younger than patients taking neither insulin nor glucose-lowering drugs (e.g., third group, nothing, far right). The difference between groups taking only glucose-lowering drugs (e.g., fourth group, middle left) and taking insulin only (e.g., second group) is also significant, and the fourth group is predicted younger than the second group. Additionally, the fourth group also tends to be predicted younger patients taking neither insulin nor glucose-lowering drugs (e.g., third group).
man				
	US9501824	Non-touch optical detection of vital signs from amplified visual variations of reduced images of skin		
	WO2014098308A1	Method for displaying unified app information based on open app store, and computer readable recording medium therefor		
	WO2017161401A1	Method for the self-location of vehicles		
	US10860683	Pattern change discovery between high dimensional data sets	 Moreover, MANy commonly used metrics, such as L-norms, K-L divergence, or more generally, BregMAN divergence, are defined based on the Euclidean distance.	One may wonder what makes high-dimensional data different when it comes to change detection. For almost all the magnitude change detection methods, an invisible pitfall arises with the increase of data's dimensionality. The tricky conflict between Euclidean distance and dimensionality is illustrated in FIG. 1. Here we use Euclidean distance because it is the most intuitive and popular metric. Moreover, many commonly used metrics, such as L-norms, K-L divergence, or more generally, Bregman divergence, are defined based on the Euclidean distance. FIG. 1 gives two pairs of vectors (v1, v2) and (v′1, v′2), and the angles, θ, θ′ between each pair, respectively. Under Euclidean distance, ∥v1−v2∥, and ∥v1′−v2′∥ are the same. In other words, Euclidean distance fails to detect θ≠θ′, and therefore, is unable to differentiate the length difference from the direction difference introduced by the dimensionality.
	US10049492	Method and apparatus for rendering facades of objects of interest from three-dimensional point clouds	Automatic urban scene object recognition refers to the process of segmentation and classifying of objects of interest in an image into predefined seMANtic labels, such as “building”, “tree” or “road”.	Automatic urban scene object recognition refers to the process of segmentation and classifying of objects of interest in an image into predefined semantic labels, such as “building”, “tree” or “road”. This typically involves a fixed number of object categories, each of which requires a training model for classifying image segments. While many techniques for two-dimensional (2D) object recognition have been proposed, the accuracy of these systems is to some extent unsatisfactory, because 2D image cues are sensitive to varying imaging conditions such as lighting, shadow etc.
military				
	EP3293700A1	3d reconstruction for vehicle		
	US9225889	Photographic image acquisition device and method	 In particular, the amount of computational power that would be required to perform such computations in real-time during image acquisition (as opposed to post-capture) exceeds by orders of magnitude the computational power typically available in most cameras, including high-end cameras used in certain MILITARY applications.	Importantly, post-capture computational super-resolution methods from the prior art as disclosed herein, are computationally expensive. In particular, the amount of computational power that would be required to perform such computations in real-time during image acquisition (as opposed to post-capture) exceeds by orders of magnitude the computational power typically available in most cameras, including high-end cameras used in certain military applications. Moreover, even if the sufficient computational power were available for a real-time super-resolution reconstruction, the additional bandwidth to transmit the super-resolved images and the additional storage space to record them would be impractical in many applications.
	US9940726	System and method to improve object tracking using tracking fingerprints	, a car, a truck, a MILITARY vehicle, a watercraft, an aircraft, an autonomous aircraft, a spacecraft, a missile, a rocket, etc.	The image processing system 102 may be configured to process image data of the sequence of images 140 and to provide object tracking of an object in the sequence of images 140. The object may be indicated based on a user input (e.g., a user may select an object in an image via a user input device) or the object may be selected automatically by the image processing system 102 based on criteria stored at a memory accessible to the image processing system 102. To perform the object tracking, the image processing system 102 may include a first tracking system 110 and a second tracking system 130. The first tracking system 110 may be configured to maintain “tracking fingerprints” corresponding to the object in the sequence of images 140 and to support the second tracking system 130. The second tracking system 130 may be configured to track the object in the sequence of images 140. The object may be a moving object or an object that moves with respect to a field of vision of the image (or video) capture device 104. For example, a moving object may include a person that is walking or running, a vehicle (e.g., a car, a truck, a military vehicle, a watercraft, an aircraft, an autonomous aircraft, a spacecraft, a missile, a rocket, etc.), an object located on a moving vehicle, or some other type of moving object. As another example, the image (or video) capture device 104 may be coupled to an aircraft, such as an autonomous aircraft (e.g., a drone aircraft), and the object may move with respect to the field of vision of the image (or video) capture device 104 due to movement of the aircraft.
	US10554901	Real-time HDR video for vehicle control	 The HDR system may be provided as part of, or for use in, a MILITARY or emergency response vehicle, such as a HUMVEE, tank, jeep, fire truck, police vehicle, ambulance, bomb squad vehicle, troop transport, etc.	The HDR system may be provided as part of, or for use in, an automobile, such as a consumer's “daily driver” or in a ride-sharing or rental car. Such a vehicle will typically have 2 to 7 seats and a form factor such as a sedan, compact SUV or CUV, SUV, wagon, coupe, small truck, roadster, or sports car. Additionally or alternatively, the HDR system may be provided as part of, or for use in, a cargo truck, semi truck, bus, or other load carrying vehicle. The HDR system may be provided as part of, or for use in, a military or emergency response vehicle, such as a HUMVEE, tank, jeep, fire truck, police vehicle, ambulance, bomb squad vehicle, troop transport, etc. The HDR system may be provided as part of, or for use in, a utility vehicle such as a forklift, warehouse robot in a distribution facility, office mail cart, golf cart, personal mobility device, autonomous security vehicle, Hollywood movie dolly, amusement park ride, tracked or trackless mine cart, or others. The HDR system may be provided as part of, or for use in, a non-road-going vehicle, such as a boat, plane, train or submarine. In fact, it may be found that the HDR camera offers particular benefits for vehicles that operate in lighting conditions not well suited to the human eye, such as in the dark, among rapidly flashing lights, extremely bright lights, unexpected or unpredictable lighting changes, flashing emergency lights, light filtered through gels or other devices, night-vision lighting, etc. Thus, compared to vehicles controlled solely by a human, a vehicle using the control system may perform better in environments such as night, underground, Times Square, lightning storms, house fires or forest fires, emergency road conditions, military battles, deep-sea dives, mines, etc.
	US10311297	Determination of position from images and associated camera positions	Point mensuration is required in wide range of MILITARY and non-MILITARY applications, many of which require accurate determination of geographic coordinates in a timely manner.	Point mensuration is required in wide range of military and non-military applications, many of which require accurate determination of geographic coordinates in a timely manner. Such applications include, but are not limited to, indirect targeting of precision guided munitions, in which case extant rules of engagement adhered to by a military unit, derived from both targeting policy and legal constraints, determine the required accuracy, precision and timeliness.
office				
	US10748319	Composite radiographic image that corrects effects of parallax distortion	 Such networking environments are commonplace in OFFICE networks, enterprise-wide computer networks, intranets and the Internet, which are all types of networks.	The computer 900 may operate in a networked environment using logical connections to one or more remote computers. These logical connections may be achieved by a communication device coupled to or integral with the computer 900; the application is not limited to a particular type of communications device. The remote computer may be another computer, a server, a router, a network personal computer, a client, a peer device, or other common network node, and typically includes many or all of the elements described above relative to the computer 900, although only a memory storage device has been illustrated in FIG. 9. The computer 900 can be logically connected to the Internet 972. The logical connection can include a local area network (LAN), wide area network (WAN), personal area network (PAN), campus area network (CAN), metropolitan area network (MAN), or global area network (GAN). Such networking environments are commonplace in office networks, enterprise-wide computer networks, intranets and the Internet, which are all types of networks.
	US10679044	Human action data set generation in a machine learning system	 All videos in the UT Kinects data set were taken in an OFFICE environment with similar lighting conditions and the positions of the camera was fixed.	The UT Kinects data set is a human activity recognition data set developed by researchers from the University of Texas and captured by a KINECT sensor available from Microsoft Corporation. The UT Kinects data set includes 10 action labels including walk, sit-down, stand-up, throw, push, pull, wave-hand, carry, and clap-hands. The UT Kinects data set also includes 10 subjects that perform each of these actions twice in front of a rig of RGB camera and Kinect. This means the UT Kinects data set includes a total of 200 action clips of RGB and depth data (however depth data is ignored for purposes of the following experiments). All videos in the UT Kinects data set were taken in an office environment with similar lighting conditions and the positions of the camera was fixed. For the training setup, 2 random subjects were left out (20%, used for testing) and the experiments were carried out using 80% of the subjects. The reported results are the average of six individual runs. Results of the 6 train/test runs were constant throughout the experiments.
	US9501824	Non-touch optical detection of vital signs from amplified visual variations of reduced images of skin	 This can be particularly advantageous when the host computer system is the hand-held device 2100 subscriber's OFFICE computer system.	The PIM 2142 includes functionality for organizing and managing data items of interest to the user, such as, but not limited to, e-mail, contacts, calendar events, voice mails, appointments, and task items. A PIM application has the ability to transmit and receive data items via the wireless network 2105. PIM data items may be seamlessly integrated, synchronized, and updated via the wireless network 2105 with the hand-held device 2100 subscriber's corresponding data items stored and/or associated with a host computer system. This functionality creates a mirrored host computer on the hand-held device 2100 with respect to such items. This can be particularly advantageous when the host computer system is the hand-held device 2100 subscriber's office computer system.
	US10372982	Methods and apparatus for repetitive iris recognition	Biometric recognition methods are used in the fields of security, protection, financial transaction verification, airports, OFFICE buildings, to determine or verify the identity of an individual person based on captured biometric characteristics of the individual person.	Biometric recognition methods are used in the fields of security, protection, financial transaction verification, airports, office buildings, to determine or verify the identity of an individual person based on captured biometric characteristics of the individual person. Various factors can affect the performance of biometric recognition. For example, variations in pose between the images results in matching errors even if the faces being compared are from the same person. For another example, the dynamic range or sensitivity of the sensor may not be sufficient to capture biometric information related to the face. In addition, the illumination may vary between the images being matched in the face recognition system. Changes in illumination can result in poor match results since detected differences are due to the illumination changes and not to the fact that a different person is being matched.
	US9842266	Method for detecting driver cell phone usage from side-view images	Many of the enacted laws are primary enforcement which means an OFFICEr may cite a driver for using a hand-held mobile phone without any other traffic offense taking place.	Many of the enacted laws are primary enforcement which means an officer may cite a driver for using a hand-held mobile phone without any other traffic offense taking place. However, to enforce the rules, current practice requires dispatching law enforcement officers at the road side to visually examine oncoming cars or having human operators manually examine image/video records to identify violators. Both of the processes are expensive, difficult, and ultimately ineffective. Therefore, there is a need for an automatic or semi-automatic solution.
pedestrian				
	US9363489	Video analytics configuration	 Additionally, cameras may cover crowded areas and may include objects which are occluded by other objects, for example static occlusions where an urban feature occludes a portion of a street thereby occluding PEDESTRIANs and vehicles passing behind the urban feature or where PEDESTRIANs are occluded by each other such as in crowded environments.	Additionally, many video surveillance networks cover a wide geographic area and may have non-overlapping or non-contiguous view field coverage. Additionally, cameras may cover crowded areas and may include objects which are occluded by other objects, for example static occlusions where an urban feature occludes a portion of a street thereby occluding pedestrians and vehicles passing behind the urban feature or where pedestrians are occluded by each other such as in crowded environments.
	CN106339725A	Pedestrian detection method based on scale constant characteristic and position experience	The invention belongs to technical field of image processing, it is related to a kind of PEDESTRIAN detection method.	The invention belongs to technical field of image processing, it is related to a kind of pedestrian detection method.
	EP2960829A2	Lane boundary estimation device and lane boundary estimation method		
	US9467750	Placing unobtrusive overlays in video content	 This way, in a video scene containing, for example, trees waving in wind, a new wind gust or change of direction might not cause much novelty, whereas a PEDESTRIAN or other important object appearing in the video scene would be successfully detected.	Embodiments can detect surprising events in video content 404 and can improve temporal placement of overlays 506 based on the detected surprising events. Certain embodiments detect surprising events in video content based on saliency maps and as part of attention modeling. These embodiments define a surprise framework to incorporate elements from two complementary domains: saliency and novelty. Here, saliency can represent outliers in the spatial domain, while novelty works with the temporal domain of video content. In the novelty framework, one approach is to assume that each pixel's intensity in a frame comes from a mixture of Gaussians distribution. This way, in a video scene containing, for example, trees waving in wind, a new wind gust or change of direction might not cause much novelty, whereas a pedestrian or other important object appearing in the video scene would be successfully detected. Embodiments assume that probability distribution is not known beforehand. Instead, a Bayesian framework can be used so that every new piece of information in video content frames changes the probability distribution that models what the users expect to see. Divergence between the distributions before and after the new information is seen is used as a measure of how much novelty the information contains.
	CN102598113A	Method circuit and system for matching an object or person present within two or more images	"Velastin; "" Colour constancy techniques for re-recognition of PEDESTRIANs from multiple surveillance cameras (discerning constant color technology again) "" from the PEDESTRIAN of a plurality of surveillance cameras; "" Workshop on Multi-camera and Multi-modal Sensor Fusion Algorithms and Applications "" ("" about the symposial of multiple-camera and multi-modal sensor fusion algorithm and application) (M2SFA22008); In October, 2008, Marseille, FRA."	"[2] A.Colombo, J.Orwell and S.Velastin; "" Colour constancy techniques for re-recognition of pedestrians from multiple surveillance cameras (discerning constant color technology again) "" from the pedestrian of a plurality of surveillance cameras; "" Workshop on Multi-camera and Multi-modal Sensor Fusion Algorithms and Applications "" ("" about the symposial of multiple-camera and multi-modal sensor fusion algorithm and application) (M2SFA22008); In October, 2008, Marseille, FRA."
prison				
	CN105787501B	Power transmission line corridor region automatically selects the vegetation classification method of feature		
	CN107330396A	A kind of pedestrian's recognition methods again based on many attributes and many strategy fusion study		
	CN106815854A	A kind of Online Video prospect background separation method based on normal law error modeling		
	CN104537393B	A kind of traffic sign recognition method based on multiresolution convolutional neural networks		
	CN109801312A	More camera motion track monitoring methods, system, equipment and storage medium		
prisoner				
	CN103020655A	Remote identity authentication method based on single training sample face recognition	With reference to Figure of description 1, configure and be people's face acquisition module, obtaining generation by image or sheet image data is other photo, namely the human face photo of left part among the figure; Configuration human face photo storehouse, the library file of the coupling such as the storehouse of pursuing and capturing an escaped PRISONER of normally national people information storehouse, the Ministry of Public Security, bank online checking storehouse also comprises the personnel storehouse such as certain company naturally, its existing often single photo, such as the human face photo of identity document, and expression is often more single.	With reference to Figure of description 1, configure and be people's face acquisition module, obtaining generation by image or sheet image data is other photo, namely the human face photo of left part among the figure; Configuration human face photo storehouse, the library file of the coupling such as the storehouse of pursuing and capturing an escaped prisoner of normally national people information storehouse, the Ministry of Public Security, bank online checking storehouse also comprises the personnel storehouse such as certain company naturally, its existing often single photo, such as the human face photo of identity document, and expression is often more single.In addition, the most long-distance distribution in human face photo storehouse of configuration, deposit part this locality, such as the library file take company as unit, even if intra-company may relate to a plurality of collection points, corresponding also long-range laying of library file.
	US10437884	Navigation of computer-navigable physical feature graph	 For instance, the user might be trying to identify movements of PRISONERs within a prison environment.	Other example time-based relationships include absolute time relationship. For instance, the example relationship might be that the nodes represent activity that occurred at approximately the same time, or at dissimilar time. Again, such absolute time relationships might be expressed via gestures. For instance, a particular gesture might indicate that the user is interested in events that have occurred within five minutes after the events that the user is currently observing. For instance, the user might be trying to identify movements of prisoners within a prison environment.
	US10853901	System and method for visitation management in a controlled environment	 By providing a coordinated system for managing the different forms of visitation, significant burden can be removed from prison staff, while simultaneously enhancing the PRISONER's communication options.	In light of the above, the present disclosure provides a system and method of facilitating different types of visitations in a controlled-access environment. This consists of a system that provides scheduling services to coordinate actual visits, provides the exchange of the electronic communications between the parties, performs monitoring of the audio and/or video for rules violations or other security risks, etc. By providing a coordinated system for managing the different forms of visitation, significant burden can be removed from prison staff, while simultaneously enhancing the prisoner's communication options.
	US10321094	Secure video visitation system	 The rights of these PRISONERs are largely restricted for a number of reasons, such as for their safety and the safety of others, the prevention of additional crimes, as well as simple punishment for crimes committed.	American prisons house millions of individuals in controlled environments all over the country. The rights of these prisoners are largely restricted for a number of reasons, such as for their safety and the safety of others, the prevention of additional crimes, as well as simple punishment for crimes committed. However, these prisoners are still entitled to a number of amenities that vary depending on the nature of their crimes. Such amenities may include phone calls, commissary purchases, access to libraries, digital media streaming, as well as others.
	US9060714	System for detection of body motion	); (iv) monitoring of PRISONERs or soldiers (e.	Applications of particular embodiments include: (i) monitoring activities in elderly people, the disabled and the chronically ill (e.g., remote over a network) for nursing homes and hospitals; (ii) hospital emergency room monitoring for nursing coverage; (iii) diagnosis of diseases (e.g., Parkinson's, etc.); (iv) monitoring of prisoners or soldiers (e.g., where sensors are embedded in uniforms), such as via base station monitoring; (v) athletic training, (vi) monitoring patients in clinical drug studies, (vii) monitoring of animal activities and (viii) machine monitoring. Of course, particular embodiments are also amenable to many more applications.
purchase				
	US10540378	Visual search suggestions	 For example, a user wanting to PURCHASE a product might access an electronic marketplace in order to search the types of products offered through that marketplace.	Users are increasingly utilizing electronic devices to obtain various types of information. For example, a user wanting to purchase a product might access an electronic marketplace in order to search the types of products offered through that marketplace. Unless the user knows an exact brand or style of product that the user wants, however, the user might have to search through hundreds or thousands of different products using various options to attempt to locate the type of product in which the user is interested. If the user is interested in a product of a specific type, the user might have no option but to sift through these results. Further, it can be cumbersome to describe a product and refine the results when using standard text input. In some situations, a user interested in acquiring information about a product can capture an image of the product and submit the captured image to an object recognition system to obtain information associated with the product. However, conventional object recognition approaches may not be able to successfully and/or quickly identify all types of products. In either situation, the process can be time consuming and potentially frustrating for a user, which can result in the user not locating an item of interest and the marketplace not completing a transaction.
	US9189886	Method and apparatus for estimating body shape	When a person PURCHASEs clothing from a retailer (e.	When a person purchases clothing from a retailer (e.g. over the Internet) using their body model, the size and brand information may be (optionally) stored with their body model. This information may be entered manually by the user with a graphical interface or automatically by software that collects the retail purchase information. Optionally the user can provide one or more ratings of the item related to its fit or other properties and these may be stored in the database in association with the clothing entry.
	US10534808	Architecture for responding to visual query	 The bar code search result may include one result, the name of the product corresponding to that bar code, or the bar code results may include several results such as a variety of places in which that product can be PURCHASEd, reviewed, etc.	For example, if a visual query was a photograph of a bar code, there may be portions of the photograph which are irrelevant parts of the packaging upon which the bar code was affixed. The interactive results document may include a bounding box around only the bar code. When the user selects inside the outlined bar code bounding box, the bar code search result is displayed. The bar code search result may include one result, the name of the product corresponding to that bar code, or the bar code results may include several results such as a variety of places in which that product can be purchased, reviewed, etc.
	US10841551	User feedback for real-time checking and improving quality of scanned image	 Document 106 may be any suitable document that user 104 desires to image using smartphone 102, such as a page from a book or a magazine, a business card, a PURCHASE receipt, or any other type of document.	In this example, the object is a document 106 and the image frames are assembled into a composite image representing a scan of document 106. Document 106 may be any suitable document that user 104 desires to image using smartphone 102, such as a page from a book or a magazine, a business card, a purchase receipt, or any other type of document. Document 106 may also be held by user 104 or located at a distance from user 104, and it is not a requirement that document 106 be placed on surface 108.
	US10554901	Real-time HDR video for vehicle control	 Additionally the ceaseless roiling of endless waves may afford no PURCHASE to the balancing tools of the inner ear, causing a human operator to lack the kinesthetic sense necessary to correctly perceive an absolute frame of reference including down and up and left and right.	Embodiments of the invention provide an HDR system for a boat. The system includes an HDR camera operable to produce a real-time HDR video and a processing system. The processing system communicates with the HDR camera and a control system of the boat. Using the HDR video, the processing system determines an appearance of an item in an environment of the boat and issues to the control system an instruction to control the boat based on the appearance of the item. Boats are essentially surrounded by water and do not offer the same visual cues as roadways. Swells and valleys among waves, with breaking crests and sea foam in the air can present a scene of sudden bright sun glints and rapidly changing contrast scenes without the types of anchor points the human eye expects to see. A human may be so consumed attempting to navigate a harbor as to lack the residual attention to understand the water's urges. Moreover, reading the crests and troughs and what all the buoys signify may be difficult for a human due essentially to strange patterns (both spatial and temporal) of visual contrast in the great volume of the sea. Additionally the ceaseless roiling of endless waves may afford no purchase to the balancing tools of the inner ear, causing a human operator to lack the kinesthetic sense necessary to correctly perceive an absolute frame of reference including down and up and left and right. The HDR system may read surfaces of the waves, detect and interpret buoys and other navigational markers, and aid in controlling the boat. Optionally, the HDR system may communicate with a global positioning system, compass, level, or other such instruments to maintain an absolute reference frame. The system may interact with, or include, sonar systems that can read the ocean floor or look forward to obstacles. The processing system can synthesize this information and offer such useful benefits as, for example, an autopilot mode that drives a boat from slip to sea, navigating out of the harbor.
recommend				
	US9189886	Method and apparatus for estimating body shape	 The matching and ranking means can be used to make selective RECOMMENDations based on similar body shapes.	Finally, a means for body shape matching takes a body produced from some measurements (tailoring measures, images, range sensor data) and returns one or more “scores” indicating how similar it is in shape to another body or database of bodies. This matching means is used to rank body shape similarity to, for example, reorder a display of attributes associated with a database of bodies. Such attributes might be items for sale, information about preferred clothing sizes, images, textual information or advertisements. The display of these attributes presented to a user may be ordered so that the presented items are those corresponding to people with bodies most similar to theirs. The matching and ranking means can be used to make selective recommendations based on similar body shapes. The attributes (e.g. clothing size preference) of people with similar body shapes can be aggregated to recommend attributes to a user in a form of body-shape-sensitive collaborative filtering.
	US9208543	Deblurring images having spatially varying blur	 The blur kernel sizing module 220 may include an autocorrelation-based size estimator 222 to estimate a first size of the blur kernel, a latent image-based size estimator 224 to estimate a second size of the blur kernel, and process the results to suggest or RECOMMEND a size of the blur kernel to be used to deblur the image.	As described herein, example systems and methods are described that may automatically estimate or otherwise determine the size of a blur kernel used to deblur at least one region of a blurred image. FIG. 2 is a block diagram of example components (or modules) of a blur kernel sizing engine 150, in accordance with an example embodiment. The example components may be hardware, software, or a combination of hardware and software, and may be executed by one or more processors. In the example embodiment, the blur kernel sizing engine 150 is shown to include an image module 210 and a blur kernel sizing module 220. The blur kernel sizing module 220 may include an autocorrelation-based size estimator 222 to estimate a first size of the blur kernel, a latent image-based size estimator 224 to estimate a second size of the blur kernel, and process the results to suggest or recommend a size of the blur kernel to be used to deblur the image. It is however to be noted that the autocorrelation-based size estimator 222 and the latent image-based size estimator 224 are merely examples of estimators, and that other mathematical techniques may be used in other embodiments.
	US9715902	Audio-based annotation of video	Furthermore, the computer system may provide a RECOMMENDation for another content item (or multiple other content items) that includes additional audio information and additional video information based on the determined annotation items.	Furthermore, the computer system may provide a recommendation for another content item (or multiple other content items) that includes additional audio information and additional video information based on the determined annotation items. Alternatively or additionally, the computer system may provide a rating of the content item based on the determined annotation items. This rating may indicate: an estimated popularity of the content item, quality of the audio information, quality of the video information, and/or quality of the content item. Note that the recommendation and/or the rating may be determined and/or provided separately for the content item and/or the audio information and the video information.
	CN104732491A	Edge priority guide single-frame remote sensing image super-resolution processing method		
	US9418482	Discovering visited travel destinations from a set of digital images	 Further, action suggestions can RECOMMEND a book or video that describes the depicted landmark that might be of interest to the user, or a link for the user to write an email to a friend from her social network that has traveled to the depicted landmark.	When a user views images that depict the travels of her connections within the social networking system, it is common for her to desire to travel to one or more of the landmarks she has viewed. Referring again to FIG. 6, action suggestions 609 can be placed in association with the geo-recognized image 333. These action suggestions can provide a suggested action for the user: to plan travel with one or more partner sites (e.g., the fictitious TravelHelper or SeeUSA travel sites). Further, action suggestions can recommend a book or video that describes the depicted landmark that might be of interest to the user, or a link for the user to write an email to a friend from her social network that has traveled to the depicted landmark. In certain embodiments, the action suggestion can be a link to one or more partner sites, including travel, tourism, or review/rating websites, and can be provided to the user for the purpose of aiding the user to travel to the destination or venue depicted in the image. The partner sites can provide payment for appearing as an action suggestion for the user.
reidentification				
	US9535563	Internet appliance system and method	 Thus, once an object is identified, an expected change in that object will not necessitate a REIDENTIFICATION of the object.	In a preferred embodiment, a model contained in a database includes a three or more dimensional representation of an object. These models include information processed by a fractal-based method to encode repetitive, transformed patterns in a plane, space, time, etc., as well as to include additional degrees of freedom, to compensate for changes in morphology of the object, to allow continuous object identification and tracking. Thus, once an object is identified, an expected change in that object will not necessitate a reidentification of the object. According to one embodiment, a fractal-like processing is executed by optical elements of an optical or optical hybrid computer. Further, in order to temporarily store an optical image, optically active biological molecules, such as bacteriorhodopsins, etc. may be used. Liquid crystals or other electrophotorefractive active materials may also used. These imagers may be simple two dimensional images, holograms, or other optical storage methods. A preferred holographic storage method is a volume phase hologram, which will transform an impressed image, based on hologram to image correlation. Thus, these models would be somewhat linear transform independent, and would likely show some (planar) transform relationship. Thus, an optical computer may be advantageous because of its high computational speed as compared to digital computers for image analysis, due to inherent parallelism and high inherent speed.
	USRE46310	Ergonomic man-machine interface incorporating adaptive pattern recognition based control system	 Thus, once an object is identified, an expected change in that object will not necessitate a REIDENTIFICATION of the object.	In a preferred embodiment, a model contained in a database includes a three or more dimensional representation of an object. These models include information processed by a fractal-based method to encode repetitive, transformed patterns in a plane, space, time, etc., as well as to include additional degrees of freedom, to compensate for changes in morphology of the object, to allow continuous object identification and tracking. Thus, once an object is identified, an expected change in that object will not necessitate a reidentification of the object. According to one embodiment, a fractal-like processing is executed by optical elements of an optical or optical hybrid computer. Further, in order to temporarily store an optical image, optically active biological molecules, such as bacteriorhodopsins, etc. may be used. Liquid crystals or other electrophotorefractive active materials may also used. These imagers may be simple two dimensional images, holograms, or other optical storage methods. A preferred holographic storage method is a volume phase hologram, which will transform an impressed image, based on hologram to image correlation. Thus, these models would be somewhat linear transform independent, and would likely show some (planar) transform relationship. Thus, an optical computer may be advantageous because of its high computational speed as compared to digital computers for image analysis, due to inherent parallelism and high inherent speed.
	US9607245	Adapted vocabularies for matching image signatures with fisher vectors	In accordance with another aspect of the exemplary embodiment, a method for generating a system for object REIDENTIFICATION includes providing a universal generative model generated using local descriptors extracted from images in a training set.	In accordance with another aspect of the exemplary embodiment, a method for generating a system for object reidentification includes providing a universal generative model generated using local descriptors extracted from images in a training set. The universal generative model is adapted to a first camera to obtain a first camera-dependent generative model using local descriptors extracted from images captured by the first camera. The universal generative model is adapted to a second camera to obtain a second camera-dependent generative model using local descriptors extracted from images captured by the first camera. A component is provided for computing at least one of an image-level representation of a first image using the first camera-dependent generative model, and an image-level representation of a first image using the first camera-dependent generative model. A component is provided for computing a similarity between the first image-level descriptor and the second image-level descriptor.
	US10361802	Adaptive pattern recognition based control system and method	 Thus, once an object is identified, an expected change in that object will not necessitate a REIDENTIFICATION of the object.	In a preferred embodiment, a model contained in a database includes a three or more dimensional representation of an object. These models include information processed by a fractal-based method to encode repetitive, transformed patterns in a plane, space, time, etc., as well as to include additional degrees of freedom, to compensate for changes in morphology of the object, to allow continuous object identification and tracking. Thus, once an object is identified, an expected change in that object will not necessitate a reidentification of the object. According to one embodiment, a fractal-like processing process is executed by optical elements of an optical or optical hybrid computer. Further, in order to temporarily store an optical image, optically active biological molecules, such as bacteriorhodopsins, etc. may be used. Liquid crystals or other electrophotorefractive active materials may also used. These imagers may be simple two dimensional images, holograms, or other optical storage methods. A preferred holographic storage method is a volume phase hologram, which will transform an impressed image, based on hologram to image correlation. Thus, these models would be somewhat linear transform independent, and would likely show some (planar) transform relationship. Thus, an optical computer may be advantageous because of its high computational speed as compared to digital computers for image analysis, due to inherent parallelism and high inherent speed.
	CN107944340A	A kind of combination is directly measured and the pedestrian of indirect measurement recognition methods again		
security				
	US9483689	Biometric matching technology	 The system 300 also determines whether the context of the situation is in a crime-sensitive area or jewelry store, whether the context of the situation is in an airport or railway station, whether the context of the situation is in a public transport location, a public park, or vehicular surveillance, or whether the context of the situation is in home SECURITY or school campus SECURITY.	The system 300 references the data structure 1500 in setting the batch size. For example, the system 300 determines the context and criticality of the situation based on user input, a pre-defined setting, and/or an alert feed (e.g., a threat level alert provided by a government organization or other organization). In this example, the system 300 determines whether the criticality of the situation is very high, high, medium, or low. The system 300 also determines whether the context of the situation is in a crime-sensitive area or jewelry store, whether the context of the situation is in an airport or railway station, whether the context of the situation is in a public transport location, a public park, or vehicular surveillance, or whether the context of the situation is in home security or school campus security. Based on the determinations, the system 300 references the data structure 1500 and sets the batch size as the number of images defined by the data structure 1500. For instance, the system 300 selects fifty images based on determining that the criticality is very high and the context is in a crime-sensitive area or jewelry store, selects twenty images based on determining that the criticality is high and the context is in an airport or railway station, selects ten images based on determining that the criticality is medium and the context is in a public transport location, a public park, or vehicular surveillance, and selects five images based on determining that the criticality is low and the context is in home security or school campus security.
	US10679044	Human action data set generation in a machine learning system	 Thus, this new data generation technique can improve the performance of current machine learning-based human action recognition computer vision systems, which may be used in, for example, SECURITY surveillance, sports analysis, smart home devices, and health monitoring.	The disclosed solution enables generation of human action video clips from a small set of original human action video clips without costly and time-consuming data acquisition. Moreover, by independently modeling action, subject, and context, the disclosed solution can generate any number of human action videos with varying backgrounds, human appearances, actions, and ways each action is performed. Further, through use of the generative model trained on a small labeled dataset of skeleton trajectories of human actions as described above with the first aspect, the system described herein may allow the use of unlimited unlabeled human action videos in training. Thus, this new data generation technique can improve the performance of current machine learning-based human action recognition computer vision systems, which may be used in, for example, security surveillance, sports analysis, smart home devices, and health monitoring.
	US10540749	System and method for learning-based image super-resolution	 For example, the application device can perform SECURITY applications.	Additionally, or alternatively, the output interface includes an application interface adapted to connect the image processing system 300 to an application device that can operate based on results of image upsampling and super-resolution. For example, the application device can perform security applications. For example, the application device can be operatively connected to the image generator trained by the image processing system 300 and being configured to upsample an input image using the image generator and perform a control action based on the upsampled input image.
	WO2019212749A1	Stabilizing video to reduce camera and face movement		
	US9292969	Dimensioning system calibration systems and methods	 Such functionality may include system SECURITY settings, system configuration settings, language preferences, dimension and volume preferences, and the like.	The processor(s) 220 can execute one or more instruction sets that are stored in whole or in part in the non-transitory, machine-readable storage media 218. The machine executable instruction set(s) can include instructions related to basic functional aspects of the one or more processors 220, for example data transmission and storage protocols, communication protocols, input/output (“I/O”) protocols, USB protocols, and the like. Machine executable instruction sets related to all or a portion of the calibration and dimensioning functionality of the dimensioning system 110 and intended for execution by the processor(s) 220 while in calibration or pre-run time mode, in run time mode, or combinations thereof may also be stored within the one or more non-transitory, machine-readable storage media 218, of the processor(s) 220, or within both the non-transitory, machine-readable storage media 218 and the processor(s) 220. Additional dimensioning system 110 functionality may also be stored in the form of machine executable instruction set(s) in the non-transitory, machine-readable storage media 218. Such functionality may include system security settings, system configuration settings, language preferences, dimension and volume preferences, and the like.
sex				
	CN106999111A	System and method for detecting invisible human emotion		
	WO2018134829A1	Method for sex sorting of mosquitoes and apparatus therefor		
	CN107704800A	Face identification method, system and its application system		
	CN103164858A	Adhered crowd segmenting and tracking methods based on superpixel and graph model	S13, build a weighted graph model according to human body prior shape information and colouring information on the super pixel segmentation figure of prospect, find optimal segmentation border between the adhesion target by the method for seeking optimal path, concrete steps are: S131, build a body shape model, and human body target is carried out template matches; S132, the limit on pixel segmentation figure super according to prospect and the shape difference of the object module opposite SEX build the shape weight on border; S133, the color distortion on pixel segmentation figure super according to prospect between every limit adjacent area block of pixels builds the color weight on border; S134 determines that one cuts apart starting point and end point, finds the path of Least-cost on weighted graph, is the optimal segmentation border of adhesion human body.	S13, build a weighted graph model according to human body prior shape information and colouring information on the super pixel segmentation figure of prospect, find optimal segmentation border between the adhesion target by the method for seeking optimal path, concrete steps are: S131, build a body shape model, and human body target is carried out template matches; S132, the limit on pixel segmentation figure super according to prospect and the shape difference of the object module opposite sex build the shape weight on border; S133, the color distortion on pixel segmentation figure super according to prospect between every limit adjacent area block of pixels builds the color weight on border; S134 determines that one cuts apart starting point and end point, finds the path of Least-cost on weighted graph, is the optimal segmentation border of adhesion human body.
	US10665326	Deep proteome markers of human biological aging and methods of determining a biological aging clock	 Many studies analyzing transcriptomes or proteomes of biopsies in a variety of diseases indicated that age and SEX of the patient have significant effects on gene expression and subsequent protein production and that there are noticeable changes in gene expression with age in mice, resulting in development of mouse aging gene expression databases and in humans.	Many biomarkers of aging have been proposed including telomere length, intracellular and extracellular aggregates, racemization of the amino acids and genetic instability. Gene expression and DNA methylation profiles change during aging, which also may be used as biomarkers of aging. As a result, protein production profiles that are translated from the genetically expressed mRNA may correspondingly be used as biomarkers of aging. Many studies analyzing transcriptomes or proteomes of biopsies in a variety of diseases indicated that age and sex of the patient have significant effects on gene expression and subsequent protein production and that there are noticeable changes in gene expression with age in mice, resulting in development of mouse aging gene expression databases and in humans.
sexuality				
	EP2960828A1	Face authentication device and face authentication method		
social network				
	US10586372	Online modeling for real-time facial animation	 Compared to communication via recorded video streams that only offer limited ability to alter the appearance of users, facial animation opens the door to fascinating new applications in computer gaming, SOCIAL NETWORKs, television, training, customer support, or other forms of online interactions.	Recent advances in real-time performance capture have brought within reach a new form of human communication. Capturing dynamic facial expressions of users and re-targeting these facial expressions on digital characters enables a communication using virtual avatars with live feedback. Compared to communication via recorded video streams that only offer limited ability to alter the appearance of users, facial animation opens the door to fascinating new applications in computer gaming, social networks, television, training, customer support, or other forms of online interactions. However, a successful deployment of facial animation technology at a large scale puts high demands on performance and usability.
	US8923570	Automated memory book creation	 Facial recognition may include the use of a database of known individuals, such a collection of images from a SOCIAL NETWORK, or a locally compiled database of images where individuals are pre-identified.	At 312, an image score may be determined or adjusted based on the identity of one or more individuals in an image through the use of facial recognition processes to determine the identity of individuals in the image. Facial recognition may include the use of a database of known individuals, such a collection of images from a social network, or a locally compiled database of images where individuals are pre-identified. An image score may be calculated or adjusted based on the frequency with which an individual appears in a set of images that are being analyzed, or the frequency in which images of the individual appears in a user's social network or local collection of images.
	US9357123	Image defocus blur estimation	, Snapfish®, Shutterfly®, and the like), photo storage and/or sharing services Flickr®), SOCIAL NETWORK services (e.	The environment 100 further depicts one or more service providers 112, configured to communicate with computing device 102 over a network 114, such as the Internet, to provide a “cloud-based” computing environment. Generally speaking, service providers 112 are configured to make various resources 116 available over the network 114 to clients. In some scenarios, users may sign up for accounts that are employed to access corresponding resources from a provider. The provider may authenticate credentials of a user (e.g., username and password) before granting access to an account and corresponding resources 116. Other resources 116 may be made freely available, (e.g., without authentication or account-based access). The resources 116 can include any suitable combination of services and/or content typically made available over a network by one or more providers. Some examples of services include, but are not limited to, photo editing services, image illustrating services, photo printing services (e.g., Snapfish®, Shutterfly®, and the like), photo storage and/or sharing services Flickr®), social network services (e.g., Facebook®, Twitter®, Instagram®, Hyperlapse®, and the like), and so forth.
	WO2019212749A1	Stabilizing video to reduce camera and face movement		
	US10129464	User interface for creating composite images	” The user interface may provide one or more user interface elements (fields) corresponding to one or more target destinations, such as an electronic storage, a server of a SOCIAL NETWORK site (e.	 FIG. 5C illustrate an exemplary interface for sharing composite images. The composite image displayed in the user interface may correspond to the composite image selected by the user (e.g., via the slider element shown in FIGS. 5A, 5B). The user interface may provide context for the sharing operation via text “SHARE THIS PHOTO.” The user interface may provide one or more user interface elements (fields) corresponding to one or more target destinations, such as an electronic storage, a server of a social network site (e.g., Facebook, Twitter, Instagram, etc.), a data server (e.g., Vimeo, Dropbox), and/or other destinations. Responsive to a user's selection of sharing the selected composite image(s), the selected composite image(s) may be communicated to one or more target destination via wired and/or wireless communications interface.
street				
	US9363489	Video analytics configuration	 Additionally, cameras may cover crowded areas and may include objects which are occluded by other objects, for example static occlusions where an urban feature occludes a portion of a STREET thereby occluding pedestrians and vehicles passing behind the urban feature or where pedestrians are occluded by each other such as in crowded environments.	Additionally, many video surveillance networks cover a wide geographic area and may have non-overlapping or non-contiguous view field coverage. Additionally, cameras may cover crowded areas and may include objects which are occluded by other objects, for example static occlusions where an urban feature occludes a portion of a street thereby occluding pedestrians and vehicles passing behind the urban feature or where pedestrians are occluded by each other such as in crowded environments.
	US9729787	Camera calibration and automatic adjustment of images	A large portion of consumer photos contain man-made structures, such as urban scenes with buildings and STREETs, and indoor scenes with walls and furniture.	A large portion of consumer photos contain man-made structures, such as urban scenes with buildings and streets, and indoor scenes with walls and furniture. However, photographing these structures properly is not an easy task. Photos taken by amateur photographers often contain slanted buildings, walls, and horizon lines due to improper camera rotations, as shown in the left column of FIG. 1.
	WO2017071926A1	Adaptive view projection for a vehicle camera system	The motion of the approaching vehicle 34 is parallel to the STREET surface, so the focus of expansion 33 is at the same height as the camera with respect to the STREET surface 16.	The motion of the approaching vehicle 34 is parallel to the street surface, so the focus of expansion 33 is at the same height as the camera with respect to the street surface 16. In a situation as shown in Fig. 6, in which the vehicles move along straight trajectories, it can also be concluded that the lines 32 through the focus of expansion are parallel to the lateral boundaries of the approaching vehicle 34 and at a right angle to the front and rear boundaries of the approach¬ ing vehicle 34. 
	US8265344	Electronic manifest of underground facility locate operation	 For example, a telephone cable located two and a half meters behind the curb of a residential STREET would be documented as being offset two and a half meters behind the curb.	It is generally recommended, or in some jurisdictions required, to document the type and number of underground facilities located, i.e. telephone, power, gas, water, sewer, etc., and the approximate geographic location of the locate marks. Often times it is also recommended or required to document the distance, or “offset” of the locate marks from environmental landmarks that exist at the dig area. An environmental landmark may include any physical object that is likely to remain in a fixed location for an extended period of time. Examples of an environmental landmark may include a tree, a curb, a driveway, a utility pole, a fire hydrant, a storm drain, a pedestal, a water meter box, a manhole lid, a building structure (e.g., a residential or office building), or a light post. For example, a telephone cable located two and a half meters behind the curb of a residential street would be documented as being offset two and a half meters behind the curb. These offsets serve as evidence supporting the location of the locate marks after those locate marks may have been disturbed by the excavation process.
	EP3579196A1	Human clothing transfer method, system and device		
surveil				
	US10250809	Video stabilization system and method	 For example, if a camera is mounted to SURVEIL an intersection, one does not want the scene to shift due to pedestrians crossing the street.	In many typical applications, it is desirable to remove background vibration while not being influenced by the movement of foreground objects. For example, if a camera is mounted to surveil an intersection, one does not want the scene to shift due to pedestrians crossing the street. In a surgical scenario, it is undesirable for surgical tools in the field of view to cause the image to shift while it is being stabilized. This is a challenging problem to solve as tool movement is unconstrained. Also, tools are generally highly textured. They are often more textured than the background scenery, which may be smooth or out of focus. In surgical applications, the subject will also not generally be still. For example, tissue is deformable and will pulse with blood flow and while being operated on. All of the previously mentioned influences can cause a stabilization algorithm to react to the outliers and introduce unnatural scene motion. The introduction of unwanted scene motion can be viewed as an artifact of stabilization.
	US9727785	Method and apparatus for tracking targets	 As one example, an unmanned aerial vehicle (UAV) may be used to SURVEIL an area and track a target of interest that is on ground.	Sensor devices are oftentimes used to generate sensor data for the purpose of tracking targets of interest. Target tracking may be performed in a number of different ways. As one example, an unmanned aerial vehicle (UAV) may be used to surveil an area and track a target of interest that is on ground. The target of interest may be, for example, but is not limited to, a ground vehicle.
	US9330315	Determining foregroundness of an object in surveillance video data	The present invention relates generally to the field of digital image processing of video SURVEILlance, and more specifically to identifying objects as either foreground or background.	The present invention relates generally to the field of digital image processing of video surveillance, and more specifically to identifying objects as either foreground or background.
	US9715639	Method and apparatus for detecting targets	 As one example, an unmanned aerial vehicle (UAV) may be used to SURVEIL an area and detect any targets of interest that are on ground.	Target detection may be performed in a number of different ways. As one example, an unmanned aerial vehicle (UAV) may be used to surveil an area and detect any targets of interest that are on ground. The target of interest may be, for example, but is not limited to, a ground vehicle, a person, or some other type of target.
	US10176405	Vehicle re-identification techniques using neural networks for image analysis, viewpoint-aware pattern recognition, and generation of multi- view vehicle representations	 This technology is useful in a variety of different contexts, such as SURVEILlance systems and intelligent transportation systems.	Generally speaking, vehicle re-identification (“vehicle re-ID”) is a technology that aims to identify a vehicle of interest across images taken by multiple cameras. For example, after a vehicle has been captured by a first camera and has exited the field-of-view (FOV) of the first camera, vehicle re-identification technologies attempt to re-identify the vehicle when it enters the FOV of other cameras. This technology is useful in a variety of different contexts, such as surveillance systems and intelligent transportation systems.
surveillance				
	US10540378	Visual search suggestions	 Examples of such client devices include personal computers, cell phones, handheld messaging devices, laptop computers, set-top boxes, personal data assistants, electronic book readers, SURVEILLANCE cameras, cameras on vehicles, helmets and glasses, and the like.	As discussed, different approaches can be implemented in various environments in accordance with the described embodiments. For example, FIG. 9 illustrates an example of an environment 900 for implementing aspects in accordance with various embodiments. As will be appreciated, although a Web-based environment is used for purposes of explanation, different environments may be used, as appropriate, to implement various embodiments. The system includes electronic client devices 902, which can include any appropriate device operable to send and receive requests, messages or information over an appropriate network 904 and convey information back to a user of the device. Examples of such client devices include personal computers, cell phones, handheld messaging devices, laptop computers, set-top boxes, personal data assistants, electronic book readers, surveillance cameras, cameras on vehicles, helmets and glasses, and the like. The network can include any appropriate network, including an intranet, the Internet, a cellular network, a local area network or any other such network or combination thereof. The network could be a “push” network, a “pull” network, or a combination thereof. In a “push” network, one or more of the servers push out data to the client device. In a “pull” network, one or more of the servers send data to the client device upon request for the data by the client device. Components used for such a system can depend at least in part upon the type of network and/or environment selected. Protocols and components for communicating via such a network are well known and will not be discussed herein in detail. Communication over the network can be enabled via wired or wireless connections and combinations thereof. In this example, the network includes the Internet, as the environment includes a Web server 906 for receiving requests and serving content in response thereto, although for other networks, an alternative device serving a similar purpose could be used, as would be apparent to one of ordinary skill in the art.
	US9808549	System for detecting sterile field events and related methods	 In previous research projects we have developed methods for dynamic networks of cameras for SURVEILLANCE [5, 28-30, 33-39].	Sensor Fusion: A crucial step in sensor fusion is the alignment and registration between sensors [10], and the ability to deal with reconfiguration of a dynamic network of sensors [35,36]. Individual raw point-clouds from different sensors can be aligned using either image-domain features (such as SIFT, HoG, STIP, etc.), or 3D features such as the ones that we proposed in [15] based on the eigenvectors of structure tensors. The space-time alignment allows for fusing of information across multiple sensors, including in particular first-person viewpoints. In previous research projects we have developed methods for dynamic networks of cameras for surveillance [5, 28-30, 33-39]. While that work provides a strong heritage to this disclosure, there are discernible differences that make this disclosure of particular interest, e.g. multiple sensor modalities (depth/IR/visible), and the fusion of first-person perspectives with third-person perspectives. For example, when employed together, multiple RGB-D sensors experience crosstalk, as each device projects its own structured light patterns over its field of view; Butler et al. [9] and Maimone and Fuchs [46] independently developed a simple hardware solution to remove such interference via gentle vibrations of the sensors. The fusion of information provided by multiple RGB-D sensors will allow for the recovery of depth and color/IR information throughout the scene. As the number of sensors viewing a particular object increases, the certainty about the object's size and position increases.
	US10679047	System and method for pose-aware feature learning		
	US9363489	Video analytics configuration	 In particular, but not exclusively, embodiments of the present invention relate to video SURVEILLANCE networks.	The present invention relates to apparatus for analyzing a sequence of video frames, a system utilizing such apparatus and a method of operating such apparatus and system. In particular, but not exclusively, embodiments of the present invention relate to video surveillance networks.
	WO2013183738A1	Information processing device, information processing method, program, and surveillance camera system		
torso				
	US9451927	Computed tomography data-based cycle estimation and four-dimensional reconstruction	 The acquisition may be designed for any application, such as a TORSO scan covering from a patient's neck or shoulder region to the hips or lower TORSO.	In act 20, data representing a patient is obtained. A CT scanner or system scans the patient. Alternatively, a CT-like or other x-ray system is used. A source and opposing detector are moved by a gantry relative to a patient. During movement or at stoppage points over a range of movement, x-ray projection images are acquired. Any scanning sequence or approach may be used, such as a helical or ciné acquisition. The acquisition may be designed for any application, such as a torso scan covering from a patient's neck or shoulder region to the hips or lower torso. Greater or lesser extent of the scan may be provided.
	US9020210	Image processing system, image processing apparatus, image processing method, and program	 The part Part 1 corresponds to the head, the part Part 2 corresponds to the TORSO, the part Part 3 corresponds to the right upper arm, the part Part 4 corresponds to the left upper arm, the part Part 5 corresponds to the right lower arm, the part Part 6 corresponds to the left lower arm, the part Part 7 corresponds to the right upper leg, the part Part 8 corresponds to the left upper leg, the part Part 9 corresponds to the right lower leg, and the part Part 10 corresponds to the left lower leg.	The human pose recognition engine 25 detects the pose of a human in an image on the basis of 10 parts Part 1 to Part 10 as shown in FIG. 7. The part Part 1 corresponds to the head, the part Part 2 corresponds to the torso, the part Part 3 corresponds to the right upper arm, the part Part 4 corresponds to the left upper arm, the part Part 5 corresponds to the right lower arm, the part Part 6 corresponds to the left lower arm, the part Part 7 corresponds to the right upper leg, the part Part 8 corresponds to the left upper leg, the part Part 9 corresponds to the right lower leg, and the part Part 10 corresponds to the left lower leg.
	US9904845	Body feature detection and human pose estimation using inner distance shape contexts	 For example, if the right arm is in front of the TORSO in the depth image, the right elbow and right wrist would be inside the human silhouette and not on the contour.	Due to reasons such as occlusions, some of the feature points may be inside the human silhouette. For example, if the right arm is in front of the torso in the depth image, the right elbow and right wrist would be inside the human silhouette and not on the contour. In one embodiment, in addition to detecting feature points along the contour of the human silhouette, the pose estimation system 100 applies additional techniques to detect feature points that fall inside the human silhouette, such as skeleton analysis and depth slicing analysis. Further information of the additional techniques for detecting features in the human figure is found in U.S. patent application Ser. No. 12/455,257, filed May 29, 2009, titled “Controlled Human Pose Estimation From Depth Image Streams”, and U.S. patent application Ser. No. 12/317,369, filed Dec. 19, 2008, entitled “Controlled Human Pose Estimation From Depth Image Streams”, both of which are incorporated by reference herein in their entirety. The feature points detected using different approaches can be treated as possible feature candidates and fed to the ambiguity resolve module 210 for selection. In addition, the pose estimation system 100 may augment the detected features with predicted features p obtained from forward kinematics computations of the reconstructed pose.
	US9262674	Orientation state estimation device and orientation state estimation method	 By way of example, let us suppose a case where both arms of a person are covered by the outline of his/her TORSO as viewed from a camera.	However, the related art cannot distinguish between a plurality of posture states having similar silhouettes, and thus has a problem in that the posture states of persons cannot be estimated accurately. By way of example, let us suppose a case where both arms of a person are covered by the outline of his/her torso as viewed from a camera. In this case, according to the related art, if the entire outline including the outlines of his/her head and legs is the same, the same posture state is obtained as the estimation result, regardless of whether the arms are bent or stretched and whether the arms are located in front or in back.
	US9384448	Action-based models to identify learned tasks	 The avatar 36 has a set of connected body parts including a head 38, a TORSO 40, two  upper arms  42 and 44, two  lower arms  46 and 48, two  upper legs  50 and 52, and two  lower legs  54 and 56.	The method 10 of FIG. 1 may be better understood by considering a system 18 of FIG. 2 that illustrates an embodiment of a reinforcement learning framework that supports the “learn by discovery” method 10. As shown, the system 18 includes an agent 20 that is connected to a learning environment 22 via perception and action. In the embodiment illustrated herein, the learning environment 22 is based on a simulated three dimensional world populated with various objects 34 of interest and an articulated three dimensional avatar 36. The avatar 36 has a set of connected body parts including a head 38, a torso 40, two  upper arms  42 and 44, two  lower arms  46 and 48, two  upper legs  50 and 52, and two  lower legs  54 and 56. The learning environment 22 state (s) 26 consists of the location and orientation of all three dimensional objects, the avatar 36 body parts and their associated joint angles. The agent 20 may control the avatar 36 by issuing adjustment instructions for specific joint angles in terms of positive or negative increments. The learning environment 22 combines these agent actions with additive noise resulting in adjustments to the articulation of the avatar 36. Further, an input (i) 24 is a set of synthetic images of the three dimensional world that are produced by a set of virtual cameras 58. It should be noted that in order to control the avatar 36, the agent 20 may need to employ a form of depth perception, and in one embodiment, this depth perception may be achieved by ensuring that the set of synthetic images is no less than two.
underage				
	WO2012031631A2	Method for finding and digitally evaluating illegal image material		
woman				
	US10311326	Systems and methods for improved image textures	 The WOMAN's skin tone 1006 in image 1001 b is also improved relative to the skin tone 906 in image 901 b.	 FIG. 10 shows the two images of FIG. 9 after processing by the disclosed methods and systems. The two images 1001 a-b, corresponding to images 901 a-b in FIG. 9, show improved detail when compared to the images 901 a-b of FIG. 9. For example, facial features such the lips 1005 a-b of FIG. 10 are improved relative to lips 905 a-b of FIG. 9. The woman's skin tone 1006 in image 1001 b is also improved relative to the skin tone 906 in image 901 b.  
	US10534808	Architecture for responding to visual query	 For example, in FIG. 14, the label hovering over the image of a WOMAN drinking includes a link to facial recognition results for the WOMAN and a link to image recognition results for that particular picture (e.	 FIG. 14 illustrates a client device 102 with a screen shot of an interactive results document 1200 with labels 1402 being the visual identifiers of respective sub-portions of the visual query 1102 of FIG. 11. The label visual identifiers 1402 each include a user selectable link to a subset of corresponding search results. In some embodiments, the selectable link is identified by descriptive text displayed within the area of the label 1402. Some embodiments include a plurality of links within one label 1402. For example, in FIG. 14, the label hovering over the image of a woman drinking includes a link to facial recognition results for the woman and a link to image recognition results for that particular picture (e.g., images of other products or advertisements using the same picture.)
	US9692984	Methods and systems for content processing	 If Tony is sitting in a bar, and his eye falls on a bottle of unusual beer in front of a nearby WOMAN, the system can identify his point of focal attention, and focus its own processing efforts on pixels corresponding to that bottle.	Eye tracking technology can be employed to identify which object in a field of view captured by an experiential-video sensor is of interest to the user. If Tony is sitting in a bar, and his eye falls on a bottle of unusual beer in front of a nearby woman, the system can identify his point of focal attention, and focus its own processing efforts on pixels corresponding to that bottle. With a signal from Tony, such as two quick eye-blinks, the system can launch an effort to provide candidate responses based on that beer bottle—perhaps also informed by other information gleaned from the environment (time of day, date, ambient audio, etc.) as well as Tony's own personal profile data. (Gaze recognition and related technology is disclosed, e.g., in Apple's patent publication 20080211766.)
	WO2014098308A1	Method for displaying unified app information based on open app store, and computer readable recording medium therefor		
	US10657652	Image matting using deep learning	, a person with dark hair on a white background, or a person not wearing green against a green screen), such methods cannot adequately handle complex real-life images, for example, a WOMAN wearing fur in front of a cityscape with buildings, cars, and trees.	Existing approaches generally implemented to create a matte for an image heavily rely on utilizing color to determine alpha values for pixels. That is, each of the propagation-based method, sampling-based method, and deep learning techniques described above rely on color to determine alpha values. Mattes for complex images with similar foreground/background textures and/or colors produced by such existing, color-based approaches, however, are oftentimes unsatisfactory to a user, as the resulting mattes often contain low frequency smearing or high frequency chunky artifacts. While the existing methods may perform well for high-contrast foreground/background images (e.g., a person with dark hair on a white background, or a person not wearing green against a green screen), such methods cannot adequately handle complex real-life images, for example, a woman wearing fur in front of a cityscape with buildings, cars, and trees.
youth				
	US9105210	Multi-node poster location	One issue regarding the education of children and YOUTH involves facilitating and encouraging the reading of stories, as well as improving reading comprehension.	One issue regarding the education of children and youth involves facilitating and encouraging the reading of stories, as well as improving reading comprehension. Moreover, complex stories including multiple characters and subplots (e.g., a Shakespeare play) may be confusing to inexperienced readers or otherwise difficult to follow thereby preventing the readers from fully enjoying the reading experience. Thus, there is a need for an augmented reality system capable of generating and displaying holographic visual aids related to a story in order to enhance the reading experience of the story and to reward the reading of the story.
	US10438052	Systems and methods for facial property identification	 In another example, the process for establishing the age classifier based on at least information associated with the second textural features in the training image sample collection includes: dividing the gender samples of each race by age groups to obtain infant training samples, child training samples, YOUTH training samples and senior training samples; training the infant training samples, the child training samples, the YOUTH training samples and the senior training samples to establish a first-level age classifier; training fifth textural features of second-level training samples associated with an age interval of five years related to the infant training samples, the child training samples, the YOUTH training samples and the senior training samples to establish a second-level age classifier; and training a linearly-fit third-level age classifier for the infant training samples, the child training samples, the YOUTH training samples and the senior training samples based on at least information associated with an age interval of five years.	According to another embodiment, the process for establishing the race classifier based on at least information associated with the second textual features in the training image sample collection includes: dividing the training sample collection into white training samples, black training samples and yellow training samples corresponding to a white race, a black race and a yellow race respectively; and training third textural features of the white training samples, the black training samples and the yellow training samples separately to obtain a ternary classifier associated with the white race, the black race and the yellow race. For example, the process for establishing the gender classifier based on at least information associated with the second textural features in the training image sample collection includes: dividing the white training samples, the black training samples and the yellow training samples by gender to obtain gender samples of each race; and training fourth textural features of the gender samples of each race separately to obtain a gender binary classifier associated with each race, wherein the binary classifier corresponds to male and female. In another example, the process for establishing the age classifier based on at least information associated with the second textural features in the training image sample collection includes: dividing the gender samples of each race by age groups to obtain infant training samples, child training samples, youth training samples and senior training samples; training the infant training samples, the child training samples, the youth training samples and the senior training samples to establish a first-level age classifier; training fifth textural features of second-level training samples associated with an age interval of five years related to the infant training samples, the child training samples, the youth training samples and the senior training samples to establish a second-level age classifier; and training a linearly-fit third-level age classifier for the infant training samples, the child training samples, the youth training samples and the senior training samples based on at least information associated with an age interval of five years.
	US10102543	Methods, systems, and devices for handling inserted data into captured images	, an image of a YOUTH soccer team playing out on the field) from a wearable computer (e.	Referring again to FIG. 12A, operation 1210 may include operation 1212 depicting receiving the captured image from a wearable computer that is configured to covertly capture one or more images, wherein the captured image depicts at least one entity. For example, FIG. 7, e.g., FIG. 7A, shows captured image that depicts at least one entity receiving from a wearable computer device that is configured to covertly capture one or more images module 712 receiving the captured image (e.g., an image of a youth soccer team playing out on the field) from a wearable computer (e.g., a Google Glass device being worn by the goalie of one of the teams) that is configured to covertly capture one or more images (e.g., the image of the youth soccer team), wherein the captured image (e.g., the image of a youth soccer team playing out on the field) depicts at least one entity (e.g., one of the persons on the soccer field).
	US9524081	Synchronizing virtual actor's performances to a speaker's voice	One issue regarding the education of children and YOUTH involves facilitating and encouraging the reading of stories, as well as improving reading comprehension.	One issue regarding the education of children and youth involves facilitating and encouraging the reading of stories, as well as improving reading comprehension. Moreover, complex stories including multiple characters and subplots (e.g., a Shakespeare play) may be confusing to inexperienced readers or otherwise difficult to follow thereby preventing the readers from fully enjoying the reading experience. Thus, there is a need for an augmented reality system capable of generating and displaying holographic visual aids related to a story in order to enhance the reading experience of the story and to reward the reading of the story.
	US10672140	Video monitoring method and video monitoring system	 For example, as to the identity feature attribute information of age, it may be quantified as children (0), juvenile (1), YOUTH (2), middle-aged (3), old-aged (4) and the like; as to the identity feature attribute information of gender, it may be quantified as male (0) and female (1); as to style of dress, it may be quantified as business attire (0), casual wear (1), sports wear (2), uniforms (3) and the like; height may be quantified as high (0), short (1) and the like; weight can be quantified as overweight (0), normal (1), underweight (2) and the like; hair color may be quantified as black (0), white (1), other color (2) and the like; hair length may be quantified as long hair (0), medium hair (1), short hair (2), super short hair (3) and the like, so as to implement multi-classification of each type of identify feature attribute information.	As will be described in detail below by making reference to the drawings, in an embodiment of the present disclosure, feature information of the at least one target object located in the second position is extracted based on the second video data by using a neural network (e.g., a convolution neural network and a feedback neural network). The feature information of the at least one target object includes, but not limited to, identify feature information, action feature information etc. The identify feature information includes, but not limited to, attribute information such as gender, age, style of dress, height, weight, hair color, hair length etc. Further, as to each identify feature attribute information, it may be quantified as a multi-classification issue. For example, as to the identity feature attribute information of age, it may be quantified as children (0), juvenile (1), youth (2), middle-aged (3), old-aged (4) and the like; as to the identity feature attribute information of gender, it may be quantified as male (0) and female (1); as to style of dress, it may be quantified as business attire (0), casual wear (1), sports wear (2), uniforms (3) and the like; height may be quantified as high (0), short (1) and the like; weight can be quantified as overweight (0), normal (1), underweight (2) and the like; hair color may be quantified as black (0), white (1), other color (2) and the like; hair length may be quantified as long hair (0), medium hair (1), short hair (2), super short hair (3) and the like, so as to implement multi-classification of each type of identify feature attribute information. The action feature information includes, but not limited to, skeleton information and action information. Similar to the identify feature information, each type of action feature information may also be quantified as a multi-classification issue. For example, an action activity of the target object in the stores may be quantified as viewing commodities (0), stopping (1), experiencing commodities (2) and the like. Therefore, as for the multi-classification issue of the extracted object feature information such as the identify feature information and the action feature information etc., associated error functions may be used respectively to adjust parameters of the neural network and train the neural network. In an embodiment, a first neural network (e.g., a first convolution neural network) and a second neural network (e.g., a second feedback neural network) are used to extract identify feature information of the target object based on the second video data. In the process of training the first neural network and the second neural network, loss functions generated by calculating each type of identify feature attribute information may be integrated together to adjust the first neural network and the second neural network and thereby obtain a trained first neural network and a trained second neural network for extracting the identity feature information of the target object. In an embodiment, a third neural network (e.g., a third convolution neural network) and a fourth neural network (e.g., a fourth feedback neural network) are used to extract action feature information of the target object based on the second video data. In the process of training the third neural network and the fourth neural network, loss functions generated by calculating each type of action feature information may be integrated together, to adjust the third neural network and the fourth neural network and thereby obtain a trained third neural network and a trained fourth neural network for extracting the action feature information of the target object.
