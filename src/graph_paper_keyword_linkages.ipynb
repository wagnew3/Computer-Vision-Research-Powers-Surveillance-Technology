{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "890796d8-29ee-464e-94a9-a1ff3294c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from optparse import OptionParser\n",
    "import os\n",
    "import compress_pickle as pickle\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "save_loc=\"/gscratch/prl/wagnew3/microsoft_academic_graph/\"\n",
    "patents_save_loc=\"/gscratch/prl/wagnew3/microsoft_academic_graph/patents/\"\n",
    "data_dir=\"/gscratch/prl/wagnew3/microsoft_academic_graph/\"\n",
    "num_papers=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cef54a0-b3dc-4b53-a202-2a204bb72bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                | 0/42 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Authors.txt\n",
      "loading from cached file /gscratch/prl/wagnew3/microsoft_academic_graph/cached_Authors_-1.lz4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▋                                                                   | 1/42 [02:59<2:02:31, 179.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading PaperFieldsOfStudy.txt\n",
      "loading from cached file /gscratch/prl/wagnew3/microsoft_academic_graph/cached_PaperFieldsOfStudy_-1.lz4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|████▉                                                                | 3/42 [12:56<2:54:01, 267.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading FieldsOfStudy.txt\n",
      "loading from cached file /gscratch/prl/wagnew3/microsoft_academic_graph/cached_FieldsOfStudy_-1.lz4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████▎                            | 25/42 [12:57<04:36, 16.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Affiliations.txt\n",
      "loading from cached file /gscratch/prl/wagnew3/microsoft_academic_graph/cached_Affiliations_-1.lz4\n",
      "loading _pcs.tsv\n",
      "loading from cached file /gscratch/prl/wagnew3/microsoft_academic_graph/cached__pcs_-1.lz4\n",
      "loading Papers.txt\n",
      "loading from cached file /gscratch/prl/wagnew3/microsoft_academic_graph/cached_Papers_-1.lz4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████████████████████████████▍                  | 31/42 [15:12<03:18, 18.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading PaperAuthorAffiliations.txt\n",
      "loading from cached file /gscratch/prl/wagnew3/microsoft_academic_graph/cached_PaperAuthorAffiliations_-1.lz4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████████████████████████████████████████████████████████████▌        | 37/42 [17:27<01:36, 19.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading _pcs_mag_doi_pmid.tsv\n",
      "loading from cached file /gscratch/prl/wagnew3/microsoft_academic_graph/cached__pcs_mag_doi_pmid_-1.lz4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 42/42 [17:42<00:00, 25.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mag\n",
      "len(all_mags) 39942894\n",
      "40393300 paper patent linkages loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 40393300/40393300 [04:23<00:00, 153515.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 2632492 patents\n"
     ]
    }
   ],
   "source": [
    "if True:#not os.path.exists(os.path.join(save_loc, 'cvpr_patents_graph_crawl_list.lz4')):\n",
    "    loaded_dict=utils.load_mag(data_dir, n_papers=num_papers)\n",
    "    cvpr_graph=pickle.load(open(os.path.join(data_dir, f'cvpr_graph_-1.lz4'), 'rb'))\n",
    "\n",
    "    ''' Save list of patents directly citing papers from conferences.\n",
    "\n",
    "    args:\n",
    "        loaded_dict: dict of MAG dataframes from load_mag\n",
    "        subgraph: subgraph with MAGS to get patents for in keys\n",
    "    returns:\n",
    "        patents, dict MAG : citing patents and metadata\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "    subgraph=cvpr_graph\n",
    "    all_mags=list(subgraph.keys())\n",
    "    for patents in list(subgraph.values()):\n",
    "        all_mags+=patents\n",
    "    all_mags=set(all_mags)\n",
    "    print(f'len(all_mags) {len(all_mags)}')\n",
    "    papers_patents=loaded_dict['_pcs_mag_doi_pmid.tsv']\n",
    "    print(f'{len(papers_patents)} paper patent linkages loaded')\n",
    "    papers_patents_old=loaded_dict['_pcs.tsv']\n",
    "    paper_to_patent={}\n",
    "    patents=[]\n",
    "\n",
    "    for row_ind in tqdm(range(len(papers_patents))):\n",
    "        # https://onlinelibrary.wiley.com/doi/10.1111/jems.12455 Table 1, 0.63% error\n",
    "        paper_mag=str(papers_patents['magid'][row_ind])\n",
    "        if paper_mag in all_mags:\n",
    "            patent_id=str(papers_patents['patent'][row_ind])\n",
    "            if paper_mag not in paper_to_patent:\n",
    "                paper_to_patent[paper_mag]=[patent_id]\n",
    "            else:\n",
    "                paper_to_patent[paper_mag].append(patent_id)\n",
    "            patents.append(patent_id)\n",
    "\n",
    "    # for row_ind in tqdm(range(len(papers_patents_old))):\n",
    "    #     paper_mag=str(papers_patents_old['magid'][row_ind])\n",
    "    #     if paper_mag in all_mags:\n",
    "    #         patent_id=str(papers_patents_old['patent'][row_ind])\n",
    "    #         if paper_mag not in paper_to_patent:\n",
    "    #             paper_to_patent[paper_mag]=[patent_id]\n",
    "    #         else:\n",
    "    #             paper_to_patent[paper_mag].append(patent_id)\n",
    "    #         patents.append(patent_id)\n",
    "\n",
    "    # Get list of unique patents\n",
    "    patents_list=list(set(patents))\n",
    "\n",
    "    pickle.dump(patents_list, open(os.path.join(save_loc, 'cvpr_patents_graph_crawl_list.lz4'), 'wb'))\n",
    "else:\n",
    "    patents_list=pickle.load(open(os.path.join(save_loc, 'cvpr_patents_graph_crawl_list.lz4'), 'rb'))\n",
    "    \n",
    "print(f'downloading {len(patents_list)} patents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c24024-f050-4d6b-94c3-5b1ee2792639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get metadata for all patents\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing import Pool\n",
    "\n",
    "patents_info_save_loc=os.path.join(save_loc, 'patents')\n",
    "all_parallel_runs=[]\n",
    "patents_scraped_info={}\n",
    "patents_list=patents_list[:10000]\n",
    "\n",
    "scrape_args=[]\n",
    "for patent in tqdm(patents_list):\n",
    "    #patents_scraped_info.update(utils.scrape_patents([patent], patents_save_loc))\n",
    "    scrape_args.append(([patent], patents_save_loc, False, True))\n",
    "\n",
    "# # for i in range(10):\n",
    "# #     utils.scrape_patents(scrape_args[i])\n",
    "    \n",
    "# with ThreadPoolExecutor(max_workers=80) as p:\n",
    "#     results=p.map(utils.scrape_patents, scrape_args)\n",
    "#     print(f'len(results {len(results)}')\n",
    "#     for result in tqdm(results):\n",
    "#         u=0\n",
    "        \n",
    "with Pool(processes=64) as pool, tqdm(total=len(scrape_args)) as pbar: # create Pool of processes (only 2 in this example) and tqdm Progress bar\n",
    "    for data in pool.imap_unordered(utils.scrape_patents, scrape_args):                   # send urls from all_urls list to parse() function (it will be done concurently in process pool). The results returned will be unordered (returned when they are available, without waiting for other processes)\n",
    "        pbar.update()                           \n",
    "\n",
    "# pool = mp.Pool(processes=80, maxtasksperchild=1)\n",
    "# for patent in tqdm(patents_list):\n",
    "#     #patents_scraped_info.update(utils.scrape_patents([patent], patents_save_loc))\n",
    "#     run=pool.apply_async(utils.scrape_patents, args=([patent], patents_save_loc, False, True))\n",
    "#     all_parallel_runs.append(run)\n",
    "\n",
    "# scrapped_succesfully=0\n",
    "# num_runs=len(all_parallel_runs)\n",
    "# for run_num in tqdm(range(num_runs)):\n",
    "#     run=all_parallel_runs[run_num]\n",
    "#     all_parallel_runs[run_num]=None\n",
    "#     run.get()\n",
    "    \n",
    "    \n",
    "#     patent_id=list(ret_dict.keys())[0]\n",
    "#     patent_info=ret_dict[patent_id]\n",
    "#     if 'title' in patent_info:\n",
    "#         scrapped_succesfully+=1\n",
    "#         patent_info_row=[patent_id, patent_info['title'], patent_info['inventor_name'], patent_info['assignee_name_orig'], patent_info['assignee_name_current'],\n",
    "#                          patent_info['pub_date'], patent_info['priority_date'], patent_info['grant_date'], patent_info['filing_date'],\n",
    "#                          patent_info['abstract_text'], patent_info['patent_full_url'], patent_info['classification_code_descss'], patent_info['patent_text']] \n",
    "\n",
    "#         pickle.dump(patents_scraped_info, open(os.path.join(patents_info_save_loc, f'{patent_id}.lz4'), 'wb'))\n",
    "\n",
    "        \n",
    "#patents_scraped_info=utils.scrape_patents(patents_list)\n",
    "\n",
    "\n",
    "# patents_info_csv=[['patent_id',\n",
    "#                    'title',\n",
    "#                    'inventor_name',\n",
    "#             'assignee_name_orig',\n",
    "#             'assignee_name_current',\n",
    "#             'pub_date',\n",
    "#             'priority_date',\n",
    "#             'grant_date',\n",
    "#             'filing_date',\n",
    "#             'abstract_text',\n",
    "#             'patent_full_url']]\n",
    "\n",
    "\n",
    "# scrapped_succesfully=0\n",
    "# for patent_id in patents_scraped_info:\n",
    "#     patent_info=patents_scraped_info[patent_id]\n",
    "#     # patent_pdf=requests.get()\n",
    "\n",
    "\n",
    "\n",
    "#     # with open(, 'w') as f:\n",
    "#     #     f.write(patent_pdf)\n",
    "#     if 'title' in patent_info:\n",
    "#         scrapped_succesfully+=1\n",
    "#         patent_info_row=[patent_id, patent_info['title'], patent_info['inventor_name'], patent_info['assignee_name_orig'], patent_info['assignee_name_current'],\n",
    "#                          patent_info['pub_date'], patent_info['priority_date'], patent_info['grant_date'], patent_info['filing_date'],\n",
    "#                          patent_info['abstract_text'], patent_info['patent_full_url'], patent_info['classification_code_descss'], patent_info['patent_text']] \n",
    "#     patents_info_csv.append(patent_info_row )\n",
    "\n",
    "print(f'saved {len(patents_info_csv)} patents')\n",
    "\n",
    "# with open(os.path.join(save_loc, patents_file_name+'_patents_metadata.tsv'), 'w', newline='') as csvfile:\n",
    "#     csvwriter = csv.writer(csvfile, delimiter='\\t', quotechar='|', quoting=csv.QUOTE_ALL)\n",
    "#     csvwriter.writerows(patents_info_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3afe6221-bd20-4813-a667-58d5d112528d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 26/26 [07:23<00:00, 17.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2549890 patents\n"
     ]
    }
   ],
   "source": [
    "patents_info_dict={}\n",
    "agg_data_dir=\"/gscratch/scrubbed/wagnew3/data/\"\n",
    "for agg_file in tqdm(os.listdir(agg_data_dir)):\n",
    "    if \"patent_agg\" in agg_file:\n",
    "        patent_info_sub=pickle.load(os.path.join(agg_data_dir, agg_file))\n",
    "        patents_info_dict.update(patent_info_sub)\n",
    "print(f\"{len(patents_info_dict)} patents\")\n",
    "\n",
    "#make MAG ID to conference and journal id dicts\n",
    "subgraph=pickle.load(open(os.path.join(data_dir, f'cvpr_graph_-1.lz4'), 'rb'))\n",
    "mag_to_conf_id={}\n",
    "mag_to_journal_id={}\n",
    "mag_to_title={}\n",
    "conf_save_path=os.path.join(save_loc, 'mag_conf_journ_title.lz4')\n",
    "if not os.path.exists(conf_save_path):\n",
    "    papers=loaded_dict[\"Papers.txt\"]\n",
    "    for paper_ind in tqdm(range(0, len(papers))):\n",
    "        mag_id=str(int(papers['PaperId'][paper_ind]))\n",
    "        mag_to_conf_id[mag_id]=str(papers['ConferenceSeriesId'][paper_ind])\n",
    "        mag_to_journal_id[mag_id]=str(papers['JournalId'][paper_ind])\n",
    "        title=str(papers['PaperTitle'][paper_ind])\n",
    "        mag_to_title[mag_id]=title\n",
    "    pickle.dump((mag_to_conf_id, mag_to_journal_id, mag_to_title), open(conf_save_path, 'wb'))\n",
    "else:\n",
    "    mag_to_conf_id, mag_to_journal_id, mag_to_title=pickle.load(conf_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c87cf6-9052-4c1e-90d3-f63994a2eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conference_keywords(patents_info_dict, stored_patents_keywords_dict, patent_id, keywords, \n",
    "                            depth_vec, depth, citing_patents_depth_zero, paper_mag):\n",
    "    patent_id=patent_id.replace(\"-\", \"\")\n",
    "    if not patent_id in stored_patents_keywords_dict:\n",
    "        stored_patents_keywords_dict[patent_id]={}\n",
    "        for keyword_ind in range(len(keywords)):\n",
    "            keyword=keywords[keyword_ind]\n",
    "            stored_patents_keywords_dict[patent_id][keyword]=0 \n",
    "            #if patent_id not in patent_keywords_dict:\n",
    "                # Search patent for keywords\n",
    "            keyword_in_patent=0\n",
    "            if patent_id in patents_info_dict:\n",
    "                patent_info=patents_info_dict[patent_id]\n",
    "                if isinstance(patent_info, list):\n",
    "                    if keyword in patent_info[1] or keyword in patent_info[12]:\n",
    "                        stored_patents_keywords_dict[patent_id][keyword]=1\n",
    "                    else:\n",
    "                        stored_patents_keywords_dict[patent_id][keyword]=0\n",
    "    \n",
    "    for keyword_ind in range(len(keywords)):\n",
    "        keyword=keywords[keyword_ind]\n",
    "        if stored_patents_keywords_dict[patent_id][keyword]==1:\n",
    "            depth_vec[keyword_ind,0,depth]+=1\n",
    "            if depth==0:\n",
    "                citing_patents_depth_zero[paper_mag][keyword].append(patent_id)\n",
    "        else:\n",
    "            depth_vec[keyword_ind,1,depth]+=1\n",
    "\n",
    "\n",
    "#compute distribution of citation distance to keyword\n",
    "dist_to_keyword=np.zeros(1000000+1)\n",
    "#CVPR\n",
    "conference_series_ids=['1158167855']\n",
    "\n",
    "\n",
    "def update_from_stored(mag_id, patent_keywords_dict, depth_vec, \n",
    "                       patent_in_linkage_count, paper_in_linkage_count,\n",
    "                      store_depth, depth):\n",
    "    '''\n",
    "    Update keyword search with stored search info from mag_id.\n",
    "    '''\n",
    "    store_depth_vec, store_papers, store_patents=patent_keywords_dict[mag_id]\n",
    "    # if paper_in_linkage_count is not None:\n",
    "    #     # Update patent counts\n",
    "    #     for store_patent_id in store_patents:\n",
    "    #         if store_patent_id not in patent_in_linkage_count:\n",
    "    #             patent_in_linkage_count[store_patent_id]=0\n",
    "    #         patent_in_linkage_count[store_patent_id]+=store_patents[store_patent_id]\n",
    "    #     # Update paper counts\n",
    "    #     for store_mag_id in store_papers:\n",
    "    #         if store_mag_id not in paper_in_linkage_count:\n",
    "    #             paper_in_linkage_count[store_mag_id]=0\n",
    "    #         paper_in_linkage_count[store_mag_id]+=store_papers[store_mag_id]\n",
    "    # Update depth vec with stored keyword counts\n",
    "    depth_vec[:,:,depth:]+=store_depth_vec\n",
    "    return depth_vec\n",
    "\n",
    "def find_keyword(subgraph, mag_id, patent_keywords_dict, patents_info_dict, \n",
    "                 depth, keywords, paper_in_linkage_count, patent_in_linkage_count,\n",
    "                 stored_patents_keywords_dict, citing_patents_depth_zero,\n",
    "                 max_depth, store_depth=2, visted_patents=None, depth_vec=None,\n",
    "                 mag_id_keyword_depth_vecs=None):\n",
    "    '''\n",
    "    paper_in_linkage_count: number of times a paper is in a citation chain\n",
    "    patent_in_linkage_count: number of times a patent is in citation chain\n",
    "    '''\n",
    "    depth_vec=np.zeros((len(keywords),2,max_depth))#{keyword:np.zeros((2,max_depth)) for keyword in keywords}#np.zeros((2,max_depth))\n",
    "    if depth==0:\n",
    "        citing_patents_depth_zero[mag_id]={keyword:[] for keyword in keywords}\n",
    "    if depth>=max_depth:\n",
    "        return depth_vec\n",
    "    # if depth_vec is None:\n",
    "    #     # vector of citation depth at which patent with keyword is found\n",
    "    #     depth_vec=np.zeros(max_depth)\n",
    "    \n",
    "    #Check if results are stored\n",
    "    if max_depth-depth==store_depth and mag_id in patent_keywords_dict:\n",
    "        depth_vec=update_from_stored(mag_id, patent_keywords_dict, depth_vec, \n",
    "                                     patent_in_linkage_count, paper_in_linkage_count,\n",
    "                                    store_depth, depth)\n",
    "        #print(\"cached\")\n",
    "        return depth_vec\n",
    "    elif max_depth-depth==store_depth:\n",
    "        # If search is deep enough to cache results, create cache data structures\n",
    "        copy_depth_vec=np.copy(depth_vec)\n",
    "        depth_vec=np.zeros((len(keywords),2,max_depth))#{keyword:np.zeros((2,max_depth)) for keyword in keywords}\n",
    "        copy_paper_in_linkage_count=paper_in_linkage_count\n",
    "        copy_patent_in_linkage_count=patent_in_linkage_count\n",
    "        paper_in_linkage_count={}\n",
    "        patent_in_linkage_count={}\n",
    "        \n",
    "    # Check if any citing patents contain keyword\n",
    "    if mag_id in paper_to_patent:\n",
    "        prev_sub_keyword_depth=np.copy(depth_vec)\n",
    "        citing_patents=paper_to_patent[mag_id]\n",
    "        # print('citing_patents', citing_patents)\n",
    "        # if visted_patents is not None:\n",
    "        #     visted_patents+=citing_patents\n",
    "        for citing_patent in citing_patents:\n",
    "            get_conference_keywords(patents_info_dict, stored_patents_keywords_dict, citing_patent, keywords, \n",
    "                                        depth_vec, depth, citing_patents_depth_zero, mag_id)\n",
    "            # if res==1:\n",
    "            #     depth_vec[0,depth]+=1\n",
    "            #     if patent_in_linkage_count is not None:\n",
    "            #         if citing_patent not in patent_in_linkage_count:\n",
    "            #             patent_in_linkage_count[citing_patent]=0\n",
    "            #         patent_in_linkage_count[citing_patent]+=1\n",
    "            # else:\n",
    "            #     depth_vec[1,depth]+=1\n",
    "        # if paper_in_linkage_count is not None:\n",
    "        #     if mag_id not in paper_in_linkage_count:\n",
    "        #         paper_in_linkage_count[mag_id]=0\n",
    "        #     paper_in_linkage_count[mag_id]+=np.sum(depth_vec[0])-np.sum(prev_sub_keyword_depth)\n",
    "\n",
    "    min_depth=1000000\n",
    "    if depth+1<max_depth and mag_id in subgraph:\n",
    "        for sub_mag_id in subgraph[mag_id]:\n",
    "            # print(f\"sub {sub_mag_id}\")\n",
    "            sub_depth_vec=find_keyword(subgraph, sub_mag_id, patent_keywords_dict, \n",
    "                                   patents_info_dict, depth+1, keywords,\n",
    "                                   paper_in_linkage_count, patent_in_linkage_count, stored_patents_keywords_dict,\n",
    "                                   citing_patents_depth_zero, max_depth=max_depth, visted_patents=visted_patents, \n",
    "                                   depth_vec=depth_vec, mag_id_keyword_depth_vecs=mag_id_keyword_depth_vecs)\n",
    "            # print(depth_vec, sub_depth_vec)\n",
    "            depth_vec+=sub_depth_vec\n",
    "            # if paper_in_linkage_count is not None:\n",
    "            #     if mag_id not in paper_in_linkage_count:\n",
    "            #         paper_in_linkage_count[mag_id]=0\n",
    "            #     paper_in_linkage_count[mag_id]+=np.sum(sub_depth_vec[0])\n",
    "    \n",
    "    if max_depth-depth==store_depth:\n",
    "        # Update with cached data structures if caching\n",
    "        \n",
    "        store_depth_vec=np.copy(depth_vec)[:,:,depth:]\n",
    "        # for keyword in depth_vec:\n",
    "        #     store_depth_vec[keyword]=np.copy(depth_vec[keyword])[:,depth:]\n",
    "        # print(\"store_depth_vec\", store_depth_vec)\n",
    "        patent_keywords_dict[mag_id]=(store_depth_vec, paper_in_linkage_count, patent_in_linkage_count)\n",
    "        paper_in_linkage_count=copy_paper_in_linkage_count\n",
    "        patent_in_linkage_count=copy_patent_in_linkage_count\n",
    "        depth_vec=copy_depth_vec\n",
    "        depth_vec=update_from_stored(mag_id, patent_keywords_dict, depth_vec, \n",
    "                                     patent_in_linkage_count, paper_in_linkage_count,\n",
    "                                    store_depth, depth)\n",
    "\n",
    "    if mag_id in mag_id_keyword_depth_vecs:\n",
    "        if len(mag_id_keyword_depth_vecs[mag_id][0,0])<max_depth-depth:\n",
    "            mag_id_keyword_depth_vecs[mag_id]=depth_vec[:,:,depth:]\n",
    "    else:\n",
    "        mag_id_keyword_depth_vecs[mag_id]={}\n",
    "        mag_id_keyword_depth_vecs[mag_id]=depth_vec[:,:,depth:]\n",
    "        \n",
    "    return depth_vec\n",
    "\n",
    "#Singlethreaded\n",
    "def analyze_graph(keywords):\n",
    "    keywords=keywords\n",
    "    max_depth=3\n",
    "    # How many patents with keyword citing paper at cite dist\n",
    "    cited_count_vec=np.zeros((2,max_depth))\n",
    "    # If any patents with keyword citing paper at cite dist\n",
    "    cited_pres_vec=np.zeros((2,max_depth))\n",
    "    paper_in_linkage_count={}\n",
    "    patent_in_linkage_count={}\n",
    "    patent_keywords_dict={}\n",
    "    stored_patents_keywords_dict={keyword:{} for keyword in keywords}\n",
    "    mag_id_keyword_depth_vecs={}\n",
    "    citing_patents_depth_zero={}\n",
    "    run=0\n",
    "    for mag_id in tqdm(subgraph):\n",
    "        conf_id=mag_to_conf_id[mag_id]\n",
    "        journal_id=mag_to_journal_id[mag_id]\n",
    "        # If paper is in target venues\n",
    "        if conf_id in conference_series_ids or journal_id in conference_series_ids:\n",
    "            keyword_depth_vec=find_keyword(subgraph, mag_id, patent_keywords_dict, \n",
    "                                           patents_info_dict , 0, keywords, \n",
    "                                           paper_in_linkage_count, patent_in_linkage_count,\n",
    "                                           stored_patents_keywords_dict, citing_patents_depth_zero,\n",
    "                                           max_depth=max_depth, mag_id_keyword_depth_vecs=mag_id_keyword_depth_vecs)\n",
    "            #print(keyword_depth_vec)\n",
    "            mag_id_keyword_depth_vecs[mag_id]=keyword_depth_vec\n",
    "            # cited_count_vec+=keyword_depth_vec\n",
    "            # cited_pres_vec+=1\n",
    "            # #print(f\"keyword_depth {keyword_depth}\")\n",
    "            # if np.sum(keyword_depth_vec[0])==0:\n",
    "            #     dist_to_keyword[-1]+=1\n",
    "            # else:\n",
    "            #     dist_to_keyword[np.argmax(keyword_depth_vec[0]!=0)]+=1\n",
    "            run+=1\n",
    "            # break\n",
    "    print('num papers searched from', run)\n",
    "    # print(dist_to_keyword, np.sum(dist_to_keyword))\n",
    "    # print(dist_to_keyword[:5])\n",
    "    # print(cited_count_vec, cited_pres_vec, cited_count_vec/cited_pres_vec)\n",
    "    return keywords, mag_id_keyword_depth_vecs, citing_patents_depth_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "869f3db2-9261-4a0e-a18c-14253fdc2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "affiliations=loaded_dict['Affiliations.txt']\n",
    "paper_authors=loaded_dict['PaperAuthorAffiliations.txt']\n",
    "paper_fields=loaded_dict['PaperFieldsOfStudy.txt']\n",
    "fields=loaded_dict['FieldsOfStudy.txt']\n",
    "authors=loaded_dict['Authors.txt']\n",
    "\n",
    "\n",
    "# conf_save_path=os.path.join(save_loc, 'cached_affil_author_fields_dicts.lz4')\n",
    "# if not os.path.exists(conf_save_path):\n",
    "#     # Make dict of Affiliations\n",
    "#     affiliations=loaded_dict['Affiliations.txt']\n",
    "#     affiliations_dict={}\n",
    "#     for ind in tqdm(range(len(affiliations))):\n",
    "#         affiliations_dict[affiliations['AffiliationId'][ind]]=(affiliations['NormalizedName'][ind], affiliations['Latitude'][ind], affiliations['Longitude'][ind])\n",
    "\n",
    "#     # # Make dict of Paper Author Affiliations\n",
    "#     # paper_authors=loaded_dict['PaperAuthorAffiliations.txt']\n",
    "#     # paper_author_dict={}\n",
    "#     # for ind in tqdm(range(len(paper_authors))):\n",
    "#     #     paper_id=paper_authors['PaperId'][ind]\n",
    "#     #     author_id=paper_authors['AuthorId'][ind]\n",
    "#     #     affil_id=paper_authors['AffiliationId'][ind]\n",
    "#     #     if paper_id not in paper_author_dict:\n",
    "#     #         paper_author_dict[paper_id]=[]\n",
    "#     #     paper_author_dict[paper_id].append((author_id, affil_id))\n",
    "\n",
    "#     # Make dict of Paper Fields of Study\n",
    "#     paper_fields=loaded_dict['PaperFieldsOfStudy.txt']\n",
    "#     paper_feilds_dict=paper_fields.set_index('PaperId').to_dict()\n",
    "#     # paper_feilds_dict={}\n",
    "#     # for ind in tqdm(range(len(paper_fields))):\n",
    "#     #     paper_id=paper_fields['PaperId'][ind]\n",
    "#     #     field_id=paper_fields['FieldOfStudyId'][ind]\n",
    "#     #     score=paper_fields['Score'][ind]\n",
    "#     #     if paper_id not in paper_feilds_dict:\n",
    "#     #         paper_feilds_dict[paper_id]=[]\n",
    "#     #     paper_feilds_dict[paper_id].append((field_id, score))\n",
    "\n",
    "#     # Make dict of Fields of Study\n",
    "#     fields=loaded_dict['FieldsOfStudy.txt']\n",
    "#     fields_dict={}\n",
    "#     for ind in tqdm(range(len(fields))):\n",
    "#         fields_dict[fields['FieldOfStudyId'][ind]]=(fields['NormalizedName'][ind], fields['Level'][ind])\n",
    "        \n",
    "#     pickle.dump((affiliations_dict, paper_author_dict, paper_feilds_dict, fields_dict), open(conf_save_path, 'wb'))\n",
    "# else:\n",
    "#     affiliations_dict, paper_author_dict, paper_feilds_dict, fields_dict=pickle.load(conf_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89984552-70c8-41bd-9bb2-1ca09bf43cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords=[[\"targeting\", \"advertisement\", \"ad\", \"purchase\", \"client\", \"consumer\"],\n",
    "                  [\"national\", \"resident\", \"taxpayer\", \"citizen\"],\n",
    "                  [\"travel\", \"baggage\", \"reidentification\", \"license plate\", \"border\", \"airport\"],\n",
    "                  [\"child\", \"children\", \"kid\", \"minor\", \"youth\", \"age\", \"underage\"],\n",
    "                  [\"criminal\", \"crime\", \"felon\", \"convict\", \"prisoner\", \"jail\", \"penitentiary\", \"prison\", \"fraud\", \"security\", \"suspect\"],\n",
    "                  [\"army\", \"defense\", \"combat\", \"enemy\", \"security\", \"drone\", \"aircraft\", \"military\"],\n",
    "                  [\"race\", \"ethnicity\", \"class\", \"gender\", \"female\", \"male\", \"woman\", \"man\", \"sexuality\", \"age\", \"sex\", \"caste\", \"disability\", \"nonbinary\", \"non-binary\", \"trans\", \"transgender\"],\n",
    "                  [\"medical\", \"health\", \"surgery\", \"organ\", \"bone\", \"medicine\", \"doctor\", \"hospital\", \"therapy\", \"therapies\", \"drug\", \"treatment\", \"cancer\", \"mri\", \"xray\", \"x-ray\", \"tumor\", \"disease\", \"radiologist\"],\n",
    "                  [\"iris\", \"irises\", \"face\", \"facial\", \"facial recognition\", \"gesture\", \"torso\", \"anatomy\", \"anatomies\", \"joint\", \"limb\", \"hand\", \"finger\"],\n",
    "                  [\"person\", \"people\", \"track\", \"human\", \"pedestrian\", \"foot traffic\"],\n",
    "                  [\"room\", \"scene\", \"office\", \"store\", \"street\", \"crowd\", \"home\", \"house\", \"apartment\"],\n",
    "                  [\"location\", \"geolocation\", \"GPS\", \"friend\", \"social network\", \"family\", \"preference\"],\n",
    "                  [\"image\", \"text\", \"object\"],\n",
    "                  [\"local\"],\n",
    "                  [\"wireless\", \"wifi\", \"server\", \"cloud\", \"network\"],\n",
    "                  [\"personalize\", \"preference\", \"recommend\"],\n",
    "                  [\"prison\", \"travel\", \"border\", \"criminal\", \"crime\", \"felon\", \"convict\", \"prisoner\", \"jail\", \"penitentiary\", \"prison\", \"fraud\", \"security\"],\n",
    "                  [\"surveil\", \"surveillance\"]]\n",
    "\n",
    "\n",
    "flat_keywords=[]\n",
    "for keyword in keywords:\n",
    "    flat_keywords+=keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b486f53a-6eeb-423e-9185-fd960d2e270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 15577957/15577957 [12:55<00:00, 20078.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num papers searched from 19793\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install line_profiler\n",
    "\n",
    "\n",
    "    \n",
    "# flat_keywords=[\"surveil\"]\n",
    "keywords, mag_id_keyword_depth_vecs, stored_patents_keywords_dict=analyze_graph(flat_keywords)\n",
    "\n",
    "# conf_save_path=os.path.join(save_loc, 'temp_keywords_cvpr.lz4')\n",
    "# pickle.dump((mag_id_keyword_depth_vecs, stored_patents_keywords_dict), open(conf_save_path, 'wb'))\n",
    "# print('saved')\n",
    "# %load_ext line_profiler\n",
    "# %lprun -f analyze_graph -f find_keyword -f get_conference_keywords analyze_graph(flat_keywords)\n",
    "\n",
    "# import matplotlib\n",
    "\n",
    "# paper_chain_df = pd.DataFrame(list(paper_in_linkage_count.values()))\n",
    "# paper_chain_hist = paper_chain_df.hist(bins=100, log='True')\n",
    "\n",
    "# patent_chain_df = pd.DataFrame(list(patent_in_linkage_count.values()))\n",
    "# patent_chain_df = patent_chain_df.hist(bins=100, log='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f01e736-05da-45c6-b3f7-c801e7226eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gscratch/prl/wagnew3/microsoft_academic_graph/temp_keywords_cvpr.lz4\n"
     ]
    }
   ],
   "source": [
    "conf_save_path=os.path.join(save_loc, 'temp_keywords_cvpr.lz4')\n",
    "print(conf_save_path)\n",
    "mag_id_keyword_depth_vecs, stored_patents_keywords_dict=pickle.load(open(conf_save_path, 'rb'))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6a92e47-b848-4c39-9b81-b3e12ef2dd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19793\n"
     ]
    }
   ],
   "source": [
    "num_cvpr=0\n",
    "for mag_id in mag_id_keyword_depth_vecs:\n",
    "    conf_id=mag_to_conf_id[mag_id]\n",
    "    if conf_id in conference_series_ids:\n",
    "        num_cvpr+=1\n",
    "print(num_cvpr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea4c227f-3243-44b2-bf9e-648178b10cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 2070963/2070963 [18:51<00:00, 1829.56it/s]\n"
     ]
    }
   ],
   "source": [
    "single_keywords_dict={\"paper_id\":[]}\n",
    "for keyword in flat_keywords:\n",
    "    single_keywords_dict[keyword]=[]\n",
    "    single_keywords_dict[keyword+\"_0_dist_patents\"]=[]\n",
    "\n",
    "flat_keywords=list(set(flat_keywords))\n",
    "for mag_id in tqdm(mag_id_keyword_depth_vecs):\n",
    "    single_keywords_dict[\"paper_id\"].append(mag_id)\n",
    "    for keyword_ind in range(len(flat_keywords)):\n",
    "        keyword=flat_keywords[keyword_ind]\n",
    "        single_keywords_dict[keyword].append(mag_id_keyword_depth_vecs[mag_id][keyword_ind])\n",
    "        if mag_id in stored_patents_keywords_dict:\n",
    "            single_keywords_dict[keyword+\"_0_dist_patents\"].append(stored_patents_keywords_dict[mag_id][keyword])\n",
    "        else:\n",
    "            single_keywords_dict[keyword+\"_0_dist_patents\"].append([])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93243d1b-a0bd-4d46-909e-4114835e0a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_save_path=os.path.join(save_loc, 'temp_keywords_cvpr.lz4')\n",
    "pickle.dump((mag_id_keyword_depth_vecs, stored_patents_keywords_dict), open(conf_save_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec9e3e71-0293-40db-a26d-c2fbb6c975cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_save_path=os.path.join(save_loc, 'cvpr_df_single_keywords_dict.lz4')\n",
    "pickle.dump(single_keywords_dict, open(conf_save_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8664f6-9189-44e3-88a3-e1614f573904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_single_keywords_dict={}\n",
    "test_single_keywords_dict['surveillance_0_dist_patents']=single_keywords_dict['surveillance_0_dist_patents']\n",
    "df_single_keywords_dict=pd.DataFrame.from_dict(single_keywords_dict)\n",
    "print('made df')\n",
    "conf_save_path=os.path.join(save_loc, 'cvpr_df_single_keywords_dict.lz4')\n",
    "pickle.dump(df_single_keywords_dict, open(conf_save_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7632e1-99bb-4146-a081-0c230ecd7bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing import Pool\n",
    "\n",
    "flat_keywords=[]\n",
    "for keyword in keywords:\n",
    "    flat_keywords+=keyword\n",
    "\n",
    "scrape_args=[]\n",
    "for keyword in tqdm(flat_keywords):\n",
    "    #patents_scraped_info.update(utils.scrape_patents([patent], patents_save_loc))\n",
    "    scrape_args.append(([keyword],))\n",
    "\n",
    "res_dict={}\n",
    "with Pool(processes=30) as pool, tqdm(total=len(scrape_args)) as pbar: # create Pool of processes (only 2 in this example) and tqdm Progress bar\n",
    "    for data in tqdm(pool.imap_unordered(analyze_graph, scrape_args)):                   # send urls from all_urls list to parse() function (it will be done concurently in process pool). The results returned will be unordered (returned when they are available, without waiting for other processes)\n",
    "        pbar.update()\n",
    "        res_dict[data[0][0]]=data\n",
    "\n",
    "conf_save_path=os.path.join(save_loc, 'temp_keywords_cvpr.lz4')\n",
    "pickle.dump(res_dict, open(conf_save_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6cc78-40f0-4152-a573-70c17727fb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify analyze subgraph\n",
    "\n",
    "for mag_id in tqdm(subgraph):\n",
    "    conf_id=mag_to_conf_id[mag_id]\n",
    "    journal_id=mag_to_journal_id[mag_id]\n",
    "    # If paper is in target venues\n",
    "    if conf_id in conference_series_ids or journal_id in conference_series_ids:\n",
    "        paper_keyword_depth_vec=mag_id_keyword_depth_vecs[mag_id][0]\n",
    "        sub_keyword_sum=np.zeros(4)\n",
    "        for sub_mag_id in subgraph[mag_id]:\n",
    "            sub_keyword_depth_vec=mag_id_keyword_depth_vecs[sub_mag_id]\n",
    "            sub_keyword_sum[:len(sub_keyword_depth_vec[0])]+=sub_keyword_depth_vec[0]\n",
    "        if np.sum(np.abs(paper_keyword_depth_vec[1:]-sub_keyword_sum[:-1]))!=0\n",
    "            print('error, do not match!', paper_keyword_depth_vec, sub_keyword_sum)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fe96773-37b1-4ede-a8ca-eb90515d3b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['paper_id', 'keyword_depth_vec'], dtype='object')\n",
      "len(mag_id_keyword_depth_vecs_df) 2070963\n"
     ]
    }
   ],
   "source": [
    "mag_id_keyword_depth_vecs_df_dict={\"paper_id\":[], \"keyword_depth_vec\": []}\n",
    "for mag_id in mag_id_keyword_depth_vecs:\n",
    "    mag_id_keyword_depth_vecs_df_dict[\"paper_id\"].append(mag_id)\n",
    "    mag_id_keyword_depth_vecs_df_dict[\"keyword_depth_vec\"].append(mag_id_keyword_depth_vecs[mag_id])\n",
    "    \n",
    "mag_id_keyword_depth_vecs_df=pd.DataFrame.from_dict(mag_id_keyword_depth_vecs_df_dict)\n",
    "print(mag_id_keyword_depth_vecs_df.columns)\n",
    "print('len(mag_id_keyword_depth_vecs_df)', len(mag_id_keyword_depth_vecs_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90acfe92-677a-4712-afd2-b131e12772ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implode(df, column):\n",
    "      \n",
    "    agg_keys={}\n",
    "    for key in df.columns:\n",
    "        if key!=column:\n",
    "            agg_keys[key]=list\n",
    "            \n",
    "    return df.groupby(column, as_index=False).agg(agg_keys)[df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc8deaff-5dee-468a-b227-03767a972a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_df(df):\n",
    "    print('--------------------------------------------------------------------------------------')\n",
    "    print(df.iloc[0])\n",
    "    print('--------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89b19aa7-781c-4560-888e-23330c0bc538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2070963\n"
     ]
    }
   ],
   "source": [
    "print(len(mag_id_keyword_depth_vecs_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27820b45-33b0-4a8b-99a9-4d38fc8eeefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['paper_id', 'keyword_depth_vec'], dtype='object')\n",
      "Index(['PaperId', 'AuthorId', 'AffiliationId'], dtype='object')\n",
      "2070963\n",
      "Merge paper author ids\n",
      "Merge paper authors affils\n",
      "--------------------------------------------------------------------------------------\n",
      "paper_id                                     193634663\n",
      "keyword_depth_vec           [[[0.0, 0.0], [0.0, 0.0]]]\n",
      "keyword_depth_vec_affils        [[[[0. 0.], [0. 0.]]]]\n",
      "PaperId                                    [193634663]\n",
      "AuthorId                                   [249105280]\n",
      "AffiliationId                                    [nan]\n",
      "Rank                                             [nan]\n",
      "NormalizedName                                   [nan]\n",
      "DisplayName                                      [nan]\n",
      "GridId                                           [nan]\n",
      "RorId                                            [nan]\n",
      "OfficialPage                                     [nan]\n",
      "WikiPage                                         [nan]\n",
      "PaperCount                                       [nan]\n",
      "PaperFamilyCount                                 [nan]\n",
      "CitationCount                                    [nan]\n",
      "Iso3166Code                                      [nan]\n",
      "Latitude                                         [nan]\n",
      "Longitude                                        [nan]\n",
      "CreatedDate                                      [nan]\n",
      "UpdatedDate                                      [nan]\n",
      "Name: 0, dtype: object\n",
      "--------------------------------------------------------------------------------------\n",
      "Index(['paper_id', 'keyword_depth_vec'], dtype='object')\n",
      "Index(['PaperId', 'AuthorId', 'AffiliationId'], dtype='object')\n",
      "2070963\n",
      "Merge paper author ids\n",
      "Merge paper authors names\n",
      "--------------------------------------------------------------------------------------\n",
      "paper_id                                     193634663\n",
      "keyword_depth_vec           [[[0.0, 0.0], [0.0, 0.0]]]\n",
      "keyword_depth_vec_affils        [[[[0. 0.], [0. 0.]]]]\n",
      "PaperId                                    [193634663]\n",
      "AuthorId                                   [249105280]\n",
      "AffiliationId                                    [nan]\n",
      "Rank                                             [nan]\n",
      "NormalizedName                                   [nan]\n",
      "DisplayName                                      [nan]\n",
      "GridId                                           [nan]\n",
      "RorId                                            [nan]\n",
      "OfficialPage                                     [nan]\n",
      "WikiPage                                         [nan]\n",
      "PaperCount                                       [nan]\n",
      "PaperFamilyCount                                 [nan]\n",
      "CitationCount                                    [nan]\n",
      "Iso3166Code                                      [nan]\n",
      "Latitude                                         [nan]\n",
      "Longitude                                        [nan]\n",
      "CreatedDate                                      [nan]\n",
      "UpdatedDate                                      [nan]\n",
      "keyword_depth_vec_author     [[[[[0. 0.]\\n [0. 0.]]]]]\n",
      "PaperId_author                           [[193634663]]\n",
      "AuthorId_author                            [249105280]\n",
      "AffiliationId_author                           [[nan]]\n",
      "Rank_author                                    [21226]\n",
      "NormalizedName_author        [kristoffer johan sundoy]\n",
      "DisplayName_author           [Kristoffer Johan Sundøy]\n",
      "Orcid                                            [nan]\n",
      "LastKnownAffiliationId                           [nan]\n",
      "PaperCount_author                                  [1]\n",
      "PaperFamilyCount_author                            [1]\n",
      "CitationCount_author                               [0]\n",
      "CreatedDate_author                        [2016-06-24]\n",
      "UpdatedDate_author               [2016-06-24 00:00:00]\n",
      "Name: 0, dtype: object\n",
      "--------------------------------------------------------------------------------------\n",
      "Index(['paper_id', 'keyword_depth_vec', 'PaperId', 'FieldOfStudyId', 'Score',\n",
      "       'AlgorithmVersion'],\n",
      "      dtype='object')\n",
      "38591567\n",
      "Merge paper field id\n",
      "Merge paper field info\n",
      "--------------------------------------------------------------------------------------\n",
      "paper_id                                                            193634663\n",
      "keyword_depth_vec                                  [[[0.0, 0.0], [0.0, 0.0]]]\n",
      "keyword_depth_vec_affils                               [[[[0. 0.], [0. 0.]]]]\n",
      "PaperId                                                           [193634663]\n",
      "AuthorId                                                          [249105280]\n",
      "AffiliationId                                                           [nan]\n",
      "Rank                                                                    [nan]\n",
      "NormalizedName                                                          [nan]\n",
      "DisplayName                                                             [nan]\n",
      "GridId                                                                  [nan]\n",
      "RorId                                                                   [nan]\n",
      "OfficialPage                                                            [nan]\n",
      "WikiPage                                                                [nan]\n",
      "PaperCount                                                              [nan]\n",
      "PaperFamilyCount                                                        [nan]\n",
      "CitationCount                                                           [nan]\n",
      "Iso3166Code                                                             [nan]\n",
      "Latitude                                                                [nan]\n",
      "Longitude                                                               [nan]\n",
      "CreatedDate                                                             [nan]\n",
      "UpdatedDate                                                             [nan]\n",
      "keyword_depth_vec_author                            [[[[[0. 0.]\\n [0. 0.]]]]]\n",
      "PaperId_author                                                  [[193634663]]\n",
      "AuthorId_author                                                   [249105280]\n",
      "AffiliationId_author                                                  [[nan]]\n",
      "Rank_author                                                           [21226]\n",
      "NormalizedName_author                               [kristoffer johan sundoy]\n",
      "DisplayName_author                                  [Kristoffer Johan Sundøy]\n",
      "Orcid                                                                   [nan]\n",
      "LastKnownAffiliationId                                                  [nan]\n",
      "PaperCount_author                                                         [1]\n",
      "PaperFamilyCount_author                                                   [1]\n",
      "CitationCount_author                                                      [0]\n",
      "CreatedDate_author                                               [2016-06-24]\n",
      "UpdatedDate_author                                      [2016-06-24 00:00:00]\n",
      "keyword_depth_vec_fields    [[[[0. 0.], [0. 0.]]], [[[0. 0.], [0. 0.]]], [...\n",
      "PaperId_fields              [193634663, 193634663, 193634663, 193634663, 1...\n",
      "FieldOfStudyId              [489000, 23123220, 31972630, 41008148, 4100814...\n",
      "Score                       [0.48840386, 0.47521374, 0.4069658, 0.46045673...\n",
      "AlgorithmVersion            [1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, ...\n",
      "Rank_fields                 [10776, 7741, 6446, 4252, 4252, 7693, 9684, 86...\n",
      "NormalizedName_fields       [data flow diagram, information retrieval, com...\n",
      "DisplayName_fields          [Data flow diagram, Information retrieval, Com...\n",
      "MainType                    [nan, nan, nan, nan, nan, computer.software_ge...\n",
      "Level                       [2, 1, 1, 0, 0, 1, 2, 2, 2, 2, 3, 2, 2, 2, 1, ...\n",
      "PaperCount_fields           [1193, 498741, 1325779, 27478212, 27478212, 79...\n",
      "PaperFamilyCount_fields     [1193, 498741, 1325779, 27478212, 27478212, 79...\n",
      "CitationCount_fields        [16509, 5132888, 15134772, 231013792, 23101379...\n",
      "CreatedDate_fields          [2016-06-24, 2016-06-24, 2016-06-24, 2016-06-2...\n",
      "Name: 0, dtype: object\n",
      "--------------------------------------------------------------------------------------\n",
      "Index(['paper_id', 'keyword_depth_vec', 'PaperId', 'PaperTitle', 'Year',\n",
      "       'JournalId', 'ConferenceSeriesId', 'ConferenceInstanceId'],\n",
      "      dtype='object')\n",
      "2070967\n",
      "Merge paper id\n",
      "--------------------------------------------------------------------------------------\n",
      "paper_id                                                            193634663\n",
      "keyword_depth_vec                                  [[[0.0, 0.0], [0.0, 0.0]]]\n",
      "keyword_depth_vec_affils                               [[[[0. 0.], [0. 0.]]]]\n",
      "PaperId                                                           [193634663]\n",
      "AuthorId                                                          [249105280]\n",
      "AffiliationId                                                           [nan]\n",
      "Rank                                                                    [nan]\n",
      "NormalizedName                                                          [nan]\n",
      "DisplayName                                                             [nan]\n",
      "GridId                                                                  [nan]\n",
      "RorId                                                                   [nan]\n",
      "OfficialPage                                                            [nan]\n",
      "WikiPage                                                                [nan]\n",
      "PaperCount                                                              [nan]\n",
      "PaperFamilyCount                                                        [nan]\n",
      "CitationCount                                                           [nan]\n",
      "Iso3166Code                                                             [nan]\n",
      "Latitude                                                                [nan]\n",
      "Longitude                                                               [nan]\n",
      "CreatedDate                                                             [nan]\n",
      "UpdatedDate                                                             [nan]\n",
      "keyword_depth_vec_author                            [[[[[0. 0.]\\n [0. 0.]]]]]\n",
      "PaperId_author                                                  [[193634663]]\n",
      "AuthorId_author                                                   [249105280]\n",
      "AffiliationId_author                                                  [[nan]]\n",
      "Rank_author                                                           [21226]\n",
      "NormalizedName_author                               [kristoffer johan sundoy]\n",
      "DisplayName_author                                  [Kristoffer Johan Sundøy]\n",
      "Orcid                                                                   [nan]\n",
      "LastKnownAffiliationId                                                  [nan]\n",
      "PaperCount_author                                                         [1]\n",
      "PaperFamilyCount_author                                                   [1]\n",
      "CitationCount_author                                                      [0]\n",
      "CreatedDate_author                                               [2016-06-24]\n",
      "UpdatedDate_author                                      [2016-06-24 00:00:00]\n",
      "keyword_depth_vec_fields    [[[[0. 0.], [0. 0.]]], [[[0. 0.], [0. 0.]]], [...\n",
      "PaperId_fields              [193634663, 193634663, 193634663, 193634663, 1...\n",
      "FieldOfStudyId              [489000, 23123220, 31972630, 41008148, 4100814...\n",
      "Score                       [0.48840386, 0.47521374, 0.4069658, 0.46045673...\n",
      "AlgorithmVersion            [1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, ...\n",
      "Rank_fields                 [10776, 7741, 6446, 4252, 4252, 7693, 9684, 86...\n",
      "NormalizedName_fields       [data flow diagram, information retrieval, com...\n",
      "DisplayName_fields          [Data flow diagram, Information retrieval, Com...\n",
      "MainType                    [nan, nan, nan, nan, nan, computer.software_ge...\n",
      "Level                       [2, 1, 1, 0, 0, 1, 2, 2, 2, 2, 3, 2, 2, 2, 1, ...\n",
      "PaperCount_fields           [1193, 498741, 1325779, 27478212, 27478212, 79...\n",
      "PaperFamilyCount_fields     [1193, 498741, 1325779, 27478212, 27478212, 79...\n",
      "CitationCount_fields        [16509, 5132888, 15134772, 231013792, 23101379...\n",
      "CreatedDate_fields          [2016-06-24, 2016-06-24, 2016-06-24, 2016-06-2...\n",
      "keyword_depth_vec_fields                           [[[0.0, 0.0], [0.0, 0.0]]]\n",
      "PaperId_fields                                                      193634663\n",
      "PaperTitle                                  audiovisual contents segmentation\n",
      "Year                                                                     2010\n",
      "JournalId                                                                 NaN\n",
      "ConferenceSeriesId                                                        NaN\n",
      "ConferenceInstanceId                                                      NaN\n",
      "Name: 0, dtype: object\n",
      "--------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Author Affils\n",
    "\n",
    "# Merge paper author ids\n",
    "paper_authors=loaded_dict['PaperAuthorAffiliations.txt']\n",
    "print(mag_id_keyword_depth_vecs_df.columns)\n",
    "print(paper_authors.columns)\n",
    "print(len(mag_id_keyword_depth_vecs_df['paper_id']))\n",
    "combined_df=mag_id_keyword_depth_vecs_df.merge(paper_authors, left_on='paper_id', right_on='PaperId', how='left', suffixes=(None, '_paper_authors'))\n",
    "print(\"Merge paper author ids\")\n",
    "\n",
    "# Merge paper authors affils\n",
    "combined_df=combined_df.explode(['AffiliationId'])\n",
    "combined_df=combined_df.merge(affiliations, left_on='AffiliationId', right_on='AffiliationId', how='left', suffixes=(None, '_affiliations'))\n",
    "combined_df=implode(combined_df, 'paper_id')\n",
    "print(\"Merge paper authors affils\")\n",
    "\n",
    "full_df=mag_id_keyword_depth_vecs_df.merge(combined_df, left_on='paper_id', right_on='paper_id', how='left', suffixes=(None, '_affils'))\n",
    "validate_df(full_df)\n",
    "## Author Names\n",
    "\n",
    "# Merge paper author ids\n",
    "paper_authors=loaded_dict['PaperAuthorAffiliations.txt']\n",
    "print(mag_id_keyword_depth_vecs_df.columns)\n",
    "print(paper_authors.columns)\n",
    "print(len(mag_id_keyword_depth_vecs_df['paper_id']))\n",
    "combined_df=mag_id_keyword_depth_vecs_df.merge(paper_authors, left_on='paper_id', right_on='PaperId', how='left', suffixes=(None, '_paper_authors'))\n",
    "combined_df=implode(combined_df, 'paper_id')\n",
    "print(\"Merge paper author ids\")\n",
    "\n",
    "# Merge paper authors affils\n",
    "combined_df=combined_df.explode(['AuthorId'])\n",
    "combined_df=combined_df.merge(authors, left_on='AuthorId', right_on='AuthorId', how='left', suffixes=(None, '_author'))\n",
    "combined_df=implode(combined_df, 'paper_id')\n",
    "print(\"Merge paper authors names\")\n",
    "\n",
    "full_df=full_df.merge(combined_df, left_on='paper_id', right_on='paper_id', how='left', suffixes=(None, '_author'))\n",
    "validate_df(full_df)\n",
    "\n",
    "# Merge paper field id\n",
    "combined_df=mag_id_keyword_depth_vecs_df.merge(paper_fields, left_on='paper_id', right_on='PaperId', how='left', suffixes=(None, '_paper_fields'))\n",
    "print(combined_df.columns)\n",
    "print(len(combined_df))\n",
    "print(\"Merge paper field id\")\n",
    "\n",
    "# Merge paper field info\n",
    "combined_df=combined_df.explode(['FieldOfStudyId'])\n",
    "combined_df=combined_df.merge(fields, left_on='FieldOfStudyId', right_on='FieldOfStudyId', how='left', suffixes=(None, '_fields'))\n",
    "combined_df=implode(combined_df, 'paper_id')\n",
    "print(\"Merge paper field info\")\n",
    "\n",
    "full_df=full_df.merge(combined_df, left_on='paper_id', right_on='paper_id', how='left', suffixes=(None, '_fields'))\n",
    "validate_df(full_df)\n",
    "\n",
    "papers=loaded_dict['Papers.txt']\n",
    "\n",
    "# Merge paper field id\n",
    "combined_df=mag_id_keyword_depth_vecs_df.merge(papers, left_on='paper_id', right_on='PaperId', how='left', suffixes=(None, '_paper'))\n",
    "print(combined_df.columns)\n",
    "print(len(combined_df))\n",
    "print(\"Merge paper id\")\n",
    "\n",
    "full_df=full_df.merge(combined_df, left_on='paper_id', right_on='paper_id', how='left', suffixes=(None, '_fields'))\n",
    "validate_df(full_df)\n",
    "\n",
    "# Convert and save dataset\n",
    "conf_save_path=os.path.join(save_loc, 'cvpr_procssed_dataset.lz4')\n",
    "pickle.dump(full_df, open(conf_save_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6345f6-f902-4ff9-8e49-27805cea65d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit tests/sanity checks\n",
    "import random\n",
    "\n",
    "# How many papers in conference and dataset?\n",
    "num_papers_dataset=len(loaded_dict[\"Papers.txt\"])\n",
    "print(f\"There are {num_papers_dataset} papers in MAG dataset\")\n",
    "print(f\"There are {len(all_mags)} papers in conference citation graph\")\n",
    "\n",
    "# How many patents in dataset?\n",
    "num_patents_dataset=len(patents_list)\n",
    "print(f\"There are {num_patents_dataset} patent linkages in conference citation graph and {len(patents_info_dict)} were scrapped\")\n",
    "\n",
    "# change this number to have different things printed for manual verification\n",
    "random_num=0\n",
    "# Are all paper-paper linkages present?\n",
    "def print_random_paper_paper_linkages(subgraph, mag_to_title, num_print=5):\n",
    "    '''\n",
    "    Print random paper from conference subgraph and all papers citing it to enable manual verification.\n",
    "    '''\n",
    "    for ind in range(num_print):\n",
    "        paper_mag=random.choice(list(subgraph.keys()))\n",
    "        paper_title=mag_to_title[paper_mag]\n",
    "        citing_list=[]\n",
    "        for citing_mag in subgraph[paper_mag]:\n",
    "            citing_title=mag_to_title[citing_mag]\n",
    "            citing_list.append(citing_title)\n",
    "        print(f\"Paper Title: {paper_title} Number Citing Papers: {len(citing_list)} Citing Papers: {citing_list}\\n\")\n",
    "#print_random_paper_paper_linkages(subgraph, mag_to_title, num_print=5)\n",
    "\n",
    "# Are all paper-patent linkages present?\n",
    "def print_random_paper_paper_linkages(subgraph, mag_to_title, paper_to_patent, num_print=5):\n",
    "    '''\n",
    "    Print random paper from conference subgraph and all patents citing it to enable manual verification.\n",
    "    '''\n",
    "    for ind in range(num_print):\n",
    "        paper_mag=random.choice(list(subgraph.keys()))\n",
    "        paper_title=mag_to_title[paper_mag]\n",
    "        if paper_mag in paper_to_patent:\n",
    "            print(f\"Paper Title: {paper_title} MAG: {paper_mag} Number Citing Patents: {len(paper_to_patent[paper_mag])} Citing Patents: {paper_to_patent[paper_mag]}\\n\")\n",
    "#print_random_paper_paper_linkages(subgraph, mag_to_title, paper_to_patent, num_print=50)\n",
    "\n",
    "# Is the tree search correct?\n",
    "surveil_keywords=[\"surveil\"]\n",
    "max_depth=2\n",
    "for mag_id in tqdm(subgraph):\n",
    "    conf_id=mag_to_conf_id[mag_id]\n",
    "    journal_id=mag_to_journal_id[mag_id]\n",
    "    # If paper is in target venues\n",
    "    patent_keywords_dict={}\n",
    "    if conf_id in conference_series_ids or journal_id in conference_series_ids:\n",
    "        visted_patents=[]\n",
    "        keyword_vec=find_keyword(subgraph, mag_id, patent_keywords_dict, patents_info_dict, 0, surveil_keywords, visted_patents=visted_patents, max_depth=max_depth)\n",
    "        print(f\"Title {mag_to_title[mag_id]} keyword vec: {keyword_vec} visted_patents: {visted_patents}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b97894-4270-4e30-8a3d-70b6dcc7ae68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
