{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb33d79c-29eb-492d-8d61-3b48227e0c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mag', 'conf_if'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load dataframe of CVPR+citing papers, authors, instiutions, and relations to patents with 'surveil'\n",
    "\n",
    "import compress_pickle as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "paper_info_df=pickle.load('/gscratch/prl/wagnew3/microsoft_academic_graph/cvpr_procssed_dataset.lz4')\n",
    "\n",
    "save_loc=\"/gscratch/prl/wagnew3/microsoft_academic_graph/\"\n",
    "conf_save_path=os.path.join(save_loc, 'mag_conf_journ_title.lz4')\n",
    "mag_to_conf_id, mag_to_journal_id, mag_to_title=pickle.load(conf_save_path)\n",
    "\n",
    "df_mag_to_conf_id=pd.DataFrame.from_dict(mag_to_conf_id, orient='index')\n",
    "df_mag_to_conf_id.index.name = 'mag'\n",
    "df_mag_to_conf_id.reset_index(inplace=True)\n",
    "df_mag_to_conf_id.columns=['mag', 'conf_if']\n",
    "print(df_mag_to_conf_id.columns)\n",
    "paper_info_df=paper_info_df.merge(df_mag_to_conf_id, left_on='paper_id', right_on='mag', how='left')\n",
    "\n",
    "keyword_depth_vecs=paper_info_df[\"keyword_depth_vec\"].to_list()\n",
    "keyword_depth_vecs=[np.reshape(keyword_depth_vecs[i], (-1,)) for i in range(len(keyword_depth_vecs))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "930bbd4f-6569-4b1a-98de-e18c2a204666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gscratch/prl/wagnew3/microsoft_academic_graph/temp_keywords_cvpr.lz4\n"
     ]
    }
   ],
   "source": [
    "conf_save_path=os.path.join(save_loc, 'temp_keywords_cvpr.lz4')\n",
    "print(conf_save_path)\n",
    "mag_id_keyword_depth_vecs, stored_patents_keywords_dict=pickle.load(open(conf_save_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "959ba002-31c6-4da2-95d3-665f244904da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 26/26 [03:28<00:00,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2549890 patents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "patents_info_dict={}\n",
    "agg_data_dir=\"/gscratch/scrubbed/wagnew3/data/\"\n",
    "for agg_file in tqdm(os.listdir(agg_data_dir)):\n",
    "    if \"patent_agg\" in agg_file:\n",
    "        patent_info_sub=pickle.load(os.path.join(agg_data_dir, agg_file))\n",
    "        patents_info_dict.update(patent_info_sub)\n",
    "print(f\"{len(patents_info_dict)} patents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bc6400d-5fa5-4cfb-986f-dcd187414650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US8965141', 'Image filtering based on structural information', '[{\"inventor_name\": \"Wen Fei JIANG\"}, {\"inventor_name\": \"Fan Zhang\"}, {\"inventor_name\": \"Zhi Bo Chen\"}]', '[{\"assignee_name\": \"Thomson Licensing SAS\"}]', '[{\"assignee_name\": \"\\\\n    Magnolia Licensing LLC\\\\n  \"}]', '2014-02-20', '2012-08-17', '2015-02-24', '', '\\n     A method and an apparatus for image filtering are described. Structural information is employed during the calculation of filtering coefficients. The structural information is described by the regions defined through an edge map of the image. In one embodiment, the region correlation between the target pixel and a contributing pixel is selected as a structural filtering coefficient. The region correlation, which indicates the possibility of two pixels being in the same regions, is calculated by evaluating the strongest edge cut by a path between the target pixel and a contributing pixel. The structural filtering coefficient is further combined with spatial information and intensity information to form a spatial-intensity-region (SIR) filter. The structural information based filter is applicable to applications such as denoising, tone mapping, and exposure fusion. \\n   \\n   ', 'https://patentimages.storage.googleapis.com/94/42/9a/cba97ade31bbcb/US8965141.pdf', [[('G', 'PHYSICS'), ('G06', 'COMPUTING; CALCULATING; COUNTING'), ('G06V', 'IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING'), ('G06V10/00', 'Arrangements for image or video recognition or understanding')], [('G06V10/20', 'Image preprocessing')], [('G06V10/30', 'Noise filtering')], [('G06K9/40', '')], [('G', 'PHYSICS'), ('G06', 'COMPUTING; CALCULATING; COUNTING'), ('G06T', 'IMAGE DATA PROCESSING OR GENERATION, IN GENERAL'), ('G06T5/00', 'Image enhancement or restoration'), ('G06T5/001', 'Image restoration')], [('G06T5/002', 'Denoising; Smoothing')], [('G', 'PHYSICS'), ('G06', 'COMPUTING; CALCULATING; COUNTING'), ('G06T', 'IMAGE DATA PROCESSING OR GENERATION, IN GENERAL'), ('G06T2207/00', 'Indexing scheme for image analysis or image enhancement')], [('G06T2207/20', 'Special algorithmic details'), ('G06T2207/20024', 'Filtering details')], [('G06T2207/20028', 'Bilateral filtering')], [('G', 'PHYSICS'), ('G06', 'COMPUTING; CALCULATING; COUNTING'), ('G06T', 'IMAGE DATA PROCESSING OR GENERATION, IN GENERAL'), ('G06T2207/00', 'Indexing scheme for image analysis or image enhancement')], [('G06T2207/20', 'Special algorithmic details'), ('G06T2207/20172', 'Image enhancement details')], [('G06T2207/20192', 'Edge enhancement; Edge preservation')]], '']\n"
     ]
    }
   ],
   "source": [
    "print(patents_info_dict['US8965141'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "638bc5f8-5b2d-40dd-94a8-a48fbabd28f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US9996917', 'US9996554', 'US9992467', 'US9984315', 'US9977980', 'US9977955', 'US9977816', 'US9971934', 'US9965728', 'US9946933', 'US9922265', 'US9913135', 'US9911053', 'US9898701', 'US9886761', 'US9886653', 'US9881214', 'US9875213', 'US9852326', 'US9846429', 'US9842274', 'US9835792', 'US9830527', 'US9824462', 'US9798959', 'US9792899', 'US9785828', 'US9779331', 'US9767348', 'US9760794', 'US9754188', 'US9740034', 'US9734392', 'US9721192', 'US9715902', 'US9715639', 'US9710716', 'US9710447', 'US9710109', 'US9709723', 'US9699419', 'US9697442', 'US9679370', 'US9679201', 'US9672634', 'US9665777', 'US9659384', 'US9659238', 'US9659225', 'US9652663', 'US9646212', 'US9639783', 'US9639777', 'US9639762', 'US9633284', 'US9633264', 'US9626551', 'US9582895', 'US9576371', 'US9576196', 'US9571723', 'US9569694', 'US9569531', 'US9569498', 'US9567078', 'US9552510', 'US9547791', 'US9542621', 'US9530080', 'US9504420', 'US9501710', 'US9489570', 'US9483838', 'US9471171', 'US9465998', 'US9449245', 'US9445763', 'US9443143', 'US9426375', 'US9424493', 'US9418283', 'US9414780', 'US9412180', 'US9412043', 'US9405965', 'US9396412', 'US9396404', 'US9380312', 'US9367766', 'US9367756', 'US9355360', 'US9355300', 'US9355167', 'US9342583', 'US9330312', 'US9323980', 'US9317768', 'US9311666', 'US9294664', 'US9292745', 'US9286532', 'US9286516', 'US9280565', 'US9280545', 'US9275472', 'US9275308', 'US9275305', 'US9269025', 'US9268993', 'US9268795', 'US9256945', 'US9251402', 'US9251400', 'US9245206', 'US9245186', 'US9239943', 'US9239848', 'US9235859', 'US9218365', 'US9213991', 'US9208554', 'US9202140', 'US9202126', 'US9195819', 'US9189886', 'US9183459', 'US9171210', 'US9165369', 'US9165190', 'US9158971', 'US9147105', 'US9143601', 'US9134399', 'US9131149', 'US9116921', 'US9082235', 'US9076036', 'US9064191', 'US9053367', 'US9049419', 'US9047540', 'US9043338', 'US9036917', 'US9036877', 'US9031317', 'US9020210', 'US9008365', 'US8515127', 'US8457406', 'US8452096', 'US8375036', 'US8165397', 'US10874332', 'US10872262', 'US10872242', 'US10867195', 'US10861128', 'US10839573', 'US10810433', 'US10802356', 'US10789454', 'US10781984', 'US10755105', 'US10750053', 'US10748033', 'US10740653', 'US10735959', 'US10708550', 'US10706499', 'US10685262', 'US10671850', 'US10634840', 'US10599927', 'US10592687', 'US10567671', 'US10559312', 'US10559080', 'US10546417', 'US10514694', 'US10482336', 'US10465869', 'US10459152', 'US10455144', 'US10451229', 'US10438302', 'US10430638', 'US10425635', 'US10424342', 'US10417773', 'US10408992', 'US10403076', 'US10401638', 'US10365426', 'US10359560', 'US10354406', 'US10339706', 'US10325351', 'US10325183', 'US10306135', 'US10275640', 'US10262517', 'US10262187', 'US10255517', 'US10248884', 'US10242282', 'US10223580', 'US10219736', 'US10216979', 'US10196867', 'US10185899', 'US10176384', 'US10176362', 'US10169683', 'US10169647', 'US10165230', 'US10163027', 'US10157327', 'US10157217', 'US10140726', 'US10127437', 'US10127429', 'US10127310', 'US10121054', 'US10120879', 'US10104345', 'US10095942', 'US10089762', 'US10068124', 'US10063843', 'US10055673', 'US10054732', 'US10037457', 'US10032093', 'US10022544', 'US10009549', 'US10002460', 'US10002313', 'US10002309', 'CN102598113A']\n"
     ]
    }
   ],
   "source": [
    "print(stored_patents_keywords_dict['2161969291']['person'])\n",
    "\n",
    "# for k in stored_patents_keywords_dict:\n",
    "#     print(stored_patents_keywords_dict[k])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d53f78aa-8fe0-4557-890e-1df13a50d967",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "8 columns passed, passed data had 6 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/gscratch/prl/wagnew3/miniconda3/envs/whatisai/lib/python3.8/site-packages/pandas/core/internals/construction.py:982\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 982\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m/gscratch/prl/wagnew3/miniconda3/envs/whatisai/lib/python3.8/site-packages/pandas/core/internals/construction.py:1030\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m   1031\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1032\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m     )\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_mi_list:\n\u001b[1;32m   1035\u001b[0m \n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 8 columns passed, passed data had 6 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m patent_dists\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyword_depth_vecs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0_dist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1_dist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2_dist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m3_dist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0_other\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1_other\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2_other\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m3_other\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([paper_info_df, patent_dists], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeyword_depth_vec_affils\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeyword_depth_vec_author\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPaperId_author\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPaperId_fields\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeyword_depth_vec_fields\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/gscratch/prl/wagnew3/miniconda3/envs/whatisai/lib/python3.8/site-packages/pandas/core/frame.py:721\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m         \u001b[38;5;66;03m# error: Argument 1 to \"ensure_index\" has incompatible type\u001b[39;00m\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;66;03m# \"Collection[Any]\"; expected \"Union[Union[Union[ExtensionArray,\u001b[39;00m\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;66;03m# ndarray], Index, Series], Sequence[Any]]\"\u001b[39;00m\n\u001b[1;32m    720\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    729\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    730\u001b[0m         arrays,\n\u001b[1;32m    731\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    734\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    735\u001b[0m     )\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/gscratch/prl/wagnew3/miniconda3/envs/whatisai/lib/python3.8/site-packages/pandas/core/internals/construction.py:519\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    517\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 519\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/gscratch/prl/wagnew3/miniconda3/envs/whatisai/lib/python3.8/site-packages/pandas/core/internals/construction.py:883\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    880\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    881\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 883\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m/gscratch/prl/wagnew3/miniconda3/envs/whatisai/lib/python3.8/site-packages/pandas/core/internals/construction.py:985\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    982\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 985\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    988\u001b[0m     contents \u001b[38;5;241m=\u001b[39m _convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 8 columns passed, passed data had 6 columns"
     ]
    }
   ],
   "source": [
    "patent_dists=pd.DataFrame(keyword_depth_vecs, columns=['0_dist', '1_dist', '2_dist', '3_dist', '0_other', '1_other', '2_other', '3_other'])\n",
    "df = pd.concat([paper_info_df, patent_dists], axis=1) \n",
    "df=df.drop(columns=['keyword_depth_vec_affils', \n",
    "                 'keyword_depth_vec_author',\n",
    "                'PaperId_author',\n",
    "                'AuthorId_author',\n",
    "                'AffiliationId_author',\n",
    "                'AffiliationId_author',\n",
    "                'PaperId_fields',\n",
    "                 'keyword_depth_vec_fields'])\n",
    "\n",
    "print(df.iloc[0])\n",
    "# https://learn.microsoft.com/en-us/academic-services/graph/reference-data-schema gives you most information about what is in the dataframe df. \n",
    "# \"keyword_depth_vec\" is a 2x4 numpy array, where the first row is the number of \"surveil\" patent cites at 0,1,2,3 paper citation depth, \n",
    "# and the second row is the number of non-\"surveil\" patent cites. \n",
    "# This is broken into columns '0_dist', '1_dist', '2_dist', '3_dist' (surveil patent cites) \n",
    "# '0_other', '1_other', '2_other', '3_other' (non surveil patent cites)\n",
    "\n",
    "\n",
    "def implode(df, column):\n",
    "    agg_keys={}\n",
    "    for key in df.columns:\n",
    "        if key!=column:\n",
    "            agg_keys[key]=list\n",
    "            \n",
    "    return df.groupby(column, as_index=False).agg(agg_keys)[df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3804d1-f6b4-400e-83d4-42c742f2ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load citating graph for CVPr+citing papers. Takes a long time, only needed for river charts, not bar charts\n",
    "\n",
    "cvpr_graph=pickle.load(open(os.path.join(save_loc, f'cvpr_graph_-1.lz4'), 'rb'))\n",
    "expanded_cvpr_graph={}\n",
    "for k,v in tqdm(cvpr_graph.items()):\n",
    "    expanded_cvpr_graph[k]=pd.Series([v])\n",
    "cvpr_graph_df=pd.DataFrame(expanded_cvpr_graph).T\n",
    "print(cvpr_graph_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44944b95-eb82-40b0-bd9a-35f6b660b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draft river chart function\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.graph_objects as go\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "from matplotlib.colors import rgb2hex\n",
    "\n",
    "def river_chart(df, first_col, first_col_filter, group_col, ptp_df, backward=False, max_link_ind=4, num_per_link=10):\n",
    "    # Get matching entities\n",
    "    print('df.columns', df.columns)\n",
    "\n",
    "    e_df=df.explode(first_col)\n",
    "    if len(first_col_filter)==0:\n",
    "        past_entities=e_df[e_df['conf_if']=='1158167855']\n",
    "    else:\n",
    "        past_entities=e_df[(e_df[first_col] == first_col_filter) & (e_df['conf_if']=='1158167855')]\n",
    "    # past_entities=implode(past_entities, 'paper_id')\n",
    "\n",
    "    dist_list=['0_dist', '1_dist', '2_dist', '3_dist']\n",
    "\n",
    "\n",
    "    print('len(past_entities)', len(past_entities))\n",
    "    \n",
    "    df_mag=df.set_index('paper_id')\n",
    "    past_entities=past_entities.set_index('paper_id')\n",
    "    past_entity_names=past_entities[first_col].unique()\n",
    "    top_past_entity_names=past_entity_names[:num_per_link].tolist()\n",
    "\n",
    "\n",
    "    # river chart parts\n",
    "    source=[]\n",
    "    target=[]\n",
    "    value=[]\n",
    "    color=[]\n",
    "    labels=[]\n",
    "\n",
    "    river_ind=0\n",
    "    name_to_river_ind={}\n",
    "\n",
    "\n",
    "    past_other_2=[]\n",
    "\n",
    "    wrapper=textwrap.TextWrapper(width=20)\n",
    "\n",
    "    # Get down/upstream citations\n",
    "    for link_ind in range(0, max_link_ind):\n",
    "\n",
    "        cur_other=[]\n",
    "        past_other=[]\n",
    "\n",
    "        if link_ind==0:\n",
    "            past_entry_mags=past_entities.index.tolist()\n",
    "            citing_magss=[]\n",
    "            for past_entry_mag in past_entry_mags:\n",
    "                if past_entry_mag in ptp_df.index:\n",
    "                    citing_magss.append(past_entry_mag)\n",
    "        else:\n",
    "            past_entry_mags=past_entities.index.tolist()\n",
    "            citing_magss=[]\n",
    "            for past_entry_mag in past_entry_mags:\n",
    "                if past_entry_mag in ptp_df.index:\n",
    "                    citing_magss+=ptp_df.loc[str(past_entry_mag)].tolist()[0]\n",
    "\n",
    "        dist_key=dist_list[len(dist_list)-link_ind-1]\n",
    "        in_citing_magss=citing_magss\n",
    "\n",
    "        print('len(in_citing_magss)', len(in_citing_magss))\n",
    "        citing_cols=df_mag.loc[in_citing_magss]\n",
    "        citing_cols=citing_cols[['PaperId', group_col, 'keyword_depth_vec']]\n",
    "        citing_cols['keyword_depth_vec']=citing_cols['keyword_depth_vec'].apply(lambda x: x[0][:max_link_ind-max(0,link_ind)].sum())\n",
    "        print('reduce citing_cols.columns', citing_cols.columns)\n",
    "        citing_cols=citing_cols.explode(group_col)\n",
    "        citing_cols=implode(citing_cols, group_col)\n",
    "        citing_cols['keyword_depth_vec']=citing_cols['keyword_depth_vec'].apply(lambda x: sum(x))\n",
    "        citing_cols=citing_cols.explode('PaperId')\n",
    "        citing_cols=citing_cols.sort_values('keyword_depth_vec', ascending=False)\n",
    "        # print('citing_cols', citing_cols)\n",
    "\n",
    "        # Compute flows between past and current cite level\n",
    "        #print('past_entity_names[:num_per_link]', top_past_entity_names)\n",
    "        top_cur_names=citing_cols[group_col].unique()[:num_per_link].tolist()\n",
    "        print('top_cur_names', top_cur_names)\n",
    "        # print(top_cur_names)\n",
    "        links={}\n",
    "        print('len(in_citing_magss)', len(in_citing_magss))\n",
    "        citing_df=df_mag.loc[in_citing_magss]\n",
    "        print('len(citing_df)', len(citing_df['keyword_depth_vec']))\n",
    "        unique_citing_df=df_mag.loc[set(in_citing_magss)]\n",
    "        cited_df=df_mag.loc[past_entities.index]\n",
    "        list_keyword_depth_vec=citing_df['keyword_depth_vec'].tolist()\n",
    "        list_group_col=citing_df[group_col].tolist()                     \n",
    "        citing_mag_ind=0\n",
    "        surveil_added={}\n",
    "        for mag_ind in tqdm(range(len(past_entity_names))):\n",
    "            mag=past_entities.index[mag_ind]\n",
    "            sub_past_entities=past_entity_names[mag_ind]\n",
    "\n",
    "            if not isinstance(sub_past_entities, list):\n",
    "                sub_past_entities=[sub_past_entities]\n",
    "            sub_past_entities=set(sub_past_entities)\n",
    "\n",
    "            if mag in ptp_df.index:\n",
    "                #mag_keyword_depth_vec=cited_df.loc[mag]['keyword_depth_vec']\n",
    "                if link_ind==0:\n",
    "                    citing_mags=citing_df[citing_df[first_col]==past_entity_names[mag_ind]].index\n",
    "                else:\n",
    "                    citing_mags=ptp_df.loc[mag][0]\n",
    "                    # if mag_ind==0:\n",
    "                    #     print('citing_mags', citing_mags)\n",
    "                #citing_mags=list(set(citing_mags))\n",
    "                #citing_mags.append('surveil patent')\n",
    "                #citing_infoss=unique_citing_df.loc[citing_mags]\n",
    "                for citing_mag in citing_mags:\n",
    "                    between_cites=list_keyword_depth_vec[citing_mag_ind][0][:max_link_ind-max(0,link_ind)].sum()#citing_info['keyword_depth_vec'][0][:max_link_ind-max(0,link_ind)].sum()#\n",
    "                    other_between_cites=list_keyword_depth_vec[citing_mag_ind][1][:max_link_ind-max(0,link_ind)].sum()\n",
    "\n",
    "                    direct_between_cites=list_keyword_depth_vec[citing_mag_ind][0][0]\n",
    "                    direct_other_between_cites=list_keyword_depth_vec[citing_mag_ind][1][0]\n",
    "\n",
    "                    current_entities=list_group_col[citing_mag_ind]#citing_info[group_col]#\n",
    "                    if not isinstance(current_entities, list):\n",
    "                        current_entities=[current_entities]\n",
    "                    current_entities=set(current_entities)\n",
    "\n",
    "                    citing_mag_ind+=1\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    for past_entity in sub_past_entities:\n",
    "                        for current_entity in current_entities:\n",
    "                            if past_entity in top_past_entity_names:\n",
    "                                past_name=past_entity\n",
    "                            else:\n",
    "                                past_name=f'other_{link_ind}'\n",
    "\n",
    "                            if current_entity in top_cur_names:\n",
    "                                cur_name=current_entity\n",
    "                            else:\n",
    "                                cur_name=f'other_{link_ind+1}'\n",
    "\n",
    "                            if past_name not in links:\n",
    "                                links[past_name]={}\n",
    "                            if cur_name not in links[past_name]:\n",
    "                                links[past_name][cur_name]=np.zeros(2)\n",
    "                            links[past_name][cur_name][0]+=between_cites\n",
    "                            links[past_name][cur_name][1]+=other_between_cites\n",
    "\n",
    "                            # Add links to surveillance patents\n",
    "                            if cur_name not in links:\n",
    "                                links[cur_name]={}\n",
    "                            if 'surveil patent' not in links[cur_name]:\n",
    "                                links[cur_name]['surveil patent']=np.zeros(2)\n",
    "                            # if current_entity not in surveil_added:\n",
    "                            links[cur_name]['surveil patent'][0]+=direct_between_cites\n",
    "                            links[cur_name]['surveil patent'][1]+=direct_other_between_cites\n",
    "        # print(links)\n",
    "                    # else:\n",
    "                    #     print('not in', citing_mag)\n",
    "\n",
    "        # print('past_other', past_other)\n",
    "        # print('cur_other', cur_other)\n",
    "        past_other_2=cur_other\n",
    "\n",
    "        # Add flows to graph\n",
    "        for past_entity in links:\n",
    "            for current_entity in links[past_entity]:\n",
    "                # Get ind for prev river entry\n",
    "                if past_entity not in name_to_river_ind:\n",
    "                    name_to_river_ind[past_entity]=river_ind\n",
    "                    labels.append(past_entity)\n",
    "                    river_ind+=1\n",
    "\n",
    "                # Get ind for current river entry\n",
    "                if current_entity not in name_to_river_ind:\n",
    "                    name_to_river_ind[current_entity]=river_ind\n",
    "                    labels.append(current_entity)\n",
    "                    river_ind+=1\n",
    "\n",
    "                # Add data to river chart info\n",
    "                source.append(name_to_river_ind[past_entity])\n",
    "                target.append(name_to_river_ind[current_entity])\n",
    "                value.append(links[past_entity][current_entity][0])\n",
    "                if links[past_entity][current_entity][0]+links[past_entity][current_entity][1]>0:\n",
    "                    color.append(links[past_entity][current_entity][0]/(links[past_entity][current_entity][0]+links[past_entity][current_entity][1]))\n",
    "                else:\n",
    "                    color.append(0)\n",
    "\n",
    "\n",
    "        past_entity_names=citing_df[group_col]#.unique()\n",
    "        top_past_entity_names=top_cur_names\n",
    "        # print('past_entity_names', past_entity_names)\n",
    "        past_entities=citing_df\n",
    "\n",
    "    color=np.array(color)\n",
    "    cmap = plt.cm.viridis\n",
    "    norm = matplotlib.colors.Normalize(vmin=0, vmax=1)\n",
    "    colors=cmap(norm(color))\n",
    "    colors_hex=[rgb2hex(color) for color in colors]\n",
    "\n",
    "    colors_transparent=[f'rgba(0,0,0,{opacity})' for opacity in color]\n",
    "\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        node = dict(\n",
    "          pad = 15,\n",
    "          thickness = 20,\n",
    "          line = dict(color = \"black\", width = 0.5),\n",
    "          label = labels,\n",
    "        ),\n",
    "        link = dict(\n",
    "          source = source,\n",
    "          target = target,\n",
    "          value = value,\n",
    "          color = colors_transparent\n",
    "      ))])\n",
    "\n",
    "    fig.update_layout(title_text=\"Basic Sankey Diagram\", font_size=10, width=2000, height=1500)\n",
    "    fig.show()\n",
    "\n",
    "river_chart(df, 'NormalizedName_author', 'li feifei', 'PaperTitle', cvpr_graph_df, backward=False, max_link_ind=1)\n",
    "#river_chart(df, 'NormalizedName_author', '', 'PaperTitle', cvpr_graph_df, backward=True, max_link_ind=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f50bef-321a-4ebb-ad62-10e646322327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fcff6b-fce7-4f5d-9f2f-810c9f5a9a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draft river chart function debug\n",
    "\n",
    "first_col='NormalizedName_author' \n",
    "first_col_filter='li feifei'\n",
    "group_col='NormalizedName'\n",
    "ptp_df=cvpr_graph_df \n",
    "backward=False\n",
    "num_links=4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc48a1f-336f-4bb4-b287-f17395a1cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart function\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def plots_by_col_name(df, col_plot):\n",
    "    '''\n",
    "    Plot sum of patents with 'surveil' in them at different citation distances.\n",
    "    '''\n",
    "    \n",
    "    # Consider only small subfields\n",
    "    if col_plot=='NormalizedName_fields':\n",
    "        df_e=df.explode([col_plot, 'PaperCount_fields'])\n",
    "        df_e=df_e[(df_e['PaperCount_fields'].astype('float')<=250) & (df_e['PaperCount_fields'].astype('float')>0)]\n",
    "    else:\n",
    "        df_e=df.explode(col_plot)\n",
    "    \n",
    "    print('len df_e', len(df_e))\n",
    "    # Only consider CVPR papers\n",
    "    df_e=df_e[df_e['conf_if']=='1158167855'].reset_index()\n",
    "    print('len df_e', len(df_e))\n",
    "    \n",
    "    uniques=df_e[col_plot].unique()\n",
    "    print('num items in x axis', len(uniques))\n",
    "    \n",
    "    # Compute custom y axis, store in res_dict\n",
    "    res_dict={col_plot:[], '0_dist':[], '1_dist':[], '2_dist':[], '3_dist':[], '0_other': [], '1_other': [], '2_other': [], '3_other': []}\n",
    "    for unique in tqdm(uniques):\n",
    "        res_dict[col_plot].append(unique)\n",
    "        inds=df_e[df_e[col_plot] == unique].index\n",
    "        for key in res_dict:\n",
    "            if key!=col_plot:\n",
    "                val=df_e[key][inds].sum()\n",
    "                res_dict[key].append(val)\n",
    "\n",
    "    # Make plots\n",
    "    names_dict={'NormalizedName_fields': 'topic',\n",
    "                'NormalizedName_author': 'author', \n",
    "                'NormalizedName': 'instituion', \n",
    "                'Iso3166Code': 'country'}\n",
    "\n",
    "    df_e['0_dist']=df_e['0_dist']/(df_e['0_dist']+df_e['0_other'])\n",
    "    df_e['1_dist']=df_e['1_dist']/(df_e['1_dist']+df_e['1_other'])\n",
    "    df_e['2_dist']=df_e['2_dist']/(df_e['2_dist']+df_e['2_other'])\n",
    "    df_e['3_dist']=df_e['3_dist']/(df_e['3_dist']+df_e['3_other'])\n",
    "    \n",
    "    for key in res_dict:\n",
    "        if key!=col_plot:\n",
    "            print('plot')\n",
    "            df_plot=pd.DataFrame.from_dict(res_dict)\n",
    "            df_plot=df_plot.sort_values(by=[key], ascending=False)\n",
    "            # This is where the axes are choosen, only plot top 25\n",
    "            ax=df_plot[:25].plot.bar(x=col_plot, y=key, title=f'Number Cites by Patents with \"Surveil\" by {names_dict[col_plot]} at cite distance {key}')\n",
    "            ax.set_xlabel(names_dict[col_plot])\n",
    "            ax.set_ylabel(f'Number Patents with \"Surveil\" in them at cite distance {key}')\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01e298a-3276-4c1e-9452-e2f6251b174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make bar charts\n",
    "cols_plot=['NormalizedName_fields','NormalizedName_author', 'NormalizedName', 'Iso3166Code']\n",
    "for col_plot in cols_plot:\n",
    "    plots_by_col_name(df, col_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5babce-446b-4ef0-b1dd-143a18672d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug/scratch\n",
    "df_e=df.explode('NormalizedName_author')\n",
    "df_e=df_e[df_e['paper_id']=='2155904486']\n",
    "df_sw=df_e[df_e['NormalizedName_author']=='li feifei']\n",
    "df_e=df_e.reset_index()\n",
    "inds=df_e[df_e['NormalizedName_author'] == 'li feifei'].index.tolist()\n",
    "print(inds)\n",
    "print(df_e['0_dist'][inds])\n",
    "print(df_e['0_dist'])\n",
    "val=df_e['0_dist'][inds].sum()\n",
    "print(val)\n",
    "for i in range(len(df_sw.iloc[0])):\n",
    "    print(df_sw.columns[i], df_sw.iloc[0,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125afd56-293b-4cbb-a04c-f0b3c4fb3267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
